ðŸ“ Video Description:
What if everything we think we know about AI understanding is wrong? Is compression the key to intelligence? Or is there something moreâ€”a leap from memorization to true abstraction?

In this fascinating conversation, we sit down with *Professor Yi Ma*â€”world-renowned expert in deep learning, IEEE/ACM Fellow, and author of the groundbreaking new book *Learning Deep Representations of Data Distributions*. Professor Ma challenges our assumptions about what large language models actually do, reveals why 3D reconstruction isn't the same as understanding, and presents a unified mathematical theory of intelligence built on just two principles: *parsimony* and *self-consistency*.

**SPONSOR MESSAGES START**
â€”
Prolific - Quality data. From real people. For faster breakthroughs.
https://www.prolific.com/?utm_source=mlst
â€”
cyberâ€¢Fund https://cyber.fund/?utm_source=mlst is a founder-led investment firm accelerating the cybernetic economy
Hiring a SF VC Principal: https://talent.cyber.fund/companies/cyber-fund-2/jobs/57674170-ai-investment-principal#content?utm_source=mlst
Submit investment deck: https://cyber.fund/contact?utm_source=mlst
â€”
**END**

NOTE: Based on lower retention we have EDITED out 33:45 of the original interview from near the start - philosophical stuff, an extended analogy comparing DNA evolution to AI development, speculation about where theories come from (Platonism, deductive trees), Norbert Wiener's cybernetics history. You can watch the original version on the rescript link or Spotify version.

Key Insights:

*LLMs Don't Understandâ€”They Memorize*
Language models process text (*already* compressed human knowledge) using the same mechanism we use to learn from raw data.

*The Illusion of 3D Vision*
Sora and NeRFs etc that can reconstruct 3D scenes still fail miserably at basic spatial reasoning

*"All Roads Lead to Rome"*
Why adding noise is *necessary* for discovering structure.

*Why Gradient Descent Actually Works*
Natural optimization landscapes are surprisingly smoothâ€”a "blessing of dimensionality"

*Transformers from First Principles*
Transformer architectures can be mathematically derived from compression principles

â€”

INTERACTIVE AI TRANSCRIPT PLAYER w/REFS (ReScript):
https://app.rescript.info/public/share/Z-dMPiUhXaeMEcdeU6Bz84GOVsvdcfxU_8Ptu6CTKMQ

About Professor Yi Ma

Yi Ma is the inaugural director of the School of Computing and Data Science at Hong Kong University and a visiting professor at UC Berkeley.

https://people.eecs.berkeley.edu/~yima/
https://scholar.google.com/citations?user=XqLiBQMAAAAJ&hl=en
https://x.com/YiMaTweets

*Slides from this conversation:*
https://www.dropbox.com/scl/fi/sbhbyievw7idup8j06mlr/slides.pdf?rlkey=7ptovemezo8bj8tkhfi393fh9&dl=0

*Related Talks by Professor Ma:*
- Pursuing the Nature of Intelligence (ICLR): https://www.youtube.com/watch?v=LT-F0xSNSjo
- Earlier talk at Berkeley: https://www.youtube.com/watch?v=TihaCUjyRLM

--
TIMESTAMPS:

---
REFERENCES:
Book:
[00:03:04] Learning Deep Representations of Data Distributions
https://ma-lab-berkeley.github.io/deep-representation-learning-book/
Book (Yi Ma):
[00:03:14] An Invitation to 3-D Vision
https://link.springer.com/book/10.1007/978-0-387-21779-6
[00:03:24] Generalized Principal Component Analysis
https://link.springer.com/book/10.1007/978-0-387-87811-9
[00:03:34] High-Dimensional Data Analysis with Low-Dimensional Models
https://book-wright-ma.github.io/
Slide:
[00:44:11] Slide 26: Neuroscience Evidence
https://arxiv.org/abs/2207.04630)
Person:
[00:08:24] Albert Einstein
https://quoteinvestigator.com/2011/05/13/einstein-simple/
[00:56:41] Kevin Murphy
https://probml.github.io/pml-book/book1.html
Paper:
[00:18:09] Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs
https://arxiv.org/abs/2401.06209
[00:26:13] A Global Geometric Analysis of Maximal Coding Rate Reduction
https://arxiv.org/pdf/2406.01909
[00:47:26] CRATE: White-Box Transformers via Sparse Rate Reduction
https://arxiv.org/abs/2306.01129
[00:55:05] DINOv2: Learning Robust Visual Features without Supervision
https://arxiv.org/abs/2304.07193
[01:00:36] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)
https://arxiv.org/abs/2010.11929

ðŸ“‘ Video Chapters (from description):
0:00:00 Introduction
0:02:08 The First Principles Book & Research Vision
0:05:21 Two Pillars: Parsimony & Consistency
0:09:50 Evolution vs. Learning: The Compression Mechanism
0:14:37 The Illusion of 3D Understanding: Sora & NeRF
0:20:41 All Roads Lead to Rome: The Role of Noise
0:26:11 All Roads Lead to Rome: The Role of Noise
0:26:29 Benign Non-Convexity: Why Optimization Works
0:32:50 Double Descent & The Myth of Overfitting
0:40:41 Self-Consistency: Closed-Loop Learning
0:47:18 Deriving Transformers from First Principles
0:56:26 Verification & The Kevin Murphy Question
1:00:26 CRATE vs. ViT: White-Box AI & Conclusion

---

(0:00) In the past 10 years, I think the question about the intelligence or artificial intelligence has uh captured people's imagination. Uh I'm one of them, but it took me about 10 years try to really understand uh can we actually make understanding intelligence a truly scientific or mathematical problem to formalize it. You probably will get some of my opinion and also the the facts about it and probably will change your view what is intelligence is which is also a very so certain process for me.

(0:31) How do we clarify uh some misunderstandings the common misunderstandings about the intelligence? Through this journey uh maybe we'll gain a entirely new view about what we really have done right in the past 10 years. The practice of artificial intelligence the mechanism we have implemented the all the mechanism behind all the large models deep networks and the large models are truly are and uh the true natures and uh hence

(1:01) understand their limitations and also what it takes to truly build a system that has intelligent behaviors or capabilities. I think we have reached the point right we'll be able to address what's the next for understanding even more advanced form of intelligence what's the difference between compression and abstraction difference between memorization and understanding I think of future those are the big open problems for all of us to study

(1:34) MLST is supported by cyber fund link in the description >> the idea of having to traffic and squishy people in order to make our systems go is not immediately appealing. Let's put it that way. >> This episode is sponsored by Prolific. >> Let's get few quality examples in. Let's get the right humans in to get the right quality of human feedback in. So, so we're trying to make human data or human feedback. We treat it as an infrastructure problem. We try to make it accessible. We make it cheaper. We

(2:06) effectively democratize access to this data. Professor Ma, it's amazing to have you on MLST. Welcome. >> Thank you for having me. >> So, normally I ask guests to introduce themselves, but given your your stature in the field, I think it's best that that that I give you an introduction. So, um, Yimar is a worldleading expert in deep learning and artificial intelligence. He's the inaugural director of the school of computing and data science at Hong Kong University and director of the Institute of Data

(2:36) Science at the University of Hong Kong. He's also a visiting professor at UC Berkeley where he previously served as a full professor in electrical engineering and computer science. He's an ITE E fellow, ACM fellow and CM fellow whose pioneering work on sparse representation and low rank structures has fundamentally shaped modern computer vision and machine learning. His recently published book learning deep representations of data distributions proposes a mathematical theory of

(3:07) intelligence built on two principles pasimony and self-consistency. This framework has led to whitebox transformers known as crate architectures where every component can be derived from first principles rather than empirical guesswork. So um professor mah tell me about your book. >> You know about you know uh eight or seven years ago you know you know deep networks uh deep learning has been pretty you know changed the practice of

(3:37) machine learning or artificial intelligence uh in the past decade. Um about uh eight years ago I had a chance to get back to Berkeley and um gave me a chance to look into this uh topics more deeply try to understand it from more principled approach and hence uh the book is kind of a summarizing the kind of progress we made in the past uh you know eight or beyond years myself my group as well as many colleagues uh

(4:07) trying to understand uh the principles behind Um the deep networks explained it uh from our first principle and uh in that journey we also seems to embark on um a little bit beyond that uh find uh something probably more general behind that is the uh the intelligence at a certain level of intelligence and hence um when I get back to joined Hong Kong U about two years ago had a chance to

(4:38) design or redesign some of uh curriculum to reflect those uh some of the progress updates in our field, the rapid progress in our field. Um so my students and colleagues decided that maybe it's time to uh systematic organize uh this body of knowledge and uh reflect as a as a textbook as well as a new course uh which I'm teaching this semester and uh likely will be offered as well at Berkeley next semester. So this is actually a first

(5:10) time probably we try to uh provide a more principle approach to explain the the deep networks um as well as some principle of uh intelligence >> and these principles are panimony and self-consistency. So it's it's an ambitious idea that these principles could explain natural and artificial intelligence. What do you mean by that? >> Intelligence artificial or natural uh or whatever uh adjectives you add to

(5:40) intelligence uh we have to be very specific. It's a very loaded world, right? I mean, even intelligence itselfves may have different levels um different um stages, right? So, it's high time we clarify that concept scientifically or mathematically, right? Um then so that we'll be able to talk about uh study intelligence the mechanism behind it at each level. uh there are some more unified principle

(6:11) behind even different stages of intelligence there's there's something in common there also s are different so it's high time we we we do that one of the intelligence at the level is that common to animals are human right human we are animals that int level of intelligence is what we think are very common to all life which is how memory how we learn knowledges about the external world and then memorize as part

(6:41) of our memory and use that to predict to react to the world help us make decisions predict and make better decisions with for survival and so on so forth that's very very common and this is the level of intelligence we're talking about very much for the book as well right and um hence for this level of intelligence uh how our memory works uh today we also have a fancy word for memory we call it a world model Okay. Um and it's how this how we develop such a

(7:11) memory such a world model and how the model gets evolved and uh how we use it that's actually this is the level of we talk. So we actually believe that uh for this level of intelligence for how our memory formed and how they work is precisely the two principles are incredibly important. um and we believe they're necessary uh is that me memory or knowledge is precisely try to discover what's predictable about the world. Hence um for all understand all

(7:43) such information are have intrinsically very low degree of freedom. we call the lowdimensional structures and uh hence the way to pursue such knowledge is precisely through uh trying to find the most simple representation of the data and uh hence compression denoising dimension reduction is actually all just different words to pursue such knowledge such structure and hence that's the word captured by the word parimony

(8:13) finding you know explain making things as simple as possible but not any simpler right um so this is Einstein says this word uh this the sentence Einstein used to describe science actually this is also what the intelligence at least at this level is precisely doing the same thing um the second part of the sentence not any simpler precisely says consistent consistency make sure your memory is actually consistent with be able to um

(8:43) recreate simulate the world just right um not any similar. If you're simpler, you may lose part of the um the uh predictivity uh predictivity uh and also ability to predict it well. And so that's actually the those two actually uh coexist we believe and those are two principles parsimmony and the consistency or subconsistency are actually the two characteristic about how our memory works. So we want to have

(9:14) understanding which carves the world up by the joints which represents the important invariances in the world and the thesis is I think that compression might be necessary for understanding. My possible concern with that is that what we are doing with machine learning is representing extent examples of a long phoggenetic tree of evolution. Mhm. >> So to what extent does knowing their representation now help us? Do we also

(9:46) need to know how they evolved and where they might go in the future? >> The the process to acquire knowledge to gain information about our side world that's a compression. Uh find what is compressible, what has orders, what phenomena has orders. uh has low dimension structures that allow us to predict to rule out uh variabilities to predict world uh

(10:17) tomorrows or predict world better in that sense. Um so in a sense that uh that is ability we uh believe is really what intelligence is all about uh at least the the common intelligence we're talking about right we can talk about the more higher level intelligence later uh um and if you look at the the history of life how life was developed um um so we actually come up to believe right uh

(10:47) uh you know you know the the the mechanism that we laws that governs the physical law world we call the physics right but what is the mechanism governs the evolution of life I think it's intelligence right even the process you mentioned that uh through evolution and um life evolves um precisely they they learn more and more knowledge about the world and they encode them through DNA to pass it on to next generation and that is a compressing that's a that's a

(11:18) process to compress logic that learned about the world through our DNAs. But the the the the mechanism to update it is actually very brutal, very brutal force and um through you know random mutation and the natural selection. Yes, it does evolve. It does advance but at a huge cost of resource time and also very unpredictable which if you if you're acute you

(11:50) probably observe there's some similarity with how current big model evolves right um many many groups try uh without principle try and error empirical and um the lucky one survive and gets advocated uh you everywhere and become very very popular right dominate the practice so so in a sense that you can make the allergy right I think to the people you know students ask me at which stage our artificial intelligence uh is at today

(12:22) um then there's already a allergy in in nature right we are very much at the early stage of the life form right and so hence that is a compression process that's a process that also gain knowledge about the world But of course later on we develop uh individual animals to develop the brain develop you know neural systems develop uh senses including visual and touch and so on. So we actually you start to use a very very different mechanism to learn to compress

(12:54) our observations to learn knowledge and to build memories of world and even individuals start to have that ability rather than just inherit knowledge from their DNAs. So that's a different stages uh about uh and then that part of the knowledge is not no longer encoded in our genetics in our genes but also in our brains. Um and that's actually that's actually a level of intelligence we talk about most of the time these days you know which is

(13:24) common to animals which is common to to humans and the le the knowledge the or the intelligence we talk about what brain functions. Yeah, I mean I I think we would definitely agree with the statement that intelligence as a system produces artifacts. So Shallay's example is a road building network. It produces roads and the system has adaptivity because it can create new roots where they weren't there before. >> And then there's the question of well there are many ways to compress a thing.

(13:54) >> So some ways of compression represent the world at a deep abstract level and some don't. So we might argue that LLMs today even though they do compress the data they only compress it in a superficially semantic way. >> And then there's this notion of well maybe we agree that intelligence is about the synthesis of new knowledge. So it's the acquisition of new knowledge but we can only do that if the knowledge we already have represents the world at a deep abstract level. So rather than it being random mutations in evolution,

(14:24) it's very very structured because the processes are physically instantiated which means rather than just doing something completely random, it's guided by the process which created the manifold hypothesis comes to mind which is this idea that all natural data falls on some lowdimensional you know structure you know with a low intrinsic dimension. And the other thing that springs to mind is I mean I'm a fan of geometric deep learning which is this idea of you know we should imbue

(14:54) inductive prior in the system which represent symmetries and and geometric structures in in the world and and I think as a principle that's deeply embedded in in this idea. >> Exactly. Um if if you look at my my whole life I have written four books right. uh my early interest is studying is computer vision um and the first book is vision and from that work I study multiview geometry from that work I in

(15:24) my whole book the the all the four book is actually about the one theme I realized that it's about structure in the data especially all reflected know them the first book of sweet division um the last chapter I realized precisely about what importance the symmetry plays in our perception right and we perceive object we naturally has a so human being we remember our vision we only recognize a long time ago right

(15:55) these days people oh it's about the vision is about recre 3D absolutely not right all the people says we just multiple images we create a whole point clouds mesh sign distance function lurf gion splatter right we We create a scene. See, I can see from multiple angles. Is this 3D understanding or you create some videos like Sora look at it looks good for you know absolutely not. Right?

(16:26) This is not the representation or understanding of a world model. Right? Our understanding is far beyond getting a whole bunch of you know point clouds or goshes better we can view from different angles. Right? Have you noticed that when we see something, we get excited because we understand the 3D, we understand the content, we parse it already in our brain, but the machine has no idea what the heck is in there. Okay, it's just a

(16:56) bunch of point clouds. It's a depth map. Okay, we when we see this change angle, we saw 3D. We already automatically recognize this is a hand, this is a body, this is a cup, this is a this is apple, right? We do that. We fill in that information with our brain. We think machine once they can recruit the 3D they all understand that this is completely wrong. Okay. Many many work they say we're building 3D model by creating something for people to look at. That's complete out of purpose. So look at our vision. Our vision model

(17:27) right we have hypoc campus we have our ID code is highly structured. We understand the relationships between our view ccentric, object ccentric and aloscentric reputation. Right? Neuroscientists understand this very very well. Right? Scientists understand this very well but not computer scientists. Right? Not computer vision scientist. Um some do. See for example I give you example right in order to do spatial now we actually did a test with you on the corner about a

(17:57) year ago. Test all the you know top multi-model models you know huge models GPT you know that uh GMI right to do very simple test the the title of the work called a eyes wide shot right and it's a very simple test just test the given images for the those language model or a big model multimodel highly trained highly commercialized models that do they understand the reasoning spatial reasoning what is on the left of something

(18:28) how many objects there is in space right what is beh behind something, what's on top of something very simple spatial. The question requires a little bit, not even very deep spatial understanding. But all the models fails miserably and the majority of them actually even worse than random guess right only I think only Gemini and uh GPD uh is a little bit above human a little bit about the random guess far below human understanding. Right? So that's that's

(18:58) the status right if you do those meaning that you know those act 3D understanding is very very difficult but human we do this effortlessly right so I can easily point to you right please hand me the the the bottle to your left if you want to find let's say a shopping center right say go through the door turn right go to the once you get outside of the building um head south right so through this this Remember just through this

(19:28) simple sentence already we switch from viewcentric to object centric to alocentric right so our bra if we don't have this kind of model this kind of highly structured 3D models forget about you know people talk about embodied AI or world model right we cannot conduct this very simple spatial references interacting we have this world model is not to visualize.

(19:58) We build a 3D model to interact, to manipulate, to influence, right? We're not building a 3D model just to, oh, this looks like I can change my view, look at it from this view or that view, right? Just visualize. No, turn 360 degree to visualize. No, we don't do that. That's not our purpose. Okay. Unfortunately, but there, you know, it gets distracted for that kind of uh visualization. It looks cool but uh almost to to to us if you really work on robotics uh for all

(20:28) kind you know navigation local motion manipulation they're actually pretty uh the usage is pretty limited. I won't necessarily say they're useless but they are actually pretty limited. We should introduce the coding rate formula. I did have a question about that which is there is a there's an epsilon on there. So there there's a bit of a question of how how do we tune that and and what does that mean? We should also bring we've been talking about this a little bit this um this concept of an LDR so a linear um discriminative representation and just more broadly

(20:58) with these inductive prior there's always the question of when we do abstraction to model regularities in the universe there's always a little bit left over isn't there so to to what extent can we think of these things as natural >> actually you touch upon about the deep stone you touch upon a very very deep question uh we actually uh it actually took me almost 30 years to understand it to be honest right we did mention that early on when we do try to differentiate different measure different volumes it turns out lossy coding is necessary it's

(21:30) not just something that uh is something hacky it's actually turns out to be necessary and to do lossy coding in fact we recently we start to realize that noise actually be you know plays very different roles and uh yet it's very confounded very confusing to many people Even um this is something actually my students will see to realize we actually probably will have some papers about it. I can elucidate this a little bit. If you think about the whole diffusion den noiseis and model right people very popular right now to do um why do we add

(22:02) noise to data right and to the whole world because we don't know where the distribution is right so there is a phrase everybody knows all roads to Rome right so why is that has anyone give a thought to why all roads to Rome because very simple at some point in history Rome builds the road to reach the whole Oh, right. That's a diffusion process. Then if you want to know Rome, then you do the den noise. You follow the same

(22:33) way back you get to where the no room is. So that's the no dimensional structure. That's where the knowledge is, right? That's the osis is right. So hence it's a very natural process that we add noise is adding noise to the is precisely we building roads and the noising brings us back remember where we come from and so on and that's a big slope we have to add noise to reach the whole earth there's another actually

(23:03) there's another noise right remember we only have isolated sample even we talk about manifolds Right? But how many points you have on the manifes? How many points you observe? They're always finite. Right? But why do you call it a continum? Why do you collect dots as lines, planes, surfaces? When do you do that? Hence, noise plays another role within

(23:34) the manifold. Even you have finite samples, if you allow lossy coding, if you allow packing spheres in that things start to connect. You start to connect. Noise is very important to help to connect the dots, right? We all know the phenomena of percolation, right? We see raindrops on the floor. You only see two phases, right? One phase is all the dots are isolated. Another phase is all things gets wet. You never see anything into in the middle because there's a sharp face

(24:04) transition. Once the the the sphere once the dots the density gets high enough they collects everything. Right? Maybe that's a phase transition we reach we realizing a connected plane is a better solution to explain all the data more parsimmonious more economic. The cost to memorize all the dots versus memorize other plane start to switch. Maybe abstraction has something to do with that. I don't know. But from a

(24:36) compression point of view, this can already allow us to explain when do we go from zero dimension samples to prefer a low dimensional manifolds and also how go from that low dimensional manifold to reach the rest of the world. Right? So you can see even in this process noise is already playing this epsilon plays different roles and as at some point they gets collected right around the surface and that's we still try to figure out what happens but at this big two phases uh we already

(25:07) know right the role of epsilon actually plays different roles right um and I think the the definitely in the past many years uh our understanding about the subject how do we compress how do we uh pursue the lowdimensional structure from finite samples. It's a quite our understanding about this problem has truly advanced dramatically. I'm very happy honestly this is a question baffles me when I was a graduate students. uh you can see even my early

(25:37) work about the lossy coding lossy compression reflected my bafffulness about it and I'm really feel very uh thrilled that I recently start to understand those things in a more unified more not only theoretical way but also even algorithmic way >> yeah it's so fascinating that we can look out the window >> and we we ignore so much detail we don't look at the leaves on the roads we we just find that structure and that's why when I watched your presentation I was very intrigued when you said that denoising iterative denoising is is a

(26:08) form of of compression. I wanted to mention your ICML 2024 so last year it was in Vienna right with with Wang and you found that when you have loss surfaces using using this technique they are dramatically different they're very smooth there's no kind of harsh local minima and so on what's the intuition for that >> in fact the phenomena our understanding about those phenomena is actually going back when we early days we study sparsity um you know when your data lies on very

(26:38) low dimensional uh sparse surfaces uh planes right low dimensional planes also of the planes or low rank matrix right and in there we learned a very big lesson the object function to evaluate those sparity or low dimensionality uh those function are highly nonlinear nonconvex um but yet you know traditionally in our orthodox understanding about the l convex optimization is they're always hard right and the general class is n hard and there's a lot local spirus uh

(27:10) local minima you get stuck with local minima you there's some stagnant critical points flat surface so basically the worst picture is very worse right it's a nightmare uh but through the study of those low dimensional structure uh sparse structure and that's was actually featured in our my previous book right high dimensional a low dimensional structure of high dimensional data analysis we actually realized that if a lot of long convex problem even the optimization problem had long convex

(27:40) landscape. If those problem or if those measure arises from nature very natural resource those structure actually are very highly regular highly has symmetry the landscape actually are extremely benile right quite contrary to our common understanding about linear optimization at all right this is complete 180Â°ree uh flip of views In fact, even the higher

(28:12) dimension helps. The higher the dimension, the better. We call it the blessing of dimensionality. So those regularity, those symmetry will tell us the landscape of this object function are actually beautiful, right? And uh first of all, they're highly regular. There's no staging. There's no flat surface. There's no too many spirits local minima. And even the local minima, they already have very clear geometric or statistical meaning. And hence those landscape are very aminable for very

(28:44) simple algorithm to find the optimal solution such as graded descent which in almost indirectly explain why even we're doing even more than training neuronet networks and many more we're searching uh low dimensional distribution in very high dimension spaces but somehow greed always end up with somewhere nice. Okay. Yeah, fine. You can run a long time but somehow you always end up those land landscapes are not that hard to traverse traverse right. So it actually be

(29:15) precisely because those object function are highly regular. Hence now get back to the rate reduction object function right if you look at the object function it's not something arbitrary right it's counting the volume of the whole minus the parts right it's something extremely objective right it's not like a loss function people come up with randomly oh add this term weighted sum add different weights you know if you use this you know sort of a empirical penalty or

(29:46) empirical even some kind of ad hoc So all the terms are describing physical volumes of the data right. Hence you should expect those are the quantity arise in nature. Then from our lesson we realize indeed actually that actually those object function has very benile landscapes uh even the local minima not only the global minima corresponding solutions that are give you authoral subspaces even the local ones right there not the global optimal ones they

(30:16) have similar geometric structures right and there's no other weird critical points that will slow down the search for those minimas so that's actually quite interesting So you can see this this revelation allow us to understand right where maybe intelligence is precisely exploit and harnessing those things. So there is actually a misunderstanding about you know last 10 years when we understand intelligence more and more

(30:47) there is a very big misunderstanding about intelligence right in machine you you you you study you know maybe machine learning theory right we have a tendency to believe intelligence especially intelligence and nature is designed to solve the hardest problem the worst case I actually beg to differ intelligence is precise the ability to identify what is easy to address first, what is easy to learn, what is natural to learn first.

(31:19) Then only when that has been done and the resource permit, they start to get into more and more advanced tasks. Not everybody needs to learn advanced mathematics to supply animals don't. Right? Nature find what is the easiest things with minimal energy, minimal effort to learn the most logic so they survive the best. Right? Again this is the principle of parimony at play.

(31:51) There's another level of resource parimony at play here. Right? Hence once you realize this you realizing understanding intelligence should be really understand what's really most common. Right? the low dimensional structure most easy ones smooth ones benile distribution easy to get get away with a few samples fewer samples right uh and very easy to formulate in fact that's what how science progress you know a lot of the physical models you know Newton's law they're very simple they discover

(32:21) the simple ones then we reach gradually reach to general relativity and then to quantum mechanics those equation gets f more complicated later right so this is h the same process we identify what is the most common first right what is the most easiest task first hence we don't want to many of the machine learning theory try to tend to so derive a bonds for the worst cases I think that's we probably should think twice right >> I love that character characterization it's um similar to the least action

(32:52) principle in in physics >> exactly in in a sense we solve problems by taking many many steps in different directions I think we still leave a little bit of entropy open we don't do pure hill climbing but collect Collectively we acquire these stepping stones and the totality of that process as we solve very complex problems. But I wanted to touch on you raised a very interesting point which is um we notice that when we have very large deep learning models they tend to almost self-regularize and they they learn

(33:24) better and there's this phenomenon of double descent and and all of this. Tell me about that. Fascinating question actually you uh this question actually needs to um really u bring me back to early days when I try to understand deep learning um when deep aris there's a lot of phenomena we try to understand I'm I'm one of those right try to understand those phenomenas u you know some something there's some good about dropout something about the you know thresholding different thresholding there's something about uh normalization

(33:56) then also come to somehow the model are very big um and parameters a lot somehow the deep network course has uh do not have a tendency to overfit somehow they still generalize okay right and then of course people realize and there's a sort of a uh rather unlike the re you know the traditional classical bias um virus trade-off but there's tier double descent I actually wrote a couple papers about it and um about the normalization about

(34:28) around 2000 um or late uh uh 2019 I I really told my student we should stop not to explain those isolated phenomena we only see we're like the blind mental elephants each one say a little piece each theory try to explain a little bit I think there should be a total total explanation to this if we get the big picture all those are just the consequences or implications of that suddenly you know at that time We start

(34:58) to touch upon the the the concept of maybe that the the proc of deep network are optimizing something the layer wise is realizing optim they're optimizing objective that promoting parimony promoting no dimensionality once we realize actually I was quite thrilled so then I told my student from now on we will no longer write any papers or about overfitting why Because if the neuronet networks is try

(35:31) to the operators try to compress try to realize certain contracting map compressed volume then you will never overfit right even I over parameterize it will never overfit simple example if I have data lies on a straight line a one-dimensional curve whatever I can embed this one dimensional line in a two dimension three dimension or a million dimension But if my operator is always layer-wise at each iteration, my

(36:01) operator is always just shrinking my solution towards the line in all directions. I would never lower it. Even I overparameterize embedded the line into a billions of dimension. I have a billions of parameter. But collectively all those billions of parameter are all shrinking my solution, pushing the solution, denoise it, compress it towards the line, right? like a power iteration just like a PCA right power iteration is in regardless of what the

(36:31) dimension embedded computing the first singular values right it's always powerful converge with the same speed you never over so compression by nature if the operator are performing compression or denoising which means this process will no longer overfit anything right if you conduct it right if you converge the solution will converge on the is the structure you desire for. >> That raises a natural question. We were interviewing Andrew Wilson from NYU and

(37:01) he's got this, you know, several papers about implicit biases where you kind of have a combination of, you know, hard biases of symmetries and and everything in in between. And if what you're saying is true, then why do we need inductive biases at all? Could we not pair back a little bit and just have really big models? >> No, I think see this is the thing, right? Exactly. So, this is the thing. Um um I was uh you know early on people don't understand deep networks and there's a lot of empirical uh trial and error people try to tends to use this

(37:33) the phrase inductive bias to either as some kind of magical sauce that either explain the failures or success of you do certain way to the neuronet networks design or how do you train the neuronet networks to be honest for for a long time I never understood what the inductive bias is um and maybe some regularization Some people is learning some structures about network about the data. Um but nowadays in my recent work I said that probably from at least from what I understand um all the inductive

(38:05) bias should be uh formulated as first principle right at least from we were able to for example deduce uh all the different network architectures including the recent white box crate or transformer-l like or redunet like uh reset like architecture or mixture of expert like architecture all from the only inductive bias is assuming your data distribution you are pursuing are low dimensional. Okay, you can already get the form the the

(38:35) main architecture or form of operator of for each layer as a red structure mixture of expert structure and and those operator per layer are precisely conducting denoising compression or contrasting. Are there additional assumption you can make? Yes, you can. For example, if I my job is not just to compress the data as it is. I also wanted to induce I wanted to for example in object recognition I also want to enforce make all data I want my

(39:06) classification to be translational environment which is symmetry right if you allow my task will be environ action I want to compress them together voila what do you get you can then you still through compression then you get a convolution naturally as the structure for the compression operator so convolution is not what we impose upon. is actually results from the first principle the the quote unquote inductive bias assume you want to

(39:38) compress your data also you want your compression to respect translation environment or rotation environments that's the result that is the characteristic of the compression operator for you to achieve that task right so there's a lot of um so we don't want to build in the the the inductive bus While we are searching for the solution, the inductive bias in my understanding is should be the very assumption we make in the very beginning. The rest should be deduction.

(40:10) The rest should have no induction anymore. Otherwise, we're doing trial and error, right? The inductive we so basically when we build the theory, we should have done all the inductive observations, experiment and assumptions already. The good theory should start with the very few inductive bias or assumptions or axioms then the rest should be deductive. I call that first principle. We've been speaking about

(40:41) pasimony which is what to learn and self-consistency is about how to learn and we can sketch out a journey I suppose from control theory to learning and and also this this methodology has some interesting um results I think around um you know the continual learning problem so let's sketch that out >> you can see right so the the compression um um or the the even the rate reduction to try to pursue the data distribution and also So transform it and that's the

(41:11) one way direction. there's almost there's no theoretical guarantee either your data is sufficient to identify that you know you may started with very very few samples there's no way the data is sufficient I mean may the apple there are five types maybe I only say four types right so and then but that process goes on you do compress what you have and the reach a memory right and there's no guarantee the even during that

(41:41) process you may not get stuck Maybe there's not enough iterations you may so memory you get may not be accurate may not be correct hence how do we check how do I further develop evolve improve my memory or make sure my memory actually be able to authentically predict this is a work model the model is accurate right so you actually have to decode it you can think about the memory formation is a encoding process then from my memory I want to decode I want to predict what's

(42:13) going to happen next second from what I observe right now or at night I may want to dream what happen right so that's actually the decoding is actually allows us to check if my memory serves to be right right how accurate I can predict next step so hence this actually already form a sort of autoenccoding framework now of course autoenccoding if I have access to both the observation and my memory just like our training our big data models I have control on both ends.

(42:44) I can just force auto encoding back end to end, right? The people like to talk about it. But in our natural um in a natural setting, in animal human setting, we don't have control on both ends, right? We only have control probably control of our own brain, what's inside our brain, right? We never really quite have access to measure if my prediction of the 3D world for example, right? the the the frame of the picture is rectangular. Do I ever

(43:14) measure it? Right? You don't have to measure it, but somehow everybody believes, you know, the model is correct. How do we do that? Right? Hence, there's actually a self-correcting process. In fact, you know, this is actually probably the idea goes the idea actually goes back to Noble Veer, right? And how animal be able to correct its error without see cats can capture something very accurately. or even make one single mistakes, they can correct that very clearly. Right? So somehow they're they're be able to build a world model very consistent, self-consistent with

(43:45) the world without actually physically measure their errors, right? So hence this is the idea about you actually loop it back to your brain and close the loop, right? and allows you to constant predict and based on your prediction and your observations check if there's a difference still difference between your predictive prediction and your observation within your brain very if there's error and using that error to correct turns out um this is a

(44:15) work with my student turns out you cannot do this of course the our observation will lose information right why uh we introduce noise or lose loose dimension, loose information. But turns out as long as the distribution of the world, the data, the distribution of the world is low dimensional enough. Even your encoding process, your observation pro perception process is no you this is still doable if the precisely when the distribution of the data outside world has enough structure is highly low

(44:46) dimension and hence your brain has enough degree of freedom to discern any differences. So this is actually quite interesting revelation to for us to realize that the low dimensionality is not just some or technical assumption. It's actually necessary for this kind of closed loop learning to be possible and once you be able to close loop then you actually constantly observe constantly predict hence you can constantly use your memory to predict and correct it.

(45:16) Hence support continuous learning even lifelong learning right our memory Rome is not built in one day our memory is never built in one day we constantly improve it constantly revise it and this is the mechanism of intelligence hence this mechanism is itself is already generalizable hence you don't need to add the adjective general general in front of intelligence there's no point of calling general intelligence if If you implement the intelligence

(45:47) mechanism correctly, it's already generalizable. The knowledge learned by this mechanism at any point of time may not be generalizable. The mechanism does. Right? This is a very big confusion. We think if I accumulate enough knowledge, it's generalizable. No, it's not. Will never be. Any scientific theory by definition being scientific is falsifiable, which means it's limited, right? can only explain the world up to certain point or certain

(46:18) accuracy. There's always room for improvement. The scientific activity, our ability to revise our memory to acquire new memory, that is a generalizable ability. That is intelligence, right? Through natural selection early days, through our feedback control, feedback correction, through the human history of trial and error, imp accumulating empirical knowledge, through scientific discovery

(46:48) is all doing this, right? That is common behind intelligence, right? Not the memory accumulated up to a certain point. So even we manage to memorize the whole world the knowledge we have in the whole world we will no longer be able to apply when we find oursel in a new environment in a new situation observe some phenomena we have never seen before right hence that's the limitation of you know you try to gain general general intelligence through just accumulate enough knowledge right >> we should talk about your crate series

(47:20) of architecture so crate stands for coding rate reduction transformer and you made some very interesting discoveries So for example multi head self attention can be derived as a gradient step on rate coding and also MLPS as spifying um spification operators and and also you were talking about how something like a transformer could be described in a principled way. There there's this interesting thing, isn't there, that we we we designed the well, we didn't even design them. We we

(47:50) kind of empirically tried with lots of different things and we happened upon the transformer. But something like that can actually come about from a first principles approach. If you look at the past decade also evolution of also it's kind of natural selection process for the big models right from early days Alex let l Alex letter VGG or um then reset or uh transformers by the way this is just one of those for survivors right as I said just like le selections right

(48:21) remember don't people don't forgot there's a there's a time there's a very popular areas called AutoAs, AutoML, right? People tends to do random search for better architectures, right? Somehow why only a few survive? There must be a reason, right? They must capture certain structures. They must did something right. Now from our understanding so far, the resonate actually capturing the the fact that each layer should be doing compar doing optimization.

(48:52) The resonance precisely reflect the iterative optimization architecture. Right. And precisely capture fact. We're trying to cluster compress what's similar and discern or classify what's different or contrast what's dissimilar. Right? And you want to develop different experts, right? We call them experts, we call them as cluster, we call them group. So be it, right? And the transformer again, right? Let's capture what is the

(49:23) correlation self attention. is precisely compute what's what is the correlation in the data coariance in the data what's correlated and using that to further spify further classify things to organize the the the distributions they must do something there some somehow close to something right right so also it's almost like a belief for us right if we believe there's something right then we should be able to derive create from first principle have a very clear unified understanding I think we're sort

(49:54) managed to do that at least. So for the law structure we discovered so far provide rather unified explanation to what they have done to be honest the early even maybe our early earliest mo motives try to explain to understand what we have done but once we understood it we realize that we can go much further right and realize even the current architectures there's a lot of room for improvement not only we can dramatically simplify them you can see in the past after the create um in the

(50:25) last yes last year and this year there's a series of work from my group right really just showcase people right uh you can actually you once you understand what is done with the principle you can dramatically simplify you you can even throw with the MLP layer if you only care about the compression um you don't care about the final representation and or you can make the attention head since we know what is optimizing it's optim optimizing the rate reduction object function

(50:55) Then we can find what is the equivalent variation form of that object function which is much easier to optimize. We end up with a we call the toss right the the computing the coariance the the self tension step is only linear in the dimension no longer quadratic like the current tension is doing. Of course if you look at the literature there are other people have found tried to identify linear complexity such as uh you know manga or uh I think there's the

(51:28) rk or something so empirically but again it's through trial and error but this is so now we derive this in the ma purely mathematical way because we just find an equivalent varial form of the same object function they have the same global optimal but it's that's much easier to optimize. This is a trick we do all the time, right? All the tricks see in the 200 years plus years of developing better optimization algorithm. All those ideas can help us

(51:58) now to design better operator descent operators or optimization architectures to improve the design of current architectures. Pro honestly we have not really started that far. Right? There's many acceleration techniques, preconditioning, conjugate gradient which explore different landscape. Once we understand the landscape, the type the cost of object function better there's gazillions of ideas, we can

(52:28) further improve the efficiency. Honestly, we haven't started that far, right? Um I mean that's actually what got some of my student uh excited uh to to pursue this realizing how much how little we have done. um uh from optimization perspective how much room there might still be for improvement uh some of my students are quite excited. So you can see you know Neman within last couple years we already have two two or three different uh uh uh uh generation of

(52:59) architectures that um in the past it's almost unthinkable because new generation always come from different group right it's like a random process whoever gets lucky maybe discover something works try hard enough get something to work >> it's a tantalizing idea though that through this principled optimization there could be you know a convergent evolution towards the optimum architecture >> then the search will no longer be random will be actually guided right there's still just like back to your earlier suggestion right this is become

(53:29) intelligent search is you know guided search we understand the the structure of the problems now and hence we can do science now we are no longer just to do empirical inductive search process >> why do open AI they're still using the transformer even though there are now superior architectures out there and we should talk about this token statistics um transformer. So as you just said it's a linear time complexity which means in principle this is something which is going to scale dramatically better than

(54:00) the kind of transformers we're using now. So why aren't we using it? >> Well there are tempted to try to scale this up. In fact, even you can think about uh uh many of of course when you try to scale the other factor comes in right in terms of whether or not the the the the the scalability and so on is all related to to to all the design. Um and um indeed we actually tried something else. uh you know uh for example things are much more scalable also I also tried to we also scaled up with all the

(54:30) resource we have sometimes I don't know about company right we we are very limited in resource to verify even our those architecture scales we can only do up to probably a you know few you know couple hundred cars and so on that's about it uh with our academic resources and hopefully that will convince but one thing I think recently we did um to simplify the current practice in dino Right? You know the meta has to which is the pre-trained the state-of-the-art uh people everybody talk about world model

(55:00) visual world model and uh that's sort of the best model and the the meta put a lot of effort engineer effort to pre-train the visual representation model uh which is still the best and they train on gazillions of images and they were using contrastive learning but it's a it's a very uh remarkable engineering uh feed and now people using it right um turns thought actually we found that uh the system can be dramatically simplified once we realize what the purpose what they're really trying to do right we have a work called

(55:31) a syn deno simplified the deno version one version two we simplify both version there the architecture is dramatically so we get rid of you know dozens of hyperparameters I have to do and architecture become you know extremely extremely 10 times simpler and the performance is better we managed to scale up to uh probably few few hundred millions of scales the appletoapple comparison were dramatically much easier to train much efficient everything is explainable I think that has seriously

(56:01) draw attention uh from the meta team and also from the Google team and it's currently they are there I I know there are serious effort that they're trying to scale this the new architectures up >> yes um we interviewed the dino folks at the time and we've spoken to people like Ishan Misra there's there I mean there's a potential tangent there about they're using this kind of um uh non-contrastive um self-supervised learning and also there's the whole unsupervised thing and and how useful those representations are for downstream task. May maybe we could

(56:31) go there but I should say that Kevin Murphy I'm I'm interviewing interviewing him soon and um I know that he reviewed your your book very very carefully and he asked me to give you this question. He said code reduction is great but must be subject to prediction or reconstruction loss in data space. How would you go beyond token prediction which seems especially weird for images? So that that's what Kevin asked me to ask you. >> So this is actually a great question, right? Um so in a in a rate reduction

(57:01) remember the the lossiness is actually coded through the the EPS ball where actually uh uh try to capture the uh the the samples how they connect with one another. Um right now we actually if we just minimizing the coding representation uh through this uh lossy coding and the error is kind of controlled by the IPS law but not enforced right so we respect the IPS law um through this loss coding process now

(57:34) to truly ensure remember everything could go wrong and also depends on the number of samples you have maybe the image you choose is wrong because the data does not have that density so you may not be able to percolate. Hence the repetition learn can be very very funky. So now in order to to to ensure that your your your your learn the reputation distribution learn internally actually authentically reflect the original distribution up to certain precision you have to decode right there is a constant

(58:06) encoding decoding actually our brain do that all the time predictive coding and so on so forth and hence that encoding decoding and to verify if there's error remain in your prediction in your recon construction matters a lot only in that now the question is if we don't have the oh do I do we necessary back to our earlier discussion right do we really need to measure that error in the data space in the original token space uh if we have

(58:36) that option so be it do that right make the engineering simpler make the but if we really want to have a system just like a human to selfarn just go out to observe right at with two eyes or with some sensors. Then we have to come up with a way to make sure that our sensing process is accurate enough so that we can do everything internally. We can predict go back rapid back and observe compare what we predicted and what we

(59:07) observed through same sensing channel. We compare that locally. In theory actually prove at least under ideistic cases this is possible. We can minimize the error once we correct the error. Hence the internal representation will the error in a token original data space will diminish but under tech condition under general condition we still don't know. So we actually have a paper prove that for the when your data

(59:38) distribution is a mixture of subspaces you can rigorously prove that's possible and if the dimension of the subspace is low enough compared to the capacity of the perception process. Now for general distribution we believe this is true. This is actually how we'll be able to learn all the low dimensional dynamics structure in the natural data in the motion in the in the predicted um world. So I think this is something in the future we can deduct but end to end

(1:00:09) works if you have the option to do so or if you have you don't have that option you have to figure out how to do this autonomous under what condition you can do this autonomously and allows you to do autonomously to reduce the error to almost zero. We spoke about Dino, but another example would be VIT. Um, we interviewed Lucas Bayer in Switzerland earlier this year. So, he invented VIT. And if I understand correctly, crate is now very very close to VIT, but it it's so much more principled. It's

(1:00:39) explainable and so on. How close are we to knocking VIT off the off the leaderboard if you like? >> In fact, u I think in many of the comparison, we're already very close. If you compare, it's hard to compare apple to apple, but in terms of if you similar the parameters, we're very much on par. And also, by the way, we never really quite put much engineer effort into it. We just want to verify the concept. Um indeed there one thing come out of the vi the crit is that uh what we find is

(1:01:10) not only the architecture designs principled but then once we did the training right the internal structure learned are both semantically statistically and geometrically very meaningful indeed each head actually does learns uh you know similar structures all gets basically each channel each head be truly become expert of certain certain type of visual patterns for example legs of animals ears of animals faces of animals so we see that very clearly with

(1:01:42) crates but we don't observe that in the vit of course you can see vit may learn this is actually the the the interesting thing right early days people I'm sure large models if they have redundancy they definitely learn things internally but it's very hard to say which part of that network learn the correct channels learn the correct operators because it is embedded in a more some redundant structure, right? So early days people call this lottery uh you know lucky

(1:02:12) lottery or lock lottery ticket right it's somewhere in there right then then people try to distill that that justify you should distill you should actually be able to compress even people do this Laura thing you know all the post-processing right justify that is necessary and some people you find after the post processing not only the network become smaller the performance gets better right but and so on so forth Now probably we don't have to do that. At least you know the the architecture does what it's supposed what it's designed to

(1:02:43) do right and we can actually at least explain what each component is doing something statistically geometrically very meaningful and there also results if there's enough data if your optimization uh is done training is successful that's those structure pops up naturally the the structure will do what they're designed to do. And and final question, many um ML engineers and researchers watch the show. Given everything we've spoken about, how can they find out more

(1:03:14) about your work and how can they get started building these kind of architectures? Um I think most of our actor are open sourced on GitHub um including create uh uh early reduet uh there may not be is conceptual but not very practical uh create and also even toss all the all the codes are available but by the way they're sort of kind of academic implement we don't we never be able to have the resource to scale them up most are scale up to GPT2 or image at

(1:03:45) 21 that that's we can afford deno Simply den is the one we scale the most. We we res exhaust a lot of resource and a little bit higher than that but still no comparison to all this industrial scale at all but I do believe that the meta and Google are doing something about dino simplified dino and the codes are there um and also of course if for the methodology of course um this is one way why we bite the bullet uh uh to uh wrote the book in the past two years we

(1:04:16) believe that although there's a series of papers uh but we believe that for people to get a big picture, the more systematic introduction. We put together the the books also we open sourced it um and we we will post link all the data all the code as well. We are also teaching the course. So all the we actually will have students uh practice most of the new architectures method. So all those codes will be made public available and so I think that might be a good entrance if you want people want to

(1:04:47) learn the methodology understand the theoretical chain of evidence and also even the empirical chain of evidence and I think the book is has attempt to do that we are already start to organizing we're not done yet but we are start already organizing if you find of chapter seven we're already doing that uh in chapter seven to collect the theory seriously to to all the real world data and the task such as image classification, image segmentation and

(1:05:17) the pre-training and even language uh GPT2 type scale language models and so on. Yeah, >> professor Mo, it's been an absolute honor. Thank you so much for joining us today. >> Yeah, thank you very much.