===== Video Information =====
Title: How Arm Enables AI to Run Directly on Devices
Channel: Eye on AI
URL: https://www.youtube.com/watch?v=sYFxKyCO1u4
Duration: 51:38 (3098s)
Subtitles: 1212 entries
=============================

(0:00) The ARM architecture has existed now for almost 30 years and that started from early investments from companies like Apple, early adopters of the ARM architecture that made it the stalwart that it is were companies like Nintendo, companies like Nokia back in the 80s and 90s as these smartphone revolution all that kind of stuff really started around ARM and then that's what's driven us to be where we are today. We have big CPUs and little CPUs and we're actually moving the workloads back and forth because certain times you need the performance, certain times you don't.

(0:30) And so that's really the way these devices work where you to your doorbell example, you're looking for maybe some motion, you're looking for something and then once you trigger that event, okay, now let's fire up some more computing elements. >> In business, they say you can have better, cheaper, or faster. But you only get to pick two. What if you could have all three at the same time? That's exactly what Cohair, Thompson, Reuters, and Specialized Bikes have since they

(1:00) upgraded to the next generation of the cloud, Oracle Cloud Infrastructure. OCI is the blazing fast platform for your infrastructure, database, application development, and AI needs where you can run any workload in a high availability, consistently high performance environment and spend less than you would with other clouds. How is it faster?

(1:30) OCI's block storage gives you more operations per second. cheaper. OCI costs up to 50% less for compute, 70% less for storage, and 80% less for networking. Better in test after test, OCI customers report lower latency and higher bandwidth versus other clouds. This is a cloud built for AI and all your biggest workloads right now with

(2:03) zero commitment. Try OCI for free. Head to oracle.comonai. Ionai all run together. Eye o n ai. That's oracle.comonai. So >> it's great to be here Craig. Thanks for inviting me. Um, so my name is Chris Bergie. I'm a senior vice president, general manager of the client line of business at ARM. Um, and that means I

(2:33) very much focus on all of the rich rich edge devices that ARM um, is so prominent in. So, um, things like smartphones, but also we're obviously making quite a bit of inroads into things like PCs. uh we participate all parts of your house whether that's your TVs, your smart speakers, um all kind of rich endpoints that that you have powered and and are quickly becoming AI enabled and I think that's what we're going to talk about today. Craig, um just a little bit about my background.

(3:03) I've spent almost 30 years in semiconductors um various big companies started out of school at AMD and and and moved to Broadcom for almost a decade and and also did some startups in between. So, uh, a long I can't believe it's gone by this this quickly. But, uh, I guess, you know, semiconductors have never been this cool as, uh, it seems like governments and, uh, everyone really cares about semiconductors. So, I guess I I was very fortunate in my career choice. >> Yeah, that's kind of funny, isn't it,

(3:33) how things happen that way. Uh, yeah, and it's uh, and particularly uh, semiconductors on the edge. That's like the the where everything's at right now. Uh so ARM and and we're talking about chips for AI inference. Uh so ARM's uh ARM version 9 edge AI platform has a couple of new uh processors uh designed to enable complex AI models.

(4:06) Uh, can you talk about those? Uh, or is there something even more recent that you want to talk about? >> Well, I I think that's a good starting point, Craig. And, you know, and and we've actually been on quite a journey um, you know, in the edge evolution, not just actually at the sorry, in the AI evolution, not just at the edge, but also um, we're actually a big part of the infrastructure buildout as well in data centers. it, you know, really leverages a lot of our business model and capabilities and and so we're doing

(4:36) quite well there as well. Um but but I I'd like to spoke probably most mostly about the edge today. And so we we did create we did a Lumix um our Lumix platform um which is really around kind of high-end smartphones, high-end multimedia experiences at the edge. Uh we launched that back in September and already there's uh several products um chipsets that have been launched and now phones that are being launched by um different leading phone manufacturers

(5:07) based on Lumix. Um and what's cool about it is it is the first um it's it's it's addition to the platforms of V9 as you mentioned that starts rolling outme which is the matrix extensions to the V9 architecture um and and starts pulling that into the ecosystem to allow people to um really take advantage of AI at the edge in these devices and do so with the

(5:38) traditional CPU programming model. Um so obviously there's a lot of discussions around accelerators. Accelerators are great um and and are quite dominant in data center um and are also finding uh their home in the edge. Uh but it's it is always this balance of you know accelerators have gate great metrics around maybe tops per watt. Um but they definitely are a bit more challenging to program and versus let's say a CPU. And so you really got to get the reality is

(6:10) we we talk about heterogeneous computing because I think the answer is everything right I think you know in the AI world it's about CPU capabilities it's about GPU capabilities it may be about dedicated accelerator capabilities and then it's about memory bandwidth um you know quite frankly um and really AI is putting stress on that whole system >> okay and let's back up a little bit for listeners that are not deep in the chip space. Uh uh and I' I'd like you to talk

(6:42) a little bit about V9 and what that is. uh but uh when when you talk about heterogeneous uh computing what you're and and and the programming language uh Nvidia one of the reasons it has such a strong position is very early on it developed a very uh userfriendly programming language called CUDA and and everyone has adopted that uh for uh

(7:13) programming AI on to uh GPU uh processors and that has become the standard and so a lot of new chips are arriving but you need to learn a new programming language uh and that's a barrier but uh you in this heterogeneous particularly at the edge uh what I understand is that you still use a a traditional processor with a programming

(7:43) language that you're familiar amilar with and then there's some sort of a conversion that sends some of those workloads to the edge uh chip uh and it it could all be packaged together but is is that right? >> Yeah, I think so. You you you you got a lot wrapped up into there. So, let me just first start with V9. So, um the ARM architecture um has existed now for almost 30 years. Um and that started um

(8:13) from early investments from companies like Apple. Um early uh early adopters of the ARM architecture that made it um the the stalwart that it is were companies like Nintendo, companies like Nokia back in the you know 80s and 90s as as kind of the these like you know the smartphone revolution all that kind of stuff really started around around ARM and then that's what's driven us to be to be where we are today. So V9 is is is an architecture that we launched about five or six years ago now um which

(8:44) was the next generation of architecture um which was focused on a couple different things. One was security um another was performance and third was AI and um you know we are very much at the forefront of looking at what these next generation systems are going to require. Um, and so that's really what V9 is about. And at this point in time, um, you know, a large percentage of both iOS and Android handsets, um, ship with V9, um, CPUs. Um, and you'll see that

(9:16) continue to accelerate, um, over the next couple years. And so and now V9 also is getting perforated across the other markets that that ARM participates in whether that's data center whether that's automotive um and then kind of AI IoT as well. So that's that's that's V9. Um so let's talk about the the programming uh comment that you made there. So you're right CUDA is an amazing language. Um, I've actually had the pleasure of working very closely

(9:46) with Ian Buck, who is the I guess, you know, who basically was the Stanford student that really kind of started this and has obviously now uh, you know, has been a a very important part of the NVIDIA story and where they're at today and still still is there today. um you know but and and it is um a great program for you know basically programming um GPUs and especially taking care taking advantage some of the capabilities um in the accelerators there but you know it is a accelerator

(10:17) language right um versus a CPU language and um you know what people do is that you know you basically have to start making things like driver calls and you have to move your workload off of the CPU to that now it makes a ton of sense if you're going to get, you know, I think, you know, Ian and I actually used to work on on quite a bit of um high performance HPC stuff for high performance computing, big government labs, and we would have these kind of rules where it would almost want to have a 100x uplift when you kind of move that workload over. Maybe at least you'd have

(10:49) to have 10x, but you'd really want to have that versus the convenience and and and keeping it on the CPU because you really need to load that and then you stream that workload on an accelerator. And so that's kind of as you think about this heterogeneiality of you're really moving these workloads around based upon, you know, what's required. Is it latency? Is it, you know, is it high performance? Is it lower power? All those kinds of things. And so this is, you know, what I would say the ecosystem is doing, what ARM's focused on is we're

(11:19) really focused on making all of those pieces, the CPU be as performant as possible on AI workloads and be super developer friendly, which it is. also moving some of those workloads to GPUs. ARM is actually, many people don't know this, but ARM is actually the highest volume GPU out there. We've shipped over 9 billion um GPU cores um if you because of the mobile handsets and so much in the mobile industry that's based on ARM GPUs. Um and then also in support of accelerators through um ARM's extension

(11:51) of our uh architecture and so many of those accelerators hang off of things like chai buses or um that are kind of part of the ARM architecture. >> Yeah. And and when you say uh the ARM ships GPUs uh where does the uh uh NPU sit in in fit into all that? >> Yeah. So, so in a mobile handset, let's talk about that since that's when we started. Um, you know, there are, you know, traditionally there was two large

(12:21) computing elements or, you know, there was the CPU and the GPU, right? Um, and their main function CPU was to run the OS and then eventually apps. The GPU function was to display and and play games and and and do all of those kinds of things. Well, as these other workloads become important, whether it was, you know, camera imaging is obviously become super important and the amount of video that you take, well, then we start putting little accelerators that are maybe doing um

(12:51) some of the compression for the different video codecs and and those kinds of things. And so MPUs have kind of evolved um as a way to efficiently do matrix multiplications or CNN's and different kinds of models um in as an accelerator. And so the system can choose to send that um but what really happens in the real world and this is the heterogeneous is you actually end up kicking off the job usually on the CPU.

(13:21) You may actually run it to the GPU. you may send it to the MPU and then it comes back to the CPU usually to kind of conclude and so that's tends to be how these workloads actually work in a system. It's all uh obviously not seen by the user. Um but it is the developer has to make some of those decisions as a software developer thinking about you know one kind of capability is in the handset. They're also making the decision of do they want to do it on the cloud or the edge and we should have that conversation as well today. Um, so

(13:51) these workloads move around and there's a whole set of reasons that they move around, but um, we're making the the CPU at the edge be as AI friendly as AI performant um, and power with with a good power envelope as possible. >> Yeah. And are these all packaged together, the CPU, GPU, and NPU if you're using the NPU >> or are they Yeah, >> it depends on the system. um in today's um cell phone chips, yes, they are largely all in a single SOC. So all

(14:23) these computing elements exist in an SOC. Um as you get into, you know, things that have let's say larger, you know, larger battery windows, you know, in PCs, many of the NPUs are integrated in the SOC. We are seeing trends around people adding accelerators. Obviously, people are also using the GPU that's in a um in a in a um laptop or something like that. And again, that may be integrated. It may be um discrete. Um I

(14:53) would say there's a trend towards integration in many of these markets. And um that is because of the memory pressure and the memory size requirements for for AI. And so I think if you look at some of the latest, you know, I'm going to say put it on your desk kind of computing platforms. Um we're especially proud of our partnership with Nvidia on the the GB10, the the product they've actually just started shipping last week. Um it's puts, you know, I I think um Jensen

(15:24) likes to say it puts puts a supercomputer on your desk, right? where they're actually um providing I think it's a pedaflop of AI performance um on your desk and it is because of you know you've got these ARM CPUs again um you know 20 of them coupled tightly to this accelerator with a single memory system um that offers up to 128 gigabytes of DRAM and very high bandwidths and so you've got this whole system that you can put together uh or that developers

(15:54) can use and and we're seeing this as a trend And whether you look at, you know, the latest V9 M5 that Apple just announced uh this week as well or I think it was last week. Um again, you see amazing AI performance leveraging the ARM CPUs, uh GPUs and other accelerators and then a tremendous memory system um for for that. And so that's really what we're seeing happening. And those are integrated. But some of them as they get a little bit bigger, they get discrete. Yeah. But the problem with discrete is then you have

(16:24) to split the memory system and it it gets it. >> Yeah. >> AI is so memory heavy that it just it's a it's a whole balancing act. >> Yeah. And again I'm mindful of listeners who are not uh deep in the chip space. SOC is system on a chip and that's where you you combine uh different kinds of chips into one. From the consumer's point of view it's one looks like one chip, right? It's all compressed in there with

(16:55) >> a cover. U and the uh the reason this is important is that increasingly I mean right now I have an app that I built for myself. When I drive around it talks to me about the history of the places I'm in. Uh, and and it the voice drops out a lot of time because it's got to, you know, read my GPS and send the data to

(17:25) the cloud and then, you know, do a a lookup in in the model, whatever model I'm I'm using, you know, get the the text from the model, convert it to speech, send it back to the the uh phone, Maybe the the text to speech is on the point. I'm not sure. But but that whole chain uh gets there's a million ways that it can get interrupted. And if

(17:55) you have that all happening on device, you don't have those problems with connectivity and and things like that. Can you talk about how you see uh you know V9 or or these uh these SOC's uh changing the way we uh we interact with AI? >> Yeah, absolutely Craig and and you're you're at the forefront here. I it um you know it's it's it's it my job's

(18:26) super fun because I get to talk to many of the industry leaders and and visionaries whether that's both on um you know companies like Google that we work so closely with and all the chipset companies and and then all the the um CE companies the consumer electronics companies that are actually putting these things in your hand. Um, and you know, it's it's it's fun because you know, I always I'm an I'm an edge guy. I think, you know, I want edge computing. That's kind of that's the business I run. And so, you know, I you know, of

(18:56) course, I want edge computing to happen. And so, but I I do often go to these partners and say, hey, you know, okay, why can't you run it in the cloud? You know, why why can't we do it? You know, it seems to work just fine. People love Chad GPT. They love Gemini. You know, they're starting to really utilize these services. And you know they they basically said you know what what gives me reassurance is all these companies are like no no no we need to put these in devices and and and one of the reasons is what you just said Craig which is you know we the what

(19:29) the goal of AI is that you know we're seeing some of the early cool use cases real time translation um you know some some smart some agents that are starting to do some things but we're we're in the early early innings here. And you know, one of the analogies that I like to use is um touch. And if you take a child, let's say less than 10 years old, and you give them a screen, they just start touching it because they

(20:00) don't know anything that's not a touch screen. Um and and you know, whereas you know, you and I, you know, like the mouse was a big thing once once we you know, we thought that was cool. Um, but and I and I use that analogy because that is how AI is going to be. If something doesn't have AI and you can't interact with it and it can't start figuring out what you're trying to do, it's going to be that like that child that says this thing's not have a touchcreen. I don't know how to use it. I don't want to use it. Right? And so,

(20:30) so first off, you need to just believe that that is that is how essential this is going to be. It's it's the you know it's how annoying it is when it takes a long time for the app to boot up and it's how annoying it is if you have an app die or you know those kinds of things that we we work out the edges and they don't really happen as much anymore but as technolog is new and so it's the same thing with AI where you know in talking to one of these partners you know they said hey Chris we you know okay yes maybe the AI in the cloud works

(21:02) great you 90% of the time, but you know, you're driving up 101. I'm I'm here in in in Silicon Valley in San Jose. You're driving up the highway, there's a dead spot. And you know, and basically, you're going to get a bunch of latency that you know, it's not going to become conversational. You're going to be waiting. Um, and the reality is is that people aren't going to comp complain about to their carrier saying, "Hey, you got this dead spot on 101. When are you gonna fix it?" They're going to say,

(21:32) "Hey, Craig's app. I don't like that experience. It's frustrating. It doesn't work. Hey, you know, three times a day. I'm I'm trying to use it." You know, it's it's like doing these video calls, right? When it doesn't take long for somebody to have a shaky connection, you're kind of like, "Hey, let's just talk next week or let's talk [laughter] in the office, right? It's just too frustrating." So, that's just one example. There's privacy, there's performance, there's all kinds of other things that is um that really is going to drive this to the edge. the the counter side is the models have

(22:04) to get smaller. there is a real cost because of the computing and the memory computing um pressure it puts on it and so it's a balance but it is happening um and again training and there'll still be quite a bit of inference that'll happen in the cloud but as much as we can move to the edge um I think it's pretty unilateral whether you're a hyperscaler whether you're an app developer whether you're a device manufacturer all are quite incentivized to try to make it run very very well on the edge

(22:35) in the device. >> But there are challenges and there are trade-offs as you said. Uh one is uh that uh there's a power issue uh and uh a heat issue as a result of that. Uh how do you manage that? because and that was I mentioned this company Brain Chip I had a conversation with that's using neuromorphic chips that only uh fire when the when

(23:08) there's enough activity to wake them up. So, for example, in a in a doorbell camera, you don't [clears throat] want the doorbell absorbing the the the video, sending it to the cloud, computing, and sending it back when when there's nothing happening in the scene. But that's in fact what happens. Uh, how do you manage that? >> Yeah. So I I think that I mean we we we

(23:38) have these management techniques that get used in you know in almost every aspect of a a computing device right I mean we get we have very clever engineers who figure out how to kind of fool you that the device is running full power yet we're we're obviously very aggressively changing things in the background and that that was actually one of the ARM innovations um over 10 years ago we came up with this big little concept around we have big CPUs and little CPUs and we're actually moving the workloads back and forth

(24:08) because certain times you need the performance, certain times you don't. And so that's really the way these devices work where you to your doorbell example, you're looking for maybe some motion. You're looking for something and then once you trigger that event, okay, now let's fire up some more computing elements. Let's go figure out, oh, it's a face. Okay, is that a face? We know we don't. Let's let's trigger the cloud or whatever. And that's the way these systems work. Um so you know I think it is about um really these more

(24:38) intelligent computing platforms um and getting smarter on how you do these accelerators and that's you know that's for example our our the theme I mentioned that's part of this theme 2 that's part of V9 um it is this kitely coupled accelerator but we do it in a very very efficient manner um where it's actually shared across multiple CPU cores so you're also being very area efficient um which really drives cost. Um so I mean those are things but the

(25:08) other thing is there's tons of innovation. Um I I again don't want to get too technical but your your your uh listeners maybe have heard about HBM right HBM memory is stacked DRAMM with a very high bandwidth interface and it's become a huge en enabler for um AI in the cloud right because we have to have all this memory bandwidth and we put these HBM stacks next to these accelerators we're looking at very similar things um at the edge relative to you know if where where is the power,

(25:40) right? Is it in the computing element? Is it in the jewels per bit of memory transfer? All of these things are opportunities for innovation and there's a ton of research and a ton of investment that's going into them. So I I don't worry about the power. I actually think the power is fairly manageable relative to uh inference. I think the bigger pressure we have right now is actually memory uh memory size. Um so the model size and um and how how

(26:11) big it how how do we make it small enough because at the end of the day that drives um it drives cost it drives power and other kinds of things. So it it's happening and the good news is we're in this innovation cycle where you know there's two things you can do. You can shrink the model and get the same performance and and that seems to be shrinking you know almost you know 50% a year if not faster than that. And on the other side, you can say, hey, I want a three gig, you know, gig, three billion

(26:41) parameter model, and it's just getting more and more intelligent every every six months, every year, right? And so there's different ways to solve the problem, but this is that's really the enabler, I think. >> Yeah. And again, I'm just thinking about listeners so that so we don't lose them. is scalable matrix uh extension and that's a a way to optimize the the math operation correct uh that's that's used >> um and uh are you you know on the models

(27:13) uh are you guys exploring state space models uh that optimize memory usage I mean that have a different uh way of handling uh uh memory Um so I'm not as familiar personally with states spate. I I think I understand the general concept of it. Um I would say that you know we provide this platform that many of these innovations sit on top. So you know many

(27:43) of the um what we've basically done with for example our our matrix extension engine that you just talked about uh we basically built what we call clidy framework on top of it which is these libraries that now developers can basically leverage so it just does the right thing in the hardware relative to taking advantage of which you know do you have the latest RMV9 etc. So a lot of those how the model works and states fates and and what you know are they

(28:13) using a KV cache or how are they doing the different updates those actually sit at a little bit higher level in the stack where we're kind of more of a I guess a plumbing enabler is kind of the way that I would say it. So we we support all of that and we're making it super easy for developers to um to explore that and and to build their innovations on top. But there's nothing unique at least at this point in time that we're necessly doing from a state state point of view in in our CPUs >> and can you talk us about some of the uh

(28:45) the current applications uh and where you see this going this move to the edge? Uh I mean obviously uh you know uh self-driving cars is is one where you you can't afford to risk connectivity and latency by sending data to the cloud. It's got to be computed on uh on the platform or on the on in the car. uh what are some of the other

(29:16) applications that you're currently uh putting chips into uh and or current devices I should say and and where do you see that going I mean how another the form factor of these chips how small can they be uh so that they're they're sitting I mean I was talking to a guy the I wear hearing aids uh that uh you know you'll be able to have these chips in the hearing aid uh you know filtering

(29:48) or or isolating sound and that sort of thing. Uh so can you talk about that a little bit? >> Yeah, I mean that's a great question. I I mean I think the scalability is almost everywhere, right? I mean your your hearing aid example is a good example and and and this is one of the reasons why ARM builds the the huge set of portfolio that we do because we are in many of those those hearing aids and those kinds of things and we've we've actually announced AI across I mentioned

(30:18) Lumix but we have our edge AI platforms that we've also announced back in February. So we are enabling that. Um and it um it it just comes down to if you know the task you are trying to do you can make things quite small and um and efficient right so in a hearing aid right in a hearing aid you're probably not trying to run a large language model not yet at least um you're doing things like trying to do you know reduce noise

(30:48) or picking out you know amplifying only the the interesting you know what what are you trying to hear. Now, obviously adding translation and those kinds of things are probably going to be possible soon. Um, now if I know what language I'm translating to, then I can, you know, that going to make my model smaller. Uh, but yeah, everything, um, you know, one of the things that I like to think about is how much, you know, we use apps today to configure things. Like a good example is like a security camera, right? So today you probably you

(31:20) know when you try to install your security camera in the past you would have went to your laptop and and connected it. Now probably use your smartphone to do that. Well, you know that is a use model. We've gotten used to it. But but why can't you just do it with your voice? Like why can't it talk to you and have the the camera have a large language model and you say hey it asks for what is your SSID? Here are the ones I see. And you know now again that may or may not be a better user experience but we just see the applicability across the board. Again I

(31:50) I use that touch analogy because touch has become so prevalent. AI is going to be way more prevalent than even touch is today in changing the use model how you interact with these things. Um you know a a great example I'll give you Craig is we work very closely with Meta um who is is is really doing a great job. I think with with the you know some of the advances in their glasses, right? So the the XR glasses um and their and the

(32:20) product they just announced two weeks ago, three weeks ago um they they now have a wristband that goes with that product and and that actually uses one of our Ethos uh NPUs which is again super small, super low power that you can have this wristband that basically you know has a huge battery life. I forget how many days or weeks you can wear it. And it literally just, you know, you manipulate things just by moving your fingers like this. And it senses in your wrist the changes and

(32:54) what's happening, you know, below your skin using AI to basically figure out that, oh, you just did your second finger, so that's going to do this or you just did this, so that's or, you know, you're doing this. So basically that is the new UI of a wristband that has AI in it that has you know a long long battery life and has a tiny tiny battery. That's that's how small we can make AI for very specific use cases. >> Yeah. Uh and and what's the focus uh

(33:27) with with ARM right now? Is it reducing size? I mean obviously it's going to be all these things but but where do you see the next breakthrough? Is it reducing size, reducing power consumption, increasing the model size that can can be uh on these chips? Uh yeah, where where do you see it going? [gasps] Um, it's a great question, Craig, and it

(33:57) it is a it is a a very open design space right now to be honest with you. Um, you know, definitely there has been a focus on power in the past and that's kind of where our ethos product line comes from and but a lot of that was around CNN networks and some of the early stuff. Now you're starting to see the move to transformer networks. What's next? Right? I don't think that transformers are actually the end goal. And so we also need to allow flexibility as these models change because you know back to

(34:29) kind of educating your users. I mean it takes two years almost at a minimum to design silicon and get it in a shipping product and you can see how fast AI is moving. Um yeah >> and so there's also this flexibility piece but I I think most of it is performance. it's really memory performance and it's um and it's tops and and CPU performance and then um and and trying to shrink that down as as as the best we can for for whatever the

(34:59) power envelope is to um you know how much computing you can get and I think that's obviously happened in the data center as well right where it's okay we're trying to get we're now building gigawatt data centers but how much how you know how many tokens can you create How much performance can you get in that? So almost everywhere in the space, it's like, okay, tell me what your power envelope is. It's a gigawatt here. It's, you know, seven six watts in a phone. It's, you know, in my wrist example,

(35:30) it's literally, you know, a couple hundred mills. You know, how much AI performance can you get? And so it it really is spanning a a large um space. >> Yeah. And you guys don't uh you're you're producing IP, right? You're not uh building uh you don't have a fab. You're not uh uh sending your designs out to a fab to make your chips.

(36:00) You're licensing this to other chip makers. >> That's correct. Yes. Our business model is we provide IP to uh you know the a large portion of the semiconductor ecosystem. Um and um and then our partners build on top of it and create uh create silicon solutions that can span like we talked about anything from you know a wristband or or your hearing aid to uh you know your next self-driving car.

(36:30) >> Yeah. Yeah. And and where is most of this? Um I mean obviously most of it's going into the US, but uh do you have partners that are are licensing your IP? Uh where what are the other markets that you're operating in? >> Well, you know, I would say that traditionally semiconductors have been quite global, right? um you know that the the cost of developing a

(37:00) semiconductor is tremendous right um obviously the foundry cost and I'm not even saying like put aside the foundry costs um but literally building a chip um you know if you're example if you're using u one of the latest nodes um just the mass costs alone of of when you're done your design and getting that manufacturer is in the tens of millions of dollars um the development costs up front can often be in the hundred million plus range. So you're talking

(37:30) about tremendous amount of cost to get to the first unit. Um but what's made semiconductor so great is that you can scale that and because we have these amazing manufacturing capabilities etc. And so they tend to be very large markets and so generally it's been a global market for us. Um and yes, um you know, obviously, um there's many great IC or semiconductor companies in the US that do designs. Um we work very closely with with European semiconductor

(38:00) companies, uh in Taiwan, in Korea, um as well. And um we also have had a um armed China, a JV where um semiconductors get built in China based on our IP. And so it we get uh it's pretty global market. Yeah. Uh I would imagine the Chinese uh relationship has been strained by the sanctions. How does that affect you guys? >> Well, yeah. I mean, we always make sure

(38:31) we're following the local uh laws and obviously where we're located. Um most of the constraints have been around the manufacturing process and not necessarily um some of the IP. there is some around IP enablement but um you know it's something that we always are very careful about make sure we're we're complying to what is required um and uh and we'll see as things evolve >> in that that you guys are uh are primarily selling IP uh do you work with

(39:04) a large developer community and what's that relationship like uh is how much of this is open source for example and Um, and then if you could talk about where you see I mean you mentioned wearables where wearables are going to be a hot thing where you see the edge really exploding the public u uh interaction or experience with AI.

(39:34) [snorts] >> Okay. Yeah. So I mean let's let's start about developers. So you know ARM is um you know developers are super important to ARM and in fact we now have uh the world's largest developer ecosystem. We believe we have over 22 million software developers that are developing software on ARM. Um and that's just because of our our footprint um that spans all the way from you know iOS to now Windows on ARM and then of course things like um

(40:04) Chrome, Android and Linux. um you know we've been a longtime supporter of so um you know really the the world's largest software ecosystems are um quite uh you know work work closely with ARM um and we we support them and uh and we really focused on that developer experience and and how do we improve that developer experience I mentioned uh CID before in this in this conversation that's really about again trying to make it seamless for developers to use AI because you You

(40:36) talked about CUDA AI is not um simple from many of the programming elements and I would say we're still seeing not necessarily the hardware abstraction that we've gotten other software ecosystems to. We still see a fairly tight coupling to today's models to actually the hardware that they run on. Um whether that's the operators they support, whether that's you know the way they kind of assume the model is going to propagate and those kinds of things. So um you know but we are we're clearly

(41:07) focused on the supporting those developers and uh and and seeing what they can build right and I think that kind of goes to your second question of you know what's kind of on on the forefront uh and that's why what makes my job so fun because I get to to to to interact with so many of these super innovative folks u both startups and establish established companies on kind of what is next or what what's the art possible. Um, and and if I was just to

(41:37) use a general concept, it would just be intelligence. The idea of something is capable of interacting with you in a way that exceeds your current expectations, right? And you maybe I'll use another product example. Um, I'm a huge Amazon fan. Um, but one of the things I've noticed is that before I upgraded to Alexa plus, you know, because of the way I was using a chatbot or chat GPT or

(42:09) Gemini, I started talking to it in a in a more conversational manner because that's what I could do. But then when I would come home and use Alexa, it wouldn't necessar it was expecting more of kind of a search kind of give me, right? you know, really break it down pretty, you know, I'm giving you three words and try to contextualize those three words, you know, and so and I think, by the way, Amazon's done a great job with Lexa Plus and I think they're they're on the right path here. But that's just a change in expectations and I and I think that's going to only go, you know, just through

(42:41) the roof relative to, you know, the idea of, you know, the things that we think are how you interact with a device for it just to be smart. Um, you know, one of the things people ask me about is like what's a gentic AI or how's that going to feel, right? And and one of the things I like to use and I think I think Microsoft has done an amazing job of really integrating AI into their products as large as a company they are, you know, the way they push copilot, all

(43:11) those kind of things. But what I like is a is a a silly example of settings. So, when you use Windows in Windows settings, we've all done it and you have to figure out, okay, I want to add a screen or I don't like this resolution screen. How many like windows do you have to click in to kind of say figure out how to make this change, right? Well, they've changed it to be more of an agentic experience in Windows 11 where you just say, what do you want to do? I want to add a screen. I want to

(43:43) reduce this. And it just says, okay, well, here's how you do it. or I'll do that for you. And so, you know, to me that's a touchyfey thing of how many times do we, you know, click through three apps. Oh, I'm going to cut this now. I have to go to my flight app. Then I have to go to my rental car app. Then I like, you know, so many of these things that we're just trained to think like, oh, it works good enough. I you know but like this idea of things are just so smart that you know when you call an Uber it of course knows where you're going to go and it's already told

(44:13) people when you're going to show up and and all these kinds of things. So you know those are and of course those are consumerry examples. There's huge amount of enterprise examples of you know teams that are not you sharing data or people don't understand or they're looking for information they can't find. Um, and so I think it's just going to be the general idea of just how do we make things just super smart um, and really kind of exceed expectations of what technology limitations are today. >> Yeah. Yeah. And robotics is is another

(44:46) one. I'm not a great believer in humanoid robotics. I think we're a long way from that. But uh what you're describing I mean it's u you know the day will come when uh our children or grandchildren will will look back and can you believe that you know any object that you use daily uh was a dumb object back then instead of being able to talk to it and and have

(45:18) it like configure itself or or whatever it is. And that that's pretty exciting. What about robotics? Uh I mean obviously you guys uh are involved in that. >> Yeah, I mean it's pretty amazing the what's what's happening there. Um and the ability to train the robots in virtual worlds as well as real worlds. Um and we are a huge believer in that and and and we're quite involved with many of the ecosystems. Um, you know, I

(45:48) think it is really taking the advantage of all, you know, the camera sensor has been an amazing enabler, right? If you just look at what our cell phones are able to do in capturing images, but really the the camera sensor has now unleashed this idea of physical AI and how do you, you know, because we can use vision systems and AI is so good with vision systems, right? whether that's looking at an MRI and or you know maybe and and augmenting a doctor's reading of that or in a system and trying to

(46:18) understand what's going on. So it is it is a huge area. I think we're going to see a lot of um innovation. I think you know to your robotics example you know I think I don't know go back to the Jetsons or whatever. I mean people said we're gonna have flying cars and they were going to be able to do all these autonomy things. It's taken a long time, but you know, another great ARM powered device, my Tesla, you know, it has gotten pretty darn good relative to self-driving and and and I really enjoy

(46:48) having that augment me in in many, many scenarios. And so, it is, like you said, it's going to just be an expectation and you're just going to be disappointed when it doesn't do something that they clearly can't do today. But these things are hard. I mean, you know, I think that that blend of physical um and and you know, is going to be a hard thing, but like anything, it's probably, you know, I go back to I was involved with Bluetooth in the early days and and you know, oh, it's going to be amazing.

(47:19) You're going to have all these Bluetooth devices and just and and it it didn't work great in the beginning. The experience wasn't great, but just look at how fundamental Bluetooth is today. Again, when I walk up to my Tesla, it just unlocks. It just like it locks. and that's all over Bluetooth and augmented by UWB or you know so it's it it's just these technologies you know sometimes the hype cycle gets a little bit ahead of expectations and that's that's our fault as technologists but at the end of the day the technology delivers and uh and it's going to be an exciting future for um our children and

(47:51) grandchildren. >> Yeah, absolutely. And yeah, with moving into physical, the challenges aren't really uh the AI, it's the it's the mechanics, the actuators and and the the wear and tear and the dust and the grease and all that stuff that people kind of forget about. >> Uh that that's that's it's still got a long ways to go. Um yeah. Okay. Uh so so

(48:24) is ARM on the verge of announcing anything uh new or is this uh is it really the uh the V9 that you're >> well you guys are focused on >> we we've got quite a broad product set of products right so yes I you know I have my responsibilities but we have a whole automotive and and robotics group we have a whole group that focuses on data center same thing with edge AI and and additional intelligence So, uh, I think you'll keep seeing quite a bit

(48:54) from us. Uh, lots of great stuff going on and, um, and again, we're just super excited about the partnerships that we get to partner with and how much developers, um, and hopefully some of your listeners are inspired to, uh, build things on top of the ARM architecture and uh, and help make the future what it's what's what's show us what's possible. >> Yeah. And and on that note, [clears throat] we'll end on that. uh developers if they're interested uh do you have a a portal developer portal?

(49:26) >> We do. developer.arm.com. Um great great resource to uh to find out more figure out how to um you know build and and uh and explore many of these new technologies. Um obviously many of the makers are familiar with Raspberry Pi. That is a uh also ARMPowered and something that many um you know kids and and ele uh educators start um start their journey and and those things are getting super excited now with robotics and and beyond.

(49:57) >> In business they say you can have better, cheaper or faster but you only get to pick two. What if you could have all three at the same time? That's exactly what Cohair, Thompson, Reuters, and Specialized Bikes have since they upgraded to the next generation of the cloud, Oracle Cloud Infrastructure. OCI is the blazing fast platform for your infrastructure, database, application development, and AI needs

(50:29) where you can run any workload in a high availability, consistently high performance environment and spend less than you would with other clouds. How is it faster? OCI's block storage gives you more operations per second, cheaper. OCI costs up to 50% less for compute, 70% less for storage, and 80% less for networking. Better in test after test,

(51:02) OCI customers report lower latency and higher bandwidth versus other clouds. This is a cloud built for AI and all your biggest workloads right now with zero commitment. Try OCI for free. Head to oracle.comonai. Ionai. All run together. Eye o n ai. That's oracle.com/

(51:35) Iion AI.