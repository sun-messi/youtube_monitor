# How Inference-First Infrastructure Is Powering the Next Wave of AI

## 📹 视频信息

- **频道**: Eye on AI
- **发布日期**: 2026-01-17
- **时长**: 56:03
- **原始链接**: [https://www.youtube.com/watch?v=0EizteFD2Hs](https://www.youtube.com/watch?v=0EizteFD2Hs)

---

# How Inference-First Infrastructure Is Powering the Next Wave of AI

本文内容整理自Sircale产品副总裁尼克·潘德（Nick Pander）在Eye on AI频道的技术访谈。

## TL;DR

Sircale作为专注于AI推理优化的新型云服务商（Neocloud），通过与高通等芯片厂商合作，为企业提供比传统云巨头更具成本效益的推理服务，推动AI从试点阶段向生产环境转型的关键在于定制化基础设施和白手套服务。

---

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-05:00 | 介绍与背景 | Nick Pander介绍Sircale专注于AI推理的定位 |
| 05:00-10:00 | 从训练到推理的转变 | 解释为什么企业转向使用现有模型而非自建 |
| 10:00-15:00 | 成本与性能优化 | 探讨推理优化的经济效益和技术挑战 |
| 15:00-20:00 | 推理优化芯片 | 介绍高通等推理专用芯片的优势 |
| 20:00-25:00 | 企业AI应用实例 | 金融保险等行业的自动化案例 |
| 25:00-30:00 | Neocloud概念 | 解释专门化云服务的市场定位 |
| 30:00-35:00 | 政府与监管行业 | 讨论特殊安全需求的解决方案 |
| 35:00-40:00 | 无服务器推理 | 介绍即插即用的推理服务模式 |
| 40:00-45:00 | 企业采用现状 | 分析AI从概念验证到生产的路径 |
| 45:00-56:00 | 未来展望与建议 | 企业AI转型的最佳实践和起步方法 |

---

## 📊 核心论点

### 从训练转向推理：AI产业化的关键转折

- **核心内容**：过去一年，AI行业焦点从模型训练转向推理部署。大多数企业不再自建模型，而是利用Meta Llama、AI2等开源基础模型，通过RAG（检索增强生成）或轻量级微调来满足特定需求。这种转变让企业能跳过昂贵的预训练阶段（动辄数百万美元），直接进入价值创造环节。
- **关键概念**：基础模型、RAG技术、模型微调、推理即服务、成本效益
- **实际意义**：降低了AI应用门槛，让中小企业也能部署先进AI；推理市场将远超训练市场规模；云服务商需要提供更专业化的推理优化服务。

### Neocloud：介于私有部署和超大规模云之间的新选择

- **核心内容**：Neocloud是专注于特定计算需求的云服务商，不同于AWS/GCP等通用云巨头。它们提供白手套服务，为客户定制GPU集群、网络连接和安全方案。特别适合需要7×24运行推理服务、对延迟敏感、或有特殊监管要求的企业。相比超大规模云，成本可降低30-50%。
- **关键概念**：白手套服务、专用基础设施、混合云架构、成本优化、延迟保证
- **实际意义**：为企业提供第三种云选择；推动云服务从"一刀切"向"定制化"演进；特别适合金融、医疗等监管行业的AI应用。

### 推理优化芯片：打破英伟达垄断的新机遇

- **核心内容**：高通AI 100 Ultra等推理专用芯片相比通用GPU，功耗降低70%但保持相当性能。这些芯片专为推理设计，去除了训练所需的复杂功能，优化了Transformer等模型架构的执行效率。Sircale与高通合作，成为首个提供其推理芯片云服务的Neocloud。
- **关键概念**：推理专用架构、功耗效率、Transformer优化、非CUDA生态、边缘到云端
- **实际意义**：打破英伟达CUDA生态垄断；为推理工作负载提供更经济的选择；推动AI从数据中心向边缘设备扩展。

### 企业AI应用的现实路径：从概念验证到规模化

- **核心内容**：成功的企业AI项目遵循"价值验证→概念验证→试点→生产"四阶段。以抵押贷款审批为例，AI可将21天流程缩短至3天，通过OCR和LLM自动识别文档问题。但80%的企业CEO只知道"要做AI"，缺乏具体路径。关键是从简单用例开始，逐步提升复杂度。
- **关键概念**：价值验证、渐进式部署、业务流程自动化、人机协作、投资回报率
- **实际意义**：企业应避免直接挑战最难问题；中间件和集成商将成为AI普及的关键；成功案例将带动整个行业转型。

### 无服务器推理：降低AI应用开发门槛

- **核心内容**：Sircale与高通推出的推理平台允许开发者在几分钟内获得API端点，无需配置GPU或了解底层硬件。平台预装主流模型，支持自定义微调，按使用量计费。开发者可直接询问AI模型如何集成API到自己的应用中，形成"AI帮助使用AI"的良性循环。
- **关键概念**：无服务器架构、API即服务、开箱即用、按需付费、开发者友好
- **实际意义**：让非AI专家也能部署AI应用；加速企业AI试验和迭代；推动AI从技术工具变为业务工具。

### 安全与合规：私有AI部署的核心需求

- **核心内容**：许多企业担心数据泄露，需要完全私有的AI环境。Sircale提供气隙（air-gapped）部署，支持本地数据湖连接，确保敏感数据不离开企业网络。以OpenAI的开源模型为例，企业可获得接近ChatGPT性能但完全私有的方案。
- **关键概念**：数据主权、气隙环境、私有部署、监管合规、混合云架构
- **实际意义**：解决金融、医疗、政府等行业的合规难题；推动AI在高度监管行业的应用；平衡创新与安全的需求。

### 生态系统协作：从硬件到应用的全栈整合

- **核心内容**：AI成功部署需要硬件提供商、云服务商、系统集成商和ISV的紧密合作。Sircale定位为连接器，上游对接高通等芯片商，下游服务企业客户和SaaS提供商。这种生态approach让每方专注自身优势，加速AI落地。
- **关键概念**：生态系统思维、价值链分工、技术栈整合、合作伙伴网络、端到端方案
- **实际意义**：单一厂商无法解决所有问题；生态协作将成为AI时代的主流模式；中间件和集成商机会巨大。

### 边缘计算与云端协同：AI部署的未来架构

- **核心内容**：高通在移动芯片领域的优势延伸到数据中心，实现从手机到云端的统一AI栈。未来AI工作负载将在设备端和云端动态分配，根据隐私、延迟和成本需求智能调度。这种架构特别适合需要实时响应的场景。
- **关键概念**：边缘智能、云边协同、统一开发框架、动态调度、隐私计算
- **实际意义**：推动AI向更分布式架构演进；为IoT和移动应用开辟新场景；改变传统云计算的集中式模式。

### 从技术驱动到业务驱动：企业AI转型的文化挑战

- **核心内容**：许多企业AI项目失败源于过度关注技术而忽视业务价值。成功案例都是从具体业务痛点出发，如减少处理时间、提高准确率等可量化指标。需要业务团队深度参与，而非仅由IT部门主导。
- **关键概念**：业务价值导向、跨部门协作、变革管理、ROI量化、持续迭代
- **实际意义**：AI不是IT项目而是业务转型；需要自上而下的支持和自下而上的执行；成功指标必须与业务KPI挂钩。

### 成本透明度：推理服务的新商业模式

- **核心内容**：传统云服务按资源使用计费，但AI推理更适合按输出（tokens）计费。Sircale提供透明定价，包括基础设施成本和性能保证。企业可准确预测AI支出，避免超大规模云的不可预测费用。这种模式让AI成本从CAPEX转向OPEX。
- **关键概念**：按token计费、成本可预测性、SLA保证、TCO优化、财务模型
- **实际意义**：让企业CFO能理解和控制AI投资；推动AI从实验项目变为常规运营支出；加速AI的企业普及。

---

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Sircale | 专注AI推理的Neocloud服务商，访谈主角 | ⭐⭐⭐ |
| 高通(Qualcomm) | AI推理芯片合作伙伴，提供AI 100 Ultra等产品 | ⭐⭐⭐ |
| 英伟达(NVIDIA) | GPU市场领导者，CUDA生态系统创建者 | ⭐⭐ |
| OpenAI | ChatGPT开发商，开源模型提供者 | ⭐⭐ |
| Meta | Llama开源模型提供者 | ⭐⭐ |
| 微软(Microsoft) | Office 365 Copilot AI功能提供商 | ⭐⭐ |
| AMD | GPU市场参与者 | ⭐ |
| Cerebras | 推理优化芯片制造商 | ⭐ |
| SambaNova | 推理优化解决方案提供商 | ⭐ |
| AI2 | 开源AI模型和OCR技术提供者 | ⭐ |

---

## 💬 经典金句

> "从训练到推理的转变，是AI从实验室走向生产环境的标志。"
> — Nick Pander

> "我们不是要取代超大规模云，而是为需要24/7运行AI的企业提供更好的选择。"
> — Nick Pander

> "80%的CEO都说要做AI，但他们不知道这意味着什么。"
> — Nick Pander

> "最好的AI项目从最简单的用例开始，而不是最难的问题。"
> — Nick Pander

---

## 👤 主要人物

### Nick Pander（尼克·潘德）

**身份**：Sircale产品副总裁
**背景**：早期在英伟达和AMD工作，深耕GPU和AI领域多年，参与多个AI初创公司
**核心观点**：认为AI产业正从训练转向推理阶段，企业需要更专业化的云服务来支持AI生产部署。强调白手套服务和生态系统合作的重要性，倡导从简单用例开始逐步推进AI转型。

### Craig Smith（克雷格·史密斯）

**身份**：Eye on AI播客主持人
**背景**：资深科技记者和行业分析师
**核心观点**：对推理市场的发展趋势、企业AI采用率、新型云服务模式等话题进行深入探讨，推动访谈聚焦于实际应用场景和商业价值。

---

## 📺 视频类型判断

**访谈对话**：深度技术访谈，主持人与嘉宾就AI推理基础设施进行专业探讨

---

## 📝 完整翻译

### (0:00 - 8:00) Part 1

**(0:00 - 1:15)**

大家好，我是 Nick Pander，来自 Sircale。Sircale 是一家专注于 AI 多个领域的新云服务商，今天我们主要讨论推理服务。我是产品副总裁，我的工作是与开发团队合作，为 AI 推理服务和其他领域构建独特的能力。我的背景来自 GPU 领域，早期在 NVIDIA 工作过，也在 AMD 和几家涉及 AI 模型及其他类型模型应用的初创公司工作过。

主持人：虽然我不认为这是云服务商的工作范畴。你们参与优化推理服务而不仅仅是提供计算资源，这样做是不是很特别？

**(1:15 - 2:30)**

Nick，请先向听众介绍一下自己，然后我们再谈推理服务。

Nick：请先告诉我们 Sircale 是做什么的。

Nick：我们是一家更注重白手套服务的云服务提供商。我们专注于理解客户的特定问题，当他们寻求外部云服务时。通常我们接手的客户来自本地环境或超大规模云服务商，他们需要的服务和能力可能不是目前所获得的。从 AI 角度来看，就是能够为特定用例提供正确的 AI 计算集群。覆盖数据预处理或后处理所需的任何计算需求。提供高性能、可靠的解决方案，特别是在推理这个差异化领域。

**(2:30 - 3:45)**

我们真正希望增强这些服务，从客户通常自带技术栈和解决方案的 AI 加速计算集群，扩展到更多即用型解决方案，客户可以直接用于训练、微调以及推理能力。

主持人：你们的差异化主要体现在软件上，还是拥有专门针对推理优化的 GPU？

Nick：这是个很好的问题。我们有几个不同的层面。首先，我们专注于运营云服务，在客户寻求的能力范围内提供最高性能，同时平衡最高的可靠性。对于训练类工作负载，我们希望提供我们所说的最高作业完成率。对于推理，我们希望提供最稳定的 99.9% 正常运行时间，以及提供无服务器解决方案的独特软件能力。

**(3:45 - 5:00)**

主持人：我们在讨论推理服务，大约六个月到一年前出现了转变，人们不再谈论训练而开始谈论推理。我采访过 Cerebras 的 Andrew Feldman、Samanova 的 Rodrigo Le Young 以及其他专注于推理的芯片制造商。这些人在让开发者采用他们的硬件时遇到困难，因为开发者过于依赖 CUDA 编程语言。他们转而构建自己的云服务或试图让云服务商采用他们的芯片，提供推理服务或推理即服务。你能首先谈谈为什么会出现这种向推理的转变，以及随着 AI 在经济中的普及，你如何看待推理工作负载的爆炸式增长？

**(5:00 - 6:15)**

Nick：如果我们看从训练到推理的转变，有一个很重要的事实：有很多组织在构建模型，要么为自己的专有服务构建模型，比如 OpenAI 的 ChatGPT 类型模型，你构建的东西对很多不同的用例都很有效。模型可以支持广泛的不同用途。类似地，我们也看到当今市场上其他模型的情况。如果我们看看目前从推理角度来说大家使用最多的模型，可能是作为 Office 365 套件一部分的 Microsoft 提供的 Copilot。

**(6:15 - 7:30)**

这些模型在特定任务上表现良好。当你开始关注更多企业级工作流程，关注智能体用例和智能体工作流程时，你会利用模型输出并使用外部工作流程以及组织中可能存在的现有应用程序。目前市场上有更精选的一组模型，可以直接使用而无需预训练模型（这是一个非常昂贵且耗时的操作），也无需后训练模型。我们真正关注的是利用现有的基础模型，市面上有很多，比如 Meta 的 Llama、AI2 的开源模型等，能够采用这些模型并进行某种程度的 RAG（检索增强生成），使模型更适用于你的用例。

**(7:30 - 8:00)**

或者对于允许的模型进行一定程度的微调。有很多选择可以利用这些模型，跳过整个训练路径，利用这些现有模型并开始在你的特定工作流程中使用，连接到现有的智能体类型用例。与一年前甚至六个月前的认知相比，现成模型能做的事情要多得多。

主持人：虽然我不认为这是云服务提供商的工作。你们参与优化推理而不仅仅是提供计算资源，这样做是不是很特别？

### (8:00 - 16:00) Part 2

**(8:00 - 9:15)**

是的，大多数云服务提供商确实都提供私有云环境，甚至 OpenAI 也为这些安全需求提供私有云环境。

我们喜欢看到的差异化在于理解客户的独特需求。他们是将工作流程的某些部分连接到超大规模云服务商，将某些部分连接到本地基础设施作为端到端工作流程的一部分吗？我们能够提供更加技术丰富的解决方案来满足他们的特定需求吗？如果他们有特定的工作流程或已经在本地使用的特定软件栈，我们希望能够扩展这些。

**(9:15 - 10:30)**

如果他们已经在本地构建了推理服务层并对此满意，想知道如何将其扩展到云端，这正是我们可以帮助的领域。我们可以在 CoreWeave 一侧提供一些独特性，通过提供与其本地基础设施的连接，提供高性能计算集群，这对他们想要实现的目标具有成本效益，并为他们提供可预测的性能。

另一方面，如果我们看那些已经在超大规模云服务商中的人，当他们在超大规模云服务商内部部署这些 AI 工作负载和私有 AI 部署时，面临的最大挑战之一就是成本会迅速累积。

**(10:30 - 11:45)**

我们要提供的是你从超大规模云服务商那里期望的同样性能水平。超大规模云服务商真正擅长的是让你随时上线和下线，而不是 24/7 全天候使用那些节点。我们这样的云服务提供商，Neo 云服务商会更擅长的是：如果客户需要特定级别的计算性能来 24/7 在模型中生成 Token，然后在工作日上午 9 点到晚上 9 点期间需要更高的性能水平（人们可能更多地使用模型时需要五倍的容量），在当今的超大规模云服务商中，在某些模型部署中获得可靠性能有时是一个挑战。

**(11:45 - 12:50)**

你看到性能问题的最大指标是首 Token 时间大幅增加。所以你为模型的使用付费，但你也开始受到一些性能问题的困扰，这些问题伴随着人们看到的 AI 在许多不同组织中变得越来越普遍的巨大需求增长。

主持人：所以我想吞吐量是你们的关键指标之一。

是的。我几乎要说是为客户提供正确的计算解决方案，也就是正确的吞吐量、性能和成本效率，这些才是真正的关键驱动因素。吞吐量要真正服务那些持续的请求。如果我们从 AI 或 LLM 模型的角度来看，它是那些数百万 Token 请求，能够可靠地服务这些请求，在这些不同的模型架构中提供可预测的性能。

**(12:50 - 14:05)**

每个模型的表现都不同。所以，确保运行该模型的计算集群提供最佳性能，如果是实时用例的话，要有最低的首 Token 时间。然后具有成本效率，这样你就可以扩展，当你的用户数量增加或你的复杂性和模型使用增加时，你能够很好地覆盖这些。

我们真正看到的是理解所有这些要素的完美平衡，帮助客户真正平衡性能和能力需求与经济成本方面，部署模型并让它在特定工作流程中使用。

**(14:05 - 15:20)**

主持人：我直到大约一年前才意识到，即使是超大规模云服务商也将大量工作负载外包给私有数据中心。是这样吗？你们为超大规模云服务商提供服务吗？

是的。我们的重点确实是支持那些在生成式 AI 或 AI 模型之上构建和提供服务的人。我们更多的是在谈论那些为最终客户或组织构建服务产品的特定客户。我们希望与那些希望扩展其现有工作流程以添加 AI 服务但可能不想仅从成本和性能角度依赖其现有超大规模云服务商的 ISV 和 SaaS 提供商密切合作。

**(15:20 - 16:00)**

我们看到的最大增长领域是在今年年底和明年，有更多的企业客户已经了解他们想要做什么。他们已经看过超大规模云服务商的解决方案，在那里他们可以获得不同模型的模型花园、API 端点，他们正在寻找更多种类的模型，但需要有人能够以服务方式提供。更多的是无服务器产品，但围绕他们关心的模型。在某些情况下，我们将在某些组织中看到，这些将是可能来自 SaaS 提供商的专有模型，或者可能是该组织独有的模型的微调版本的专有模型。

### (16:00 - 24:00) Part 3

**(16:00 - 17:15)**

它们将在训练和推理方面都表现出色，但成本会很高。因此，市场上存在对解决方案的需求，这些解决方案可能不是首先考虑训练，而是首先考虑推理，对于某些工作流程，它们将成为更优化的解决方案。

是的，Qualcomm 确实专为推理优先的工作负载而设计，但让它变得更便宜的原因是什么？显然不是芯片的成本，也不是带宽，那么为什么它比超大规模云服务商更便宜呢？

**(17:15 - 18:30)**

简单来说，你得到的解决方案专注于推理优先的工作负载，它专为提供所需的吞吐量来交付模型而构建。具有支持不同模型类型、Transformer、支持模型内部使用的不同类型网络、能够支持扩散模型等的正确能力。涵盖专门需要的吞吐量用例，同时也提供更有效的功耗配置。

如果我们看看 Qualcomm 解决方案的功耗配置，它仍然是数据中心级设备，只是当我们看一个装满 Qualcomm 解决方案的机架与其他解决方案相比时，你会看到极高的能源效率。这真正归结为功耗节省。

**(18:30 - 19:45)**

对于很多这些模型来说，在某些用例中，高性能加速器与专为推理优先而平衡、为功耗效率而平衡的加速器之间的差异，你仍然可以获得出色的性能，为这些代理工作流程提供这些模型服务。这实际上取决于客户在寻找什么。

在很多情况下，你会看到对于高性能加速器，在一个有八个 GPU 的服务器中，你几乎只使用一个 GPU。你要运行一个几乎只利用一个 GPU 的模型。难道你不想要一个性能更调优的解决方案，让你充分利用整个 GPU，然后能够扩展到额外的 GPU 或运行作为你工作流程一部分的额外模型吗？

**(19:45 - 20:50)**

更好地利用硬件，你有专用于客户的硬件，但仍然服务成本效益。Seroscale 的一个关键差异化是我们在裸金属类型的解决方案中不做多租户。我们真的想依靠合作伙伴技术，比如我们与 Qualcomm AI 推理软件栈的合作，能够处理在平台上为不同用例运行多个模型。

因此，软件栈也有一些差异化，能够更好地利用硬件，确保你在较低功耗曲线内仍能获得尽可能多的性能。

**(20:50 - 22:00)**

是的。我之前为 Forbes 写过一篇文章。有一些公司正在出现，它们聚合闲置的 GPU 和 CPU 资源，然后将其作为云解决方案提供，因为大多数数据中心，特别是私有数据中心，大部分时间芯片都没有被使用。所以是的，我能理解为什么这很重要。

有两个问题。你们看到什么样的企业来找你们？我是说，企业花了一段时间，你提到了这一点，来弄清楚在企业中什么样的 AI 真正有效。已经有很多试点项目，我们现在才开始在企业中转向生产工作流程。是代理转变或向 AI 代理的转变，还是其他形式的 AI 最终在企业中站稳脚跟？

**(22:00 - 23:15)**

是的。我认为我们可以从两个不同方面来看。第一个是代理，我们稍后会谈到。但让我们先谈谈自动化。有很多任务可以自动化来改善人的工作方式。一个很好的例子是，如果我们看一个垂直细分市场，比如银行金融保险市场，想象一个抵押贷款申请包。

我想我们所有人都在某个时候办过抵押贷款或贷款，你要提交很多文件，抵押贷款提供商总是说"哦，我们会在七天内处理完"，但你知道21天后他们仍在要求文件。如果能够提交所有这些文件，有一个自动化工作流程，解析这些文件并理解是否有承保人需要立即查看的红旗，能够回到客户那里说"嘿，你缺少这些或这里需要更多信息"，那不是很棒吗？

**(23:15 - 24:00)**

现在，这个过程是到他们看完所有材料时，已经是一两周了。因此，提供某种程度的自动化，一个以多模态模型为前端的 LLM，甚至是 OCR 类型的模型。你知道，有一些现代 OCR 模型，我们一直在与 AI2 合作开发。能够处理计算机生成的、印刷文本或手写文本，能够将其转换为输出，在特定模型中理解这些输出，并理解某些东西是否填写正确，是否提供了需要提供的某些东西，这样抵押贷款的21天周转时间就可以变成，好的，在三天内，我们告诉你所有需要提供更多信息的地方，然后你就可以继续推进。

### (24:00 - 32:00) Part 4

**(24:00 - 25:15)**

超大规模云服务商在运行SaaS类型的应用程序方面非常出色，当工作负载为特定客户的特定SaaS应用程序运行时，他们确实很擅长。他们在为组织提供标准化数据湖来存储所有专有信息方面也很出色。超大规模云服务商面临挑战的地方是GPU可用性和计算可用性，以及将其与特定工作流程的成本效益联系起来，提供你想要的性能要求。

**(25:15 - 26:30)**

因此，要么你会看到客户将所有内容都放在超大规模云服务商那里，包括他们的应用程序，但他们需要在像我们这样的新云中获得额外的计算提升，以能够覆盖那些在超大规模云服务商上可能成本过高的用例；要么你会有一个工作流程真正是混合的客户，不仅仅是超大规模云服务商到像我们这样的新云，还包括本地部署。从安全角度来看，他们可能会决定我们的主要数据湖实际上是本地的，或者从监管角度来看，某些应用程序必须在更受约束或受控制的环境中运行。

**(26:30 - 27:45)**

因此，拥有混合不同类型计算但将数据保持在需要的地方的能力非常重要。所以这个想法是你的数据保留在数据湖中，当你运行推理时，你将提示输入模型，然后将输出返回到需要的地方。我们的独特之处是我们可以将这些端点呈现给模型，使其成为他们私有网络结构的一部分，该网络连接到超大规模云服务商或他们的本地部署。因此，我们可以为那些需要的客户做非常安全约束的事情。我们还可以覆盖真正气隙类型的环境。如果他们有相当严格的安全类型监督，我们也可以覆盖这些特殊用例。

**(27:45 - 28:30)**

主持人：这确实很有趣，首先一切都在本地部署，然后转到本地数据中心，然后是云、超大规模云服务商或早期的云AWS和GCP，每个人都转向那里，这是一个重大转变。向云的迁移还没有完全完成，但现在似乎人们意识到你需要云提供更具体的服务。因此出现了这些你称之为新云的服务，提供那些更具体的服务。首先，这是正确的吗？其次，你们服务的企业主要是在受监管的行业还是跨所有行业？

**(28:30 - 30:00)**

这是跨所有行业的。我们试图专注于看到最多采用的领域，那些有意愿花钱的地方——银行、金融、保险、医疗，特别是在计费方面；医疗研究方面；零售业可能涉及一系列不同的客户互动类型领域，可能是计算机视觉模型与LLM混合，用于查看人们消费方式、人们使用自助结账方式的假设等等。这实际上是在武装这些团队，他们的用例是考虑从传统类型的模型现代化到利用不同级别的AI来改善那些工作流程、改善结论和改善分析。

**(30:00 - 31:20)**

我要谈到的一点是，我们看到更多的组织在他们最初的部署中学习，他们可能在超大规模云服务商上用一两个模型做了无服务器类型的解决方案，他们学到的是他们需要在做POC或试点之前先做所谓的价值验证。查看所有这些用例。如果一个组织说"嘿，我们组织内有一百个不同的用途可以从AI中受益"，那就做价值验证来了解哪些是最有益的。作为价值验证的一部分，你是否对可能看到的一些节省有估计，你知道，你是否有理由证明作为这些节省的一部分，或者作为你实际实现某种程度的AI应用准备程度的一部分，然后你是否有足够的数据来开始POC？

**(31:20 - 32:00)**

然后你做那个POC来了解，好的，我现在能将这从真正的概念验证转变为轻量级试点吗，也许不是在整个组织中，但我能证明我从价值验证到POC再到试点的假设仍然成立吗，这样我就可以将其投入生产并看到我得到的好处。我认为组织真的必须进入那种心态来强化做价值验证。那些走捷径的通常是那些在将模型投入生产时遇到挑战并在部署后产生疑虑的组织。

### (32:00 - 40:00) Part 5

**(32:00 - 33:15)**

我们真正致力于依靠那些企业组织已经拥有的合作伙伴，所以要么是大型组织的全球系统集成商正在尝试进入AI服务和能力领域。我们完全支持这一点，因为我们希望在其中有服务合作伙伴，但很多组织没有这种级别的能力可以依靠。可能是技术不太熟练的大型企业组织。因此，我们希望能够为某些类型的用例提供无服务器即用型的能力，比如他们来找我们说，好的，我需要一个适合这个特定用例的模型，让我的员工可以询问关于HR福利的问题，这里是我所有的HR数据。我如何将这些组合成RAG并去执行呢？

**(33:15 - 34:30)**

我们希望提供解决方案帮助他们更容易地构建这些。但同时，当他们想要进一步提升时，我们希望能够说："好的，这里有一个我们审查过的合作伙伴，可以帮助你将此提升到下一个级别，这绝对是我们能做的事情，但我们希望确保我们是可扩展的，我们与那些为我们带来机会的合作伙伴合作，但同时，我们也为他们带来机会来扩大他们覆盖的客户群，并发展他们可能作为工作流程一部分正在构建的独特能力。你能谈谈Qualcomm合作伙伴关系吗，为什么这对推理特别重要，尤其是对智能体工作流程？

**(34:30 - 35:45)**

绝对可以。你知道，市场上有很好的现有解决方案。但任何技术的关键是在可用解决方案方面拥有多样性，这总是推动发展并推动市场前进。因此在Scale，我们始终希望成为那个值得信赖的合作伙伴，能够提供适合不同用例的各种解决方案。我们与Qualcomm的关系，首先我们都是圣地亚哥的公司，所以有密切的联系，我们为来自圣地亚哥的姊妹公司加油，尽管我们对市场上的其他解决方案也表示很多支持，但我们也想开始构建不同的技术。Qualcomm有独特的地位，因为用户口袋里设备的很大一部分都与某种程度的Qualcomm技术相关。

**(35:45 - 36:50)**

如果你看Android手机的大部分，里面都运行着Qualcomm设备，这些设备上开始出现AI服务，设备级别的AI服务。所以这里有一个思路，就是如何利用用户边缘各种不同设备中的那些技术，开始利用相同的框架和相同的软件堆栈，让模型不仅在设备上运行，还能更多地倚重云端。因此，能够从一个完全不同的方法进入数据中心是很有意义的，你知道从无所不包的边缘计算专家发展到想进入数据中心，这对我们来说很有趣，因为它带来了几个不同的转变。

**(36:50 - 38:00)**

首先，它将不同类别的加速器引入市场，带来了基于完全不同功耗曲线构建的加速器。功耗效率将是其中的一部分。训练需要那种原始的爆发功率。推理工作流程并不一定需要它。所以这需要平衡真正需要运行那些推理类型工作负载的东西。然后提供所有交付的软件，使像我们这样的云提供商更容易将其提供给客户。正如你之前说的，有一个在这个领域做得很好的现任者，构建了一些特定的软件技术，整个行业都已经习惯了。在推理中，情况非常不同。在推理中，没有与现任语言成为工作流程一部分的绑定。

**(38:00 - 39:10)**

它为AI模型提供服务。它提供端点。它利用现有的通用技术来服务模型并将其交付给用户。因此有机会为某些类型的客户提供更加即插即用的东西。当我们看那些今天开始做价值验证并查看用例的企业组织客户时，他们可能很快决定，哦，只要我能从某个地方获得这个特定模型，我就可以利用它。我可以将这个特定模型用作我工作流程的一部分。我们真正想看到的是更多解决方案提供这些即插即用的无服务器类型解决方案。

**(39:10 - 40:00)**

对于技术相对不太先进但仍想提供优秀AI服务的组织，我们为他们提供更简单的方案，你可以采用像Qualcomm推理产品这样我们与他们共同提供的解决方案，能够创建账户，进入那里查看模型编辑器，测试模型，查看该特定模型的端点，用API密钥集成到你的应用程序中，几分钟内就可以开始。我们说的是从设置账户到利用和找到端点再到获取API密钥，你可以开始将其集成到你的特定应用程序和工作流程中。

### (40:00 - 48:00) Part 6

**(40:00 - 41:15)**

指令模型在某些特定用例上会有更好的性能。你希望把时间专注于理解你的工作流程，而不是配置环境，让你的团队花更多时间在配置和维护环境上。开发人员希望把时间花在理解哪些模型对他们有益，给他们一个简单的实验方式。这种实验非常重要。让组织中不同的开发人员群体获得特定的API密钥来访问端点，能够随意试用，因为在某些用例中，这是从价值验证到概念验证的初始转换过程的一部分。你想要试用来理解某些模型是否在这些用例中表现更好，以及你在进入价值验证到概念验证时做出的一些假设是否错误，或者是否低估了这些假设？

**(41:15 - 42:30)**

拥有一个可以快速上手的平台，你的团队可以随意使用，这非常重要。特别是对于那些可能有开发人员一直在为他们的SaaS应用程序构建工作流程的企业组织，现在被要求构建一个聊天机器人，用户可以构建提示来在他们的SaaS应用程序中执行特定操作。你希望专注于正确构建工作流程，获得正确的模型，构建工作流程的代理部分。你不想去担心幕后发生了什么，是否运行正确。你只想知道它能运行、成本效益高，并且给你最佳性能。

**主持人：** 与高通合作，我想象他们做从边缘到移动到云端的一切。高通合作伙伴关系的优势之一是你与同一个硬件提供商合作一直到边缘吗？

**(42:30 - 43:45)**

我认为这对最终客户来说有很好的独特差异化。如果我们看某些有触及边缘的工作流程的客户，他们能够利用对模型在数据中心高性能环境中如何执行的相同理解，以及在边缘上可能支持什么。只需理解他们提供的软件栈的底层能力。这在无服务器类型用例中再次从用户、客户角度进行了抽象。但SirScale能做的一个独特方面是，如果客户说我要部署高通栈从边缘到边缘，从边缘到数据中心。端到端我们可以为他们提供带有高通栈的高通环境作为裸机，完全按照他们需要的方式，然后他们可以使用高通开发套件用于边缘，并能够利用相同的库、相同的AI框架，在那里也有一些共性。

**(43:45 - 44:50)**

对于那些超越无服务器类型环境并想为他们独特的工作流程做更多端到端工作的客户，这些是我们也可以帮助这些客户的领域。

**主持人：** 我们在通用推理和特别是代理采用曲线上处于什么位置？我与之交谈的人似乎觉得我们还处于非常早期，人们仍在刚刚走出试点阶段。你如何看待这个问题，你们的增长如何？我想象这在开始时是一种缓慢燃烧，但一旦人们获得概念验证并进入真正的生产，量将会爆炸。你如何看待Scale和整个企业在这个轨迹上的位置？

**(44:50 - 46:20)**

如果我们看今天我们在AI整体上正在做的事情，历史上我们最大的领域是在AI中，当我们看到从HPC到AI的转变，我们在AI出现之前在HPC领域有着极其深厚的传统。我们看到创业公司空间的很多人为特定的客户消费者用例构建模型，从SirScale开始，作为那个创业公司的一部分，他们成长了，我们保留了其中很大一部分作为客户。当他们构建模型时，现在转向推理。所以我们有大量客户基础今天在利用我们的AI加速，跨各种不同供应商进行推理。我们今天确实提供重要的推理服务。

**(46:20 - 47:35)**

但对企业来说，情况有点不同。在企业中，我认为这里需要考虑两个因素。第一，有些企业组织真正深入挖掘，为AI支出留出预算，关注那些自动化类型的用例，赋能员工，将外部服务引入内部，其中一些最初用模型完成，然后依靠自己的员工。我提到的抵押贷款例子就是一个很好的例子。所以有些组织绝对看到他们需要在哪里。遗憾的事实是，如果你看这些组织及其所在的行业，他们可能有少数几个同样思考的同行，而其他80%可能会说我们需要做AI，但我们不知道那意味着什么。所以确实存在这种困境，每个CEO都会说我们需要做AI，但这真正转化为什么？

**(47:35 - 48:00)**

将会有转变。第二点是我们将看到更多中间件、更多工具可以位于今天代理工作流程之上，提供聊天机器人或自动化服务，但触及现有应用程序。我们将在某些垂直客户细分中看到这一点。我们将在联邦、州、地方政府中看到它，那里的工作流程将能够利用AI加速，有人决定他们制作了很棒的粘合剂或与遗留应用程序对话的小工具，但他们可以在前面提供聊天机器人，但这是定制的。

### (48:00 - 56:00) Part 7

**(48:00 - 49:15)**

能够为企业组织更轻松地提供这些类型能力的参与者，那些可能不那么精通技术的组织，他们将会表现出色。他们也会吸引那些精通技术的组织，因为精通技术的组织会想："等等，这些人已经构建了这些工作流程，集成了我在SaaS领域拥有的一些应用程序，我不需要让我的团队专注于更复杂的那些，对吧？如果有人已经构建了其中一些，你知道，在我的150个概念验证用例中，如果这些人能解决10个，那就太棒了。我会去利用他们做的这10个，然后我会去看接下来的90个，也就是我要用自己团队接下来要做的两个或三个，对吧？"

**(49:15 - 50:30)**

是的，所以想必会有这样的浪潮。我问这个问题的原因是，我每周会收到几次来自某些研究公司的报告，说AI代理是如何失败的，你知道，60%或者其他什么百分比，某个高百分比的试点项目从未投入生产，你开始怀疑这是否只是早期阶段，还是承诺超过了现实。你对此有什么想法吗？

是的，我会回到概念验证。所以，你真的需要去构建所有这些用例的矩阵，然后开始评分，看看哪些可能在概念验证转为试点方面得分更高。我认为一些组织会看他们最困难的问题，并说我们想通过AI自动化来解决这个问题，但当你现实客观地看待它时，一个数据科学家可能会看着它说，好吧，这实际上是一个很难解决的问题，但如果你能解决它，你会获得很大收益。

**(50:30 - 51:45)**

所以我认为有时组织试图选择他们最难的问题，有时他们需要循序渐进，选择第一个或两个，取得一些成功，开始建立你想要做的事情的复杂性。我认为这就是外部服务合作伙伴可以提供帮助的地方。内部团队也有同样的问题，一些建立了重要团队的大型组织确实会转向并从错误中学习。犯错误没有什么不对，因为你从中学习。所以没有什么是完美的。但是我们作为云服务提供商，与我们的合作伙伴合作，帮助人们提供这些交钥匙类型的服务，帮助某人微调模型，构建他们的工作流程，或者我们只是作为合作伙伴与SaaS提供商或与SaaS提供商合作的中间件提供商合作。

**(51:45 - 52:50)**

如果我们提供底层硬件框架和云服务，这些组织就不必深度嵌入硬件或计算运营成本，他们可以专注于帮助客户构建新的能力。所以作为一个Neo云，我们能否帮助引入那些SaaS提供商、中间件或服务合作伙伴，并能够说："嘿，我们给你一个简单的方式来消费计算。"这样你就可以专注于构建你的服务，让更多客户更容易在企业空间中使用AI。

有人如何开始与SirScale和高通合作，提供你所谈论的现实世界影响？

**(52:50 - 54:05)**

绝对可以。对于那些技术更先进的组织，他们拥有所需的一切，只是在寻找更好的运行场所，我们可以在他们已经构建的堆栈之上提供这些计算资源。但是，正如我们正在谈论的，企业空间的很大一部分可能仍在学习并想要实验，有几种不同的方式可以开始。SirScale构建了一系列不同的推理技术作为平台，我们构建了推理平台，我们构建了一个叫做端点的能力。但开始的第一个地方是我们拥有的高通推理产品，因为这是一个很好的地方，只需设置一个账户。你会获得一些免费的平台使用，能够做一些初步实验。

**(54:05 - 55:20)**

你可以选择一些更领先、更流行类型的模型，开始推理，将它们集成到你的框架中，获得一个端点和API密钥，并能够在你的应用程序类型中使用它们。因为这都是基于AI的，那些有人员编写内部应用程序的组织会想，我不知道从哪里开始。

你可以拿一个AI模型说，我如何将端点连接到我的应用程序？我用这种语言编写应用程序，我如何连接端点并将这个提示词放入其中并获得输出？所以，你实际上可以利用AI模型。如果你是开发人员并且对初始起点感到紧张，你实际上可以使用其中一些AI模型并询问，提示模型。有一些专注于开发代码的优秀模型，你也可以这样做。所以你可以依靠其中一些模型，其中一些模型在高通平台上。

**(55:20 - 56:00)**

所以你不仅可以利用平台，还可以利用模型来帮助你理解如何使用端点集成到你的应用程序工作流程中，并在那里帮助你。所以这也是其中的关键部分，这是客户初始推理之旅的开始。但如果你是一个已经开始构建一些初始用例的组织，今天你在使用超大规模云服务商，你希望模型更好一些，高通在平台中添加了一些额外的能力来帮助微调特定模型。所以这是另一个可以很好实验的领域，只需来到与高通合作的SiroScale平台，拿走它并开始使用，开始试验。

---

*生成时间: 2026-01-19 12:27:44*
*由 YouTube Monitor & Translator (Claude CLI) 生成*