# Why Diffusion Language Models Will Define the Next Generation of LLMs

## 📹 视频信息

- **频道**: Eye on AI
- **发布日期**: 2026-01-04
- **时长**: 52:20
- **原始链接**: [https://www.youtube.com/watch?v=KeNZBcisLr8](https://www.youtube.com/watch?v=KeNZBcisLr8)

---

> 本文内容整理自Inception公司创始人兼CEO斯蒂法诺·埃尔蒙（Stefano Ermon）在Eye on AI频道的技术访谈。

---

### TL;DR（一句话核心洞察）

扩散语言模型（Diffusion LLMs）通过"并行降噪"而非"逐词预测"的全新生成范式，实现了比传统自回归模型快10倍的推理速度，同时保持相当的准确性，有望定义下一代LLM的技术架构。

---

### 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-04:46 | 扩散vs自回归模型原理 | 解释扩散模型如何通过并行生成克服自回归模型的顺序生成瓶颈 |
| 04:46-08:53 | 技术优势与架构对比 | 讨论速度优势、成本效益以及Transformer架构的延续使用 |
| 08:53-14:37 | 扩散模型训练机制 | 详解"添加噪声-去噪"的训练过程和全局重建任务 |
| 14:37-20:48 | 预测能力与可控性 | 说明扩散模型如何利用双向上下文进行预测并提供更好的生成控制 |
| 20:48-28:00 | 训练与扩展性 | 探讨数据效率、评估基准和扩展定律的优势 |
| 28:00-35:10 | Mercury模型与代码生成 | 介绍首个商用扩散语言模型Mercury及其在代码生成中的应用 |
| 35:10-43:16 | 竞争格局与未来愿景 | 分析扩散模型作为自回归模型替代品的潜力和实际应用场景 |
| 43:16-52:20 | 应用拓展与多模态融合 | 展望推理能力、多模态融合和世界模型的发展前景 |

---

### 📊 核心论点

#### 1. 扩散语言模型的并行生成革命

- **核心内容**：传统自回归LLM（如GPT、Claude）必须逐个token顺序生成，这是一个无法克服的结构性瓶颈。扩散语言模型通过训练神经网络"去除噪声"而非"预测下一个token"，实现了一次性生成整个答案。每次神经网络评估可以同时修改多个token，而不是仅一个，从而大幅提升并行计算效率。
- **关键概念**：并行生成、降噪训练、全局重建、推理加速、GPU利用率
- **实际意义**：推理速度提升10倍意味着实时应用（如语音助手、代码补全）的体验质变；成本降低使得AI应用的商业模式更加可行；为大规模AI基础设施部署提供了新的技术路径。

#### 2. 训练机制：从预测到重建的范式转变

- **核心内容**：扩散模型不是训练预测"下一个是什么"，而是训练"如何修复错误"。训练时人为添加噪声（遮盖、替换token），然后让模型学习恢复原始句子。这种方式让模型能够利用左右双向上下文，而非仅依赖左侧上文，形成了更丰富的任务集合和更强的理解能力。
- **关键概念**：噪声注入、全局去噪、双向注意力、重建任务、上下文利用
- **实际意义**：数据效率更高（相同数据量下性能更好）；对代码生成等非严格顺序任务更加适合；为未来的多模态统一架构奠定基础。

#### 3. 推理速度与成本的双重优势

- **核心内容**：Mercury模型在相同速度档位下准确性最高，虽然绝对准确性尚未达到最大规模的自回归模型（如GPT-4），但在低延迟应用场景中已经超越了同级别的自回归模型。并行计算特性使得GPU利用率大幅提升，单位推理成本显著降低。
- **关键概念**：延迟-准确性权衡、帕累托前沿、推理成本、GPU效率、速度档位
- **实际意义**：使得实时AI应用（语音助手、IDE代码补全）成为可能；降低了企业部署AI的成本门槛；推动AI从云端向边缘设备迁移。

#### 4. 代码生成的天然适配性

- **核心内容**：代码生成不像自然语言写作那样严格遵循从左到右的顺序，而是需要考虑整体结构、函数依赖、跨文件引用等。扩散模型的全局视角和填充能力（infill）使其在代码补全、代码编辑建议等任务上表现出色。Mercury在Copilot Arena基准测试中排名第一。
- **关键概念**：代码填充、非顺序生成、全局代码理解、IDE集成、编辑建议
- **实际意义**：改变了开发者与AI的交互模式，从被动等待变为实时互动；提升了代码生成的上下文理解能力；为"vibe coding"（氛围编程）等新型开发模式提供支持。

#### 5. 更强的可控性与安全性

- **核心内容**：扩散模型从一开始就能看到整个生成对象，可以在生成过程中持续检查是否满足约束条件（安全性、品牌一致性等），并实时调整生成方向。相比之下，自回归模型需要等到最后一个token才能判断整体是否符合要求。
- **关键概念**：生成控制、约束满足、实时调整、安全检查、品牌一致性
- **实际意义**：企业可以更精细地控制AI输出，确保符合合规要求；减少了有害内容生成的风险；为个性化AI助手提供了更好的定制能力。

#### 6. 数据效率与扩展定律

- **核心内容**：学术研究表明，扩散语言模型的数据效率更高——因为模型被训练解决多种重建任务（不仅是从左到右预测），相同数据量下能达到更好的性能。初步证据显示其扩展定律（scaling laws）优于自回归模型，意味着增加模型规模带来的收益更大。
- **关键概念**：数据效率、扩展定律、多任务学习、参数效率、训练成本
- **实际意义**：降低了训练顶级模型的数据需求和成本；为资源有限的研究机构和企业提供了机会；可能改变大模型竞赛的游戏规则。

#### 7. 语音助手的理想选择

- **核心内容**：语音助手pipeline包括ASR→LLM→TTS三个环节，LLM是延迟瓶颈。扩散模型的低延迟特性使得整体响应时间大幅缩短，用户体验从"等待回复"变为"自然对话"。多家企业已经将小型自回归模型替换为Mercury，获得了更好的效果。
- **关键概念**：语音管道、延迟瓶颈、实时交互、ASR-LLM-TTS、用户体验
- **实际意义**：推动语音AI从"功能型"向"陪伴型"转变；使得客服、教育、娱乐等场景的AI应用更加自然；为下一代人机交互界面奠定基础。

#### 8. 多模态统一的技术基础

- **核心内容**：扩散模型已经是图像生成（DALL-E、Stable Diffusion）和视频生成的主导技术。如果文本生成也采用扩散架构，将为构建统一的多模态AI系统铺平道路。单一架构处理文本、图像、视频、3D等所有模态将带来质的飞跃。
- **关键概念**：多模态融合、统一架构、跨模态学习、世界模型、3D理解
- **实际意义**：真正的通用AI助手将能理解和生成所有类型的内容；为AR/VR、机器人、自动驾驶等需要多模态理解的应用提供支持；开启AI理解物理世界的新篇章。

#### 9. 推理能力的下一步突破

- **核心内容**：Inception正在开发"推理扩散语言模型"，让模型能够在输出答案前进行"思考"。这将结合自回归模型的链式思考（CoT）能力与扩散模型的并行优势，为复杂推理任务提供新的解决方案。
- **关键概念**：推理模型、思考链、规划能力、智能体系统、复杂决策
- **实际意义**：使扩散模型能够处理数学证明、代码调试、战略规划等高级认知任务；为自主AI智能体（agents）提供更强大的决策引擎；可能成为通向AGI的关键技术路径。

#### 10. 商业模式与生态系统

- **核心内容**：Mercury已经作为商用产品发布，提供OpenAI兼容的API接口，开发者可以无缝切换。提供1000万免费token供开发者试用。已经成为多个主流IDE的默认代码补全模型，形成了实际的用户生态。
- **关键概念**：API兼容性、开发者生态、企业定制、即插即用、商业化
- **实际意义**：降低了新技术的采用门槛；为企业提供了自回归模型之外的选择；可能引发AI基础设施市场的重新洗牌。

---

### 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Inception | 扩散语言模型先驱，Mercury模型开发商，商业化领导者 | ⭐⭐⭐ |
| Google DeepMind | Gemini Diffusion研究，但尚未商业化部署 | ⭐⭐ |
| OpenAI/Anthropic | 自回归模型代表（GPT、Claude），性能基准对比对象 | ⭐⭐ |
| Stanford | Stefano的前东家，扩散语言模型早期研究发源地 | ⭐⭐ |
| World Labs (李飞飞) | 3D世界模型公司，多模态融合的潜在合作方向 | ⭐ |
| TMU | Copilot Arena基准测试运营方，第三方评估机构 | ⭐ |

---

### 💬 经典金句

> "The more parallel solution is the one that wins. Diffusion language models are built to be parallel from the ground up."
> — Stefano Ermon

> "It's all going to be an inference game. People are building these massive data centers not for training, but because they expect everyone will be using AI."
> — Stefano Ermon

> "We're able to drastically speed up generation and reduce costs. The computation is more parallel, we can make use of GPUs more effectively."
> — Stefano Ermon

---

### 👤 主要人物

#### Stefano Ermon（斯蒂法诺·埃尔蒙）

**身份**：Inception公司创始人兼CEO；前斯坦福大学计算机科学教授（任教10年）
**背景**：生成式AI领域先驱，在扩散模型、Flash Attention、DPO等关键技术上有开创性贡献。早期扩散模型研究者之一，其技术现已广泛应用于图像、视频生成系统。在斯坦福实验室取得扩散语言模型突破后，于2023年创立Inception公司。
**核心观点**：扩散语言模型的并行特性使其在推理效率上具有结构性优势，将成为下一代LLM的主流架构。他认为AI的未来在于推理而非训练，谁能提供最高效的推理服务谁就能赢得市场。

---

### 📺 视频类型判断

**访谈对话**：一对一深度技术访谈，主持人通过问答形式引导嘉宾详细阐述技术原理、商业应用和未来愿景。

---

## 📝 完整翻译

### (0:00 - 4:46) 扩散vs自回归模型原理
> 解释扩散模型如何通过并行生成克服自回归模型的顺序生成瓶颈

当你思考大多数现有的大语言模型时，比如 ChatGPT、Gemini、Claude，它们也被称为自回归模型。这意味着当你提出一个问题时，它们会给你一个答案，并且会逐个词或逐个 Token（我们这样称呼它）地生成答案。这种生成是顺序的，这就像一个结构性瓶颈，无法被克服，因为它已经内置到自回归的表述中了。

扩散语言模型在底层也使用神经网络。在我们的案例中，它也是一个 Transformer 神经网络。但模型训练的任务完全不同。模型不是被训练来预测下一个 Token，而是被训练来去除噪音。

大家好，我是 Stefano Ermon。我是 Inception 的创始人之一兼 CEO。在创办 Inception 之前，我在斯坦福大学担任计算机科学教授大约十年。我一直在从事生成模型的工作，也就是现在所说的生成式 AI，大约有十年时间了。我在扩散模型方面做了很早期的工作，这项技术现在为许多最先进的图像生成系统、视频生成系统提供支持。

我还研究了各种技术，比如 Flash Attention、DPO，这些技术现在在工业界被广泛用于构建大规模的 LLM 和其他类型的生成式 AI 解决方案。我在斯坦福的实验室里一直在研究扩散语言模型，试图弄清楚如何让扩散模型不仅适用于图像和视频，还适用于文本和代码生成。

去年我们实验室取得了重大突破，首次展示了这项技术能够与领先的自回归模型竞争。这促使我创办公司来扩大这些模型的规模，这就是我过去一年左右一直在做的事情。我们在训练第一批商业规模的扩散语言模型方面取得了相当大的成功，我们称之为 Mercury。

主持人：我很好奇，扩散语言模型能够工作确实很引人入胜。也许我们先谈论这个，但我想问的是，为什么扩散语言模型相比自回归的基于 Transformer 的 LLM 有优势？

Stefano Ermon：当然。原因在于，当你思考大多数现有的 LLM 时，比如 ChatGPT、Gemini、Claude，它们也被称为自回归模型。这意味着当你提出问题时，它们会给你答案，并且会逐个词或逐个 Token 地生成答案。

这种生成是顺序的，这就像一个无法克服的结构性瓶颈，因为它已经内置到自回归的表述中了。神经网络被训练来预测下一个 Token，这就是你在推理时向它提问时使用它的方式。顺序计算通常很难加速，因为如果你必须逐个 Token 进行，就没有太多可以并行化的事情。

另一方面，扩散语言模型是一次性生成整个答案的。神经网络被训练来完善其答案，类似于修复错误。然后在推理时，你使用这个过程，其中每次神经网络评估都能够同时修改多个 Token，而不仅仅是一个。

### (4:46 - 8:53) 技术优势与架构对比
> 讨论速度优势、成本效益以及Transformer架构的延续使用

所以我们看到，这些模型相比类似质量的自回归模型可以快得多。这是关键的好处。我们能够大幅加快生成速度，基本上是生成 LLM 答案所需的时间，并降低成本。这是另一个重大好处。计算更加并行化，我们可以更有效地利用 GPU 的并行计算，并且能够有效地为我们的用户降低成本。

主持人：另一个我不确定这是否是优势的问题，但是 LLM 的一个问题是——我一直在与人们谈论后 Transformer 架构——但 LLM 的一个问题是上下文，上下文窗口所需的计算呈二次方增长。所以很快就变得太昂贵了，然后还有各种上下文窗口太大的问题，LLM 会变得困惑或迷失。扩散 LLM 是否能解决这个问题？

Stefano Ermon：这是个很好的问题，我对这些问题非常熟悉。我是 Flash Attention 论文的原作者之一，这被广泛用于加速在 Transformer 中进行注意力所需的非常昂贵的二次计算。我们过去已经分享过，我很乐意确认我们的扩散语言模型在底层也是基于 Transformer 的。所以我们使用的架构仍然是带有自注意力的 Transformer。

这意味着我们在上下文长度方面得到了与自回归模型相同的优缺点。我们目前在生产中提供服务的模型大约有 13 万个 Token 的上下文。我们认为我们可以使用人们为自回归模型使用的技术来扩展它，但在优缺点方面，与自回归模型相比，真的没有任何好处或坏处。

我们也使用了不同的架构，就像自回归 LLM 领域的人们一直在实验状态空间模型、Mamba 和替代架构一样，这在扩散语言模型的背景下也是可能的。

### (8:53 - 14:37) 扩散模型训练机制
> 详解"添加噪声-去噪"的训练过程和全局重建任务

你可以在扩散语言模型内部使用任何你想要的神经网络架构，我们有一些原型，我们用这些替代架构替换了 Transformer，你会得到那些架构的好处和坏处。所以扩散与架构，这些就像你可以用来加速计算或减少内存占用的正交轴，或者权衡你可能关心的不同类型的性能特征。

主持人：对于听众和坦率地说对我自己，你能解释一下扩散是如何工作的，以及 Transformer 扮演什么角色吗？我知道在训练过程中注入噪音，然后它会将噪音反转到输出，但如果你能为外行解释一下，那就太好了。

Stefano Ermon：当然。自回归 LLM 和扩散模型在底层都依赖神经网络。有一个神经网络接受句子作为输入，然后输出一些东西。在自回归 LLM 的背景下，这个神经网络（通常是 Transformer）接受句子作为输入，然后预测下一个 Token，即我应该在该句子后面附加什么下一个词来使其有意义。你可以通过向这些模型提供大量文本或代码来训练这个神经网络做得相当好，然后经过大量训练迭代后，它们就能很好地理解什么样的 Token 是有意义的，以及你应该如何完成句子。

扩散语言模型在底层也使用神经网络，在我们的情况下，它也是一个 Transformer 神经网络。但模型训练的任务完全不同。模型不是被训练来预测下一个 Token，而是被训练来去除噪音。

你再次从可能从互联网获取的句子或代码开始，然后你人为地添加噪音。也许你遮掩一些 Token 或改变一些 Token 的值，然后你训练神经网络接受这个损坏的句子作为输入，并尝试修复错误，尝试给你返回原始句子。所以这是一个不同的训练目标。它不是下一个 Token 预测，而更像是句子的全局转换，你试图修复错误。

### (15:00 - 30:00) Part 2

### (14:37 - 20:48) 预测能力与可控性
> 说明扩散模型如何利用双向上下文进行预测并提供更好的生成控制

是的，但是预测从哪里来呢？回答这个问题，这纯粹是来自注意力机制吗，还是在去噪之前在后台发生了一些下一个 Token 预测？

不，它不仅仅是预测下一个 Token，这是该模型的另一个优势——它不一定只是按顺序逐个处理 Token。它实际上能够使用左侧和右侧的上下文来修复错误。比如说，如果第一个 Token 缺失了，但句子中的其他内容都已知，那么扩散语言模型可以尝试预测第一个 Token 应该是什么，基于我知道其他所有内容。

所以这有点像是对你通常在自回归模型中执行的任务的概括，在自回归模型中，你总是只从左到右预测一个 Token。在扩散语言模型中，你有一组更丰富的任务，你需要在训练期间能够解决，这可能只是预测下一个 Token，但也可能是在你知道其右侧所有内容的情况下预测第一个 Token。所以这更像是从右到左的预测。实际上它甚至更复杂，因为句子中可能存在各种各样的错误，模型需要弄清楚如何同时修复所有这些错误。

但在某种意义上，它们在某种程度上是相似的，因为在这两种情况下，你都试图重建输入的一部分。在自回归语言模型的情况下，你总是从左到右一次重建一个 Token；在扩散语言模型的情况下，你一次性全局地重建整个句子。所以这是一个更复杂的重建去噪任务，但我们仍然可以训练神经网络来解决它。

主持人：在图像扩散模型上，它们以难以控制而闻名。它们肯定会给你一个输出，但可能不是你想要的，即使它符合提示。这个问题如何与语言相关？

在我看来，扩散模型实际上比典型的自回归模型更容易控制。原因是在扩散模型中，你从一开始就可以访问整个对象。所以你从一开始就知道它是否与提示一致，是否满足一些安全约束或任何你想要用来引导生成的控制信号。在整个生成过程中，你始终知道你是否朝着正确的方向前进，约束是否得到满足。这就是为什么扩散模型相对容易控制的原因，人们已经提出了很多技术来将生成过程引导到某个方向。

因为你可以检查你是否满足约束条件，并且可以将生成推向你想要的方向。实际上，如果你考虑自回归模型，你需要等到最后，到最后一个像素，才能知道这是否满足约束条件。这就是为什么在我看来，扩散模型比自回归模型更可控。我们开始在扩散语言模型的背景下探索这些好处，是的，有时确实会出现问题，你希望模型以某种方式行为。当然，你希望模型是安全的，但也许你希望模型符合品牌形象。

### (20:48 - 28:00) 训练与扩展性
> 探讨数据效率、评估基准和扩展定律的优势

有更复杂的方法来定义你希望模型做什么，我们开始探索使用我们的扩散语言技术的方法，以允许我们的客户以更细粒度的方式控制语言模型。

主持人：让我整理一下思路。我有很多不同的问题。一个是关于训练的，另一个是关于引导它们。在自回归模型中，它们使用人类反馈强化学习，你们是否使用同样的方法？你们如何训练这些模型？这是与训练自回归模型相同的过程吗？

在高层次上，就像我讨论的那样，我们正在训练这些神经网络去除噪音，输入是从句子中去除噪音。我们没有披露很多细节，比如确切的训练目标或模型的确切大小或我们使用的训练数据量。有一些公开的论文，人们研究了扩散语言模型与自回归模型的缩放定律和性能，在学术文献中有一些证据表明扩散语言模型在数据效率方面要高得多。

直觉是，因为你训练模型解决许多不同类型的任务，不仅仅是预测下一个 Token，比如以任何顺序进行预测，它们往往更具数据效率。所以你需要更少的训练数据来达到一定的质量水平。当然，一切都取决于细节，有各种各样的事情你可以做，但我们有自己的配方，我们已经看到它确实做得非常非常好。除此之外，也可以进行人类反馈强化学习。也可以使用可验证奖励进行强化学习。

如果你在编程或数学的背景下，有一个可验证的奖励，有一个唯一正确或错误的答案，你可以用来检查结果的质量。在内部，我们能够使用所有这些技术，并将它们适应到我们正在处理的不同类型的生成模型中。

主持人：在评估方面，你们使用其他扩散模型进行评估吗，还是在评估中使用自回归 LLM？

评估 LLM 是出了名的困难，我们内部跟踪性能的方式是使用基准测试。社区开发了各种基准测试，试图衡量模型在人们关心的不同任务上的表现，比如数学能力、指令跟随能力、问答能力、用不同语言编写代码的能力。

所以在内部，我们监控我们的模型在各种基准测试中的性能，然后我们不断调整我们的训练方法、架构和推理方案，试图在保持计算成本低的同时获得尽可能好的性能，也试图使模型尽可能高效。所以在评估方面，它实际上完全相同，与你对传统 LLM 所做的非常非常相似。

主持人：你们迄今为止的模型参数规模是多少？

不幸的是，我们不能透露模型的确切规模。我们处在一个非常竞争激烈的领域，这是我们目前无法透露的信息。

### (28:00 - 35:10) Mercury模型与代码生成
> 介绍首个商用扩散语言模型Mercury及其在代码生成中的应用

主持人：很有趣，一个竞争激烈的领域。还有其他人在研究扩散模型、扩散 LLM 吗？

是的，在学术界有一个相当活跃的社区正在训练扩散语言模型，相对小规模的模型，包括我在斯坦福的实验室。我是说，GPT-2 规模的早期工作，我们训练的模型参数少于十亿。这种研究非常有价值，因为它允许你进行同类比较，比如小型自回归模型和小型扩散语言模型，看看哪个扩展得更好。

但这些开源的或在学术界开发的模型，它们通常表现不是特别好，因为规模相对较小，用于训练它们的数据也不是最好的。在其他行业实验室也有尝试构建扩散语言模型的努力。至少有一篇 Gemini 扩散的论文和发布的博客文章，来自 Google DeepMind 的研究人员谈论了他们围绕构建扩散语言模型的内部努力。到目前为止，它还没有在生产中部署，它只是一个演示，据我所知，客户无法使用，它没有为生产流量提供服务。

### (30:00 - 45:00) Part 3

是的，主要是这样。你们专注于编写代码有什么特殊原因吗？

是的。我们从代码开始，因为首先我们都是计算机科学家，所以代码对我们来说很亲切，我们能理解模型是否表现良好。这对扩散语言模型来说也是一个非常有趣的应用，因为它不那么自回归——从左到右的这种偏向在写文档时是合理的，这有点像我们的写作方式，虽然我认为可能仍然存在一些粗略的生成过程，我们从大纲开始然后填充细节。

但至少如果你考虑代码，从左到右的偏向要少得多，比如你考虑不同的文件，它们不一定有特定的顺序，有很多任务你真的想要能够查看整个代码库并填充你试图开发的特定函数或功能，这就是扩散模型能够大放异彩的地方，因为它们不受约束，不必从左到右进行，这种从左到右的想法不是内置的。所以这就是为什么我们最初专注于代码，因为这是我们期望看到巨大改进的领域，而且从商业角度来看也非常重要。这是我们看到LLM使用量最多的地方，人们从使用LLM中获得最大价值，用于更快地编写代码和查找代码等等，人们对此非常兴奋，所以这对我们来说是一个相当自然的第一步。

是的。这些模型能否以与自回归模型相同的方式进行微调？

当然可以。是的，我们可以为特定任务对它们进行微调，实际上这是我们与企业客户合作的模式之一。有时如果他们能够提供内部数据或他们关心的特定数据，我们实际上能够在他们的数据上微调我们的模型，并在他们关心的任务上提供更高的性能。

是的。我的意思是，你说你不能透露参数数量，但是否遵循相同的规律——参数越多，模型就越准确？

是的，通常是这样。模型越大，性能越高——只要你用相应更大量的训练数据来训练它，所以有一些注意事项，但通常你可以期望更大的模型表现更好，但它们也变得更昂贵、更慢，这就是我们面临的困境。

从你的角度来看，优势是速度和成本或功耗效率，还是不是准确性？你如何衡量相对于自回归模型的准确性？

是的，这取决于你如何看待今天存在的不同LLM，在质量、准确性、成本和速度之间总是存在权衡，如果你想要最快的模型，它们通常不是最准确的，因为它们更小，因为人们需要使用各种技巧让它们尽可能高效。我们能够实现的是，在某个特定的速度档次上，我们是最准确的模型，我们不是整体上最准确的模型，所以有一些由OpenAI、Anthropic和Gemini训练的更大模型，在绝对意义上比我们目前的水平更准确。

### (35:10 - 43:16) 竞争格局与未来愿景
> 分析扩散模型作为自回归模型替代品的潜力和实际应用场景

但在我们的速度等级中，我们是最准确的模型，所以我们的思路是，在质量和速度之间存在一个帕累托前沿，我们能够移动这个帕累托前沿，至少对于某个质量水平，公司的下一阶段是不断改进扩散语言模型的质量，直到我们真正赶上前沿级别的质量智能，目前这只在少数几个自回归模型中可用。

我的意思是，你认为扩散语言模型是自回归LLM的替代品，还是你认为扩散LLM与自回归LLM竞争，希望有一天它们能超越自回归LLM？

我们当然认为它们在功能上可以相互替代。无论何时你可以使用自回归LM，你也可以使用扩散语言模型。从这个角度来看，它们通常是解决同一问题的竞争解决方案。即使是现在，我们正在超越自回归LLM，我们的客户正在从自回归模型转向扩散语言模型，因为有许多低延迟应用，你需要能够快速给开发者或客户答案，因为你在调用语音代理。

有各种延迟很重要的应用，我们的模型在该延迟预算下实际上比人们为了足够快地输出答案而使用的自回归模型更准确。当然问题是，随着我们扩展模型，随着扩散语言模型变得更好，会发生什么，我们不知道。我可以看到一个未来，所有模型都将基于扩散。通常如果我回顾计算机科学的历史，更并行的解决方案是获胜的那个，扩散语言模型从根本上就是为并行而构建的。所以这就是我们下注的原因。但我们不知道。

也可能多种技术会并存，也许一种对某些用例更好，另一种对其他用例更好。完全有可能它们都会犯错误，但只要错误是不同的，彼此不相关，那么拥有基于自回归模型和基于扩散模型的代理就会有价值。它们可以相互交互，可以相互学习，通过拥有完全不同的智能堆栈来构建智能机器，你会获得价值。

实际上这是一个有趣的想法。你可以有一个系统，其中自回归模型检查扩散模型的工作，来回进行以完善答案。你们有试验过这种方法吗？

还没有。但我认为是的，这是未来我能看到如何运作的事情。将会有人类，将会有代理，代理将基于不同的技术。只要它们以有趣的方式相互补充，它们在组合中会很有用，整体将大于各部分之和。

对于编码，你如何看待人们使用它？它真的是作为编码助手吗？与一些最好的编码模型相比，它有多准确？人们是在使用DLM编写完整的代码库吗？

如果你考虑代码辅助，LLM有不同类型的用例。最简单也是第一个实际部署的是自动完成。就像你在编写代码时，LLM会建议你应该如何完成那行或那些行代码。这就是我们开始的地方。这是一个对延迟非常敏感的应用，开发者不会等一分钟来获得建议，建议必须是即时的，必须非常快，这样开发者才能保持流畅状态，体验才会好。

### (43:16 - 52:20) 应用拓展与多模态融合
> 展望推理能力、多模态融合和世界模型的发展前景

我们现在拥有的Mercury模型在代码完成方面是最先进的。有一个叫做Copilot Arena的基准测试，基本上是由CMU的研究人员运行的，这是他们用来评估专门用于自动完成的不同类型LLM的基准。它的工作方式是，他们有一个IDE，世界各地的开发者都在使用这个IDE，当他们编写代码时，他们不只看到一个完成建议，而是看到两个完成建议，这两个完成建议来自两个不同的模型，开发者不知道他们使用的是哪些模型，然后他们投票选择哪个更好，然后得出ELO分数，ELO排名，就像人们用于国际象棋选手的那种，来比较不同LLM的质量。我们的Mercury模型目前排名第一，与其他几个LLM并列，但我们在质量方面排名第一，在速度方面以相当大的优势排名第一。

### (45:00 - End) Part 4

我们目前只是触及了扩散语言模型可能性的表面。这就是为什么我对这些技术如此兴奋，你知道一切都相当新颖和新鲜。我认为仍有很多低垂的果实可摘。很多设计选择我认为还不够优化。我们经常只是采用自回归模型有效的方案，然后在可能的情况下直接应用到扩散语言模型的语境中。但我认为这些选择未必是最佳的，因为架构不同，模型本质上大不相同，所以在模型的研发方面仍有很多机会。

我们正在研究的其中一项是推理扩散语言模型。这是自回归模型具备的一项能力，也是它们能够大幅提升质量的方式之一 - 在输出答案前进行思考的能力。在扩散语言模型的语境中有一些有趣的方式来实现这类能力，这是我们在研发方面重点投入的领域之一，希望能开发出首个推理扩散语言模型。

主持人：哇，这很吸引人。这将对它们在智能体系统中的应用产生重大影响，对吗？

Anastasios Angelopoulos：完全正确。因为如果你考虑智能体，经常涉及大量规划和推理，一旦底层有推理模型支撑，这些能力就会变得更强大。

主持人：你们是否在研究多模态？我是说扩散已经是图像和视频生成领域的主导者了。

Anastasios Angelopoulos：是的，这是我们对扩散语言模型如此兴奋的另一个原因，因为我们已经知道扩散是图像生成、世界模型、视频生成的最佳模型。一旦我们破解并找出如何进行文本和代码生成，这就像为能够处理不同模态并跨不同模式学习的单一生成式AI系统开辟了道路。这是我们非常兴奋的事情，也是我们正在努力的另一件事，希望我们会有一些模型和公告发布。

主持人：我将与李飞飞交谈，她是你的同事，关于World Labs，我想这是她的称呼。这个大理石模型非常出色。你能谈谈扩散LLM如何融入世界模型吗？因为世界模型很棒，但真正令人惊叹的是当你有一个能够感知世界、理解物理定律，然后与语言结合的世界模型。

Anastasios Angelopoulos：完全正确。我认为一旦你结合不同模态就会有非常有趣的机会，因为如你所说，你可以拥有一个在物理书籍、工作原理和仿真代码上训练的模型，能够编写代码来弄清楚将要发生什么并对未来做出预测。我认为这将实现更强的世界建模能力。如果这个模型底层不仅在视频数据或3D数据上训练，而是真正通过所有可用文献、所有应该获得的科学知识来理解世界。

主持人：有什么我没有问到但我们应该谈论的吗？

Anastasios Angelopoulos：我认为Inception独特且令人兴奋的一点是模型真正可用。这不只是研究原型，而是开发者现在就能使用的模型。企业可以构建应用程序，他们可以用扩散语言模型替换自回归LLM。今天一切都向后兼容，API是相同的，就像OpenAI兼容的端点，所以这正在发生。我认为你的观众可能有兴趣看到这一点，并意识到这种转变，为可能改变他们构建事物、开发应用程序等方式做好准备。

主持人：是的。我们在开始录制前谈论了幻觉问题。你们如何处理幻觉？在自回归方面确实已经做了很多工作。对于扩散LLM是否采用相同的策略，这是否是完善这些模型的重点？

Anastasios Angelopoulos：当然，模型并不完美，会犯错误，我们尝试尽可能使用基准来测量。比如当你问与问答、常识相关的问题时，我们试图找出差距所在，然后通过整合额外的训练数据、额外的偏好数据或其他监督模型做正确事情的方式来解决问题。


---

*生成时间: 2026-01-05 05:55:59*
*由 YouTube Monitor & Translator (Claude CLI) 生成*