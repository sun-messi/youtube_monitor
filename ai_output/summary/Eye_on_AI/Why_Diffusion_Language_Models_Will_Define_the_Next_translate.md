# Why Diffusion Language Models Will Define the Next Generation of LLMs

## 📹 视频信息

- **频道**: Eye on AI
- **发布日期**: 2026-01-04
- **时长**: 52:20
- **原始链接**: [https://www.youtube.com/watch?v=KeNZBcisLr8](https://www.youtube.com/watch?v=KeNZBcisLr8)

---

本文内容整理自斯坦福大学计算机科学教授、Inception AI联合创始人兼CEO史蒂芬诺·阿尔蒙（Stefano Ermon）在Eye on AI频道的技术访谈。

---

## TL;DR（一句话核心洞察）

扩散语言模型（Diffusion LLMs）通过并行生成整个答案而非逐个生成token，在保持质量的同时大幅提升速度并降低成本，有望定义下一代LLM的技术路线，目前已在代码补全等延迟敏感场景中超越传统自回归模型。

---

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-05:00 | 扩散vs自回归模型核心差异 | 扩散模型并行生成所有token，突破自回归的序列瓶颈 |
| 05:00-10:00 | 扩散模型训练原理 | 通过去噪任务训练，可同时修改多个token |
| 10:00-15:00 | 预测与重建机制 | 利用双向上下文进行全局重建，而非仅左到右预测 |
| 15:00-20:00 | 控制性与训练方法 | 扩散模型更易控制，支持RLHF等传统训练技术 |
| 20:00-30:00 | 性能与商业化进展 | Inception是唯一实现商用级扩散LLM的公司 |
| 30:00-40:00 | Mercury模型与代码应用 | 在Copilot Arena基准测试中排名第一 |
| 40:00-45:00 | 语音助手与延迟优化 | 在延迟敏感场景替代小型自回归模型 |
| 45:00-52:20 | 未来发展方向 | 推理能力、多模态融合、世界模型集成 |

---

## 📊 核心论点

### 1. 扩散模型的并行计算优势

- **核心内容**：传统自回归模型必须逐个token串行生成，这是其架构的结构性瓶颈。扩散语言模型通过训练神经网络进行"去噪"任务，可以在单次前向传播中同时修改多个token。这种并行计算方式能更有效利用GPU资源，在相同质量下实现数倍的速度提升和成本降低。具体而言，模型从一个初始猜测开始，通过多轮去噪步骤逐步优化整个序列。
- **关键概念**：并行生成、去噪训练、结构性瓶颈、GPU利用率、推理效率
- **实际意义**：大幅降低LLM服务成本，使实时交互应用（如语音助手、代码补全）成为可能；推动AI从云端向边缘设备迁移；改变数据中心建设逻辑，从训练优先转向推理优先。

### 2. 双向上下文的全局重建

- **核心内容**：自回归模型只能利用左侧上下文预测下一个token，而扩散模型可以同时利用左右两侧的上下文信息进行全局重建。训练时，模型学习修复各种类型的错误：可能是预测第一个token（给定其余所有token），也可能是填充中间缺失的部分。这种更丰富的训练任务使模型具有更强的上下文理解能力和数据效率。
- **关键概念**：双向注意力、全局重建、上下文填充、数据效率、训练任务多样性
- **实际意义**：特别适合代码生成等非线性任务；提高模型的数据利用效率，用更少的训练数据达到相同性能；为代码库级别的理解和修改提供新思路。

### 3. 商业化的技术突破

- **核心内容**：Inception是全球首家将扩散语言模型推向商用的公司，其Mercury模型已在多个生产环境中部署。在Copilot Arena代码补全基准测试中，Mercury在质量上与最佳模型并列第一，同时在速度上遥遥领先。模型提供OpenAI兼容的API接口，开发者可以无缝切换。公司为新用户提供1000万免费token用于试用。
- **关键概念**：商用化部署、Copilot Arena、API兼容性、生产环境验证、基准测试第一
- **实际意义**：证明扩散模型不仅是学术概念，已具备实际商业价值；降低开发者迁移成本；推动整个行业重新评估LLM技术路线。

### 4. 延迟敏感场景的优势

- **核心内容**：在语音助手、代码自动补全等对延迟要求极高的应用中，扩散模型展现出明显优势。传统语音助手管道包括ASR→LLM→TTS三个环节，LLM是主要延迟瓶颈。扩散模型可以在保持质量的前提下，将响应时间缩短到毫秒级。开发者报告称，使用Mercury后用户体验显著提升，交互更加流畅自然。
- **关键概念**：语音助手管道、延迟瓶颈、实时交互、用户体验、毫秒级响应
- **实际意义**：使真正自然的人机对话成为可能；推动语音助手从命令式向对话式转变；为客服、教育、医疗等领域的AI应用打开新空间。

### 5. 更优的控制性和安全性

- **核心内容**：扩散模型从一开始就能访问整个生成序列，可以在每个去噪步骤中检查是否满足约束条件（如安全性、品牌一致性等）。这与自回归模型需要等到生成结束才能评估的特性形成鲜明对比。研究团队正在开发技术，让企业客户能够更细粒度地控制模型行为，确保输出符合特定要求。
- **关键概念**：全局约束检查、生成过程引导、安全性保证、品牌一致性、细粒度控制
- **实际意义**：降低企业部署LLM的合规风险；使模型输出更加可预测和可控；为特定行业（金融、医疗）的AI应用提供保障。

### 6. 代码生成的天然优势

- **核心内容**：代码生成本质上不是严格的从左到右任务。开发者经常需要在代码库的不同位置填充功能、重构变量名或进行全局修改。扩散模型的全局视角使其能够更好地理解代码结构和依赖关系。Mercury不仅在自动补全上表现出色，还支持更复杂的代码编辑建议，如删除冗余代码、重命名变量等。
- **关键概念**：代码填充、非线性生成、代码库理解、智能编辑建议、结构感知
- **实际意义**：提升开发者生产力；使AI能够进行更复杂的代码重构；为"对话式编程"提供技术基础。

### 7. 扩散模型的扩展性

- **核心内容**：研究表明，扩散语言模型在训练和推理两个维度都展现出优于自回归模型的扩展性。在推理时，并行计算的优势随着硬件发展愈发明显。在训练时，由于任务的多样性（去噪而非仅预测下一个token），模型表现出更好的数据效率。历史上，计算机科学中更并行的解决方案往往最终胜出。
- **关键概念**：扩展定律、并行计算历史、数据效率、硬件趋势、架构优势
- **实际意义**：为未来AI基础设施投资提供新方向；可能改变芯片设计优先级；预示着计算范式的根本转变。

### 8. 推理能力的突破方向

- **核心内容**：Inception正在开发首个具备推理能力的扩散语言模型。类似于自回归模型的"思考链"，扩散模型可以通过增加去噪步骤来"深思熟虑"。这种推理机制与扩散模型的全局视角相结合，可能产生新的智能涌现。团队认为这将大幅提升模型在规划、数学、逻辑推理等任务上的表现。
- **关键概念**：推理扩散模型、思考链、去噪步骤、智能涌现、规划能力
- **实际意义**：使扩散模型能够处理更复杂的认知任务；为AI智能体提供新的推理架构；可能突破当前LLM的智能上限。

### 9. 多模态融合的统一架构

- **核心内容**：扩散模型已经主导了图像生成（Stable Diffusion、DALL-E）和视频生成领域。如果文本/代码生成也采用扩散架构，将为构建统一的多模态AI系统铺平道路。这种统一架构可以在不同模态间共享知识，例如通过物理教科书理解视频中的物理规律，通过代码理解3D世界的结构。
- **关键概念**：多模态统一、跨模态学习、世界模型、物理理解、知识共享
- **实际意义**：推动通用人工智能（AGI）发展；使AI能够像人类一样综合理解世界；为机器人、自动驾驶等需要多模态理解的应用提供基础。

### 10. 互补架构的协作生态

- **核心内容**：未来可能不是扩散模型完全取代自回归模型，而是两者形成互补生态。由于底层机制不同，两类模型可能产生不同类型的错误。可以设计系统让自回归模型检查扩散模型的输出，反之亦然。这种"集成智能"方法可能产生比单一架构更强大、更可靠的AI系统。类似于人类团队中不同思维方式的互补。
- **关键概念**：架构互补、错误非相关性、集成智能、交叉验证、协作系统
- **实际意义**：提高AI系统的整体可靠性；为关键应用提供多重保障；开创新的AI系统设计范式。

### 11. 从云端到边缘的计算转移

- **核心内容**：扩散模型的高效推理特性使其特别适合部署在资源受限的边缘设备上。随着物联网和边缘计算的发展，AI推理将从集中式的数据中心向分布式的边缘设备迁移。扩散模型可能成为这一转变的关键技术，使智能手表、AR眼镜等设备能够运行复杂的AI模型。
- **关键概念**：边缘AI、分布式推理、资源效率、隐私计算、设备智能化
- **实际意义**：保护用户隐私（数据本地处理）；降低网络延迟；开辟新的硬件市场；推动真正的普适计算时代到来。

### 12. 学术到产业的快速转化

- **核心内容**：Stefano Ermon教授在斯坦福的研究直接转化为Inception的商业产品，展示了AI领域从学术到产业的快速通道。团队在一年内就将实验室原型扩展到商用规模，这种速度在传统技术领域是难以想象的。这反映了AI时代知识转化的新模式：顶尖研究者直接参与商业化，缩短创新周期。
- **关键概念**：技术转化、研究商业化、创新周期、学术创业、快速原型
- **实际意义**：激励更多研究者参与创业；加速AI技术的实际应用；改变高等教育与产业的合作模式；为其他深度技术创业提供范例。

---

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Inception | 史蒂芬诺创立的公司，首个商用扩散LLM提供商，Mercury模型开发者 | ⭐⭐⭐ |
| Stanford University | 史蒂芬诺的研究基地，扩散语言模型早期研发地，Flash Attention发明地 | ⭐⭐⭐ |
| OpenAI | ChatGPT开发商，自回归模型代表，API标准制定者 | ⭐⭐ |
| Google DeepMind | Gemini模型开发者，发布过Gemini Diffusion研究但未商用 | ⭐⭐ |
| Anthropic | Claude模型开发商，自回归模型竞争者 | ⭐⭐ |
| TMU | Copilot Arena基准测试运营方，代码补全评估权威 | ⭐⭐ |
| Linux Foundation | Agency项目（AI智能体协作层）的托管方 | ⭐ |
| World Labs | 李飞飞的公司，开发3D世界模型，潜在合作伙伴 | ⭐ |

---

## 💬 经典金句

> "历史上，计算机科学中更并行的解决方案总是最终胜出，而扩散语言模型从根本上就是为并行而生的。"
> — Stefano Ermon

> "这最终将是一场推理的游戏。人们建造大规模数据中心不是为了训练，而是因为他们预期每个人都将使用AI。"
> — Stefano Ermon

> "扩散模型实际上比自回归模型更容易控制，因为你从一开始就能访问整个对象，可以持续检查是否满足约束。"
> — Stefano Ermon

> "我认为交互永远都会存在需求。如果你想让人类参与其中，速度就是限制因素。"
> — Stefano Ermon

---

## 👤 主要人物

### Stefano Ermon（史蒂芬诺·阿尔蒙）

**身份**：Inception AI联合创始人兼CEO；斯坦福大学计算机科学教授（目前休假中）
**背景**：在生成模型领域深耕超过10年，是扩散模型技术的早期开拓者之一。发明了Flash Attention（现已广泛用于大规模LLM训练）和DPO等关键技术。2023年在斯坦福实验室取得扩散语言模型突破后，创立Inception将技术商业化。
**核心观点**：扩散模型的并行计算优势将使其在推理效率上超越自回归模型，成为下一代LLM的主流架构。他强调，AI的未来在于推理而非训练，而扩散模型正是为高效推理而生。相信多种AI架构的互补协作将产生更强大的智能系统。

---

## 📺 视频类型判断

**访谈对话**：主持人与Stefano Ermon的深度技术访谈，问答形式探讨扩散语言模型的原理、优势和未来发展

---

## 📝 完整翻译

### (0:00 - 5:00) 扩散vs自回归模型核心差异
> 扩散模型并行生成所有token，突破自回归的序列瓶颈

当你思考大多数现有的 LLM 时，比如 ChatGPT、Gemini，它们都被称为自回归模型。这意味着当你问一个问题时，它们会给你一个答案，并且会逐个生成答案，一次生成一个词或我们称之为一个 token。这种生成是顺序的，这就像一个结构性瓶颈，无法克服，因为它内建在自回归公式中。扩散语言模型在底层也使用神经网络。在我们的情况下，它也是一个 transformer 神经网络。但模型训练的任务完全不同。模型不是被训练来预测下一个 token，而是被训练来去除噪声。

Stefano Ermon：大家好，我是 Stefano Ermon，是 Inceptive 的创始人之一和CEO。在创立 Inceptive 之前，我在斯坦福大学做了大约十年的计算机科学教授。我一直在研究生成模型，也就是现在所说的生成式 AI，大约有10年时间。我在扩散模型方面做了很早期的工作，这项技术现在为许多最先进的图像生成系统和视频生成系统提供支持。我研究了各种技术，比如 flash attention、DPO，这些技术现在在行业中被广泛用于构建大规模 LLM 和其他类型的生成式 AI 解决方案。

我在斯坦福的实验室中研究扩散语言模型已有一段时间，试图找出如何让扩散模型不仅在图像和视频上工作，还能在文本和代码生成上发挥作用。去年我的实验室取得了重大突破，我们首次展示了这项技术能够与领先的自回归模型竞争。这促使我创办了这家公司来扩展这些模型，这就是我过去一年左右一直在做的事情。我们在训练第一个商业规模的扩散语言模型方面相当成功，我们称之为 Mercury。

主持人：我很好奇，扩散语言模型能够工作确实很有趣。也许我们会首先谈论这一点，但我想问为什么扩散语言模型相比基于自回归 transformer 的 LLM 有优势？

Stefano Ermon：当然，原因在于当你思考大多数现有的 LLM，比如 ChatGPT、Gemini、Claude，它们也被称为自回归模型。这意味着当你问一个问题时，它们会给你一个答案，并且会逐个生成答案，一次生成一个词或我们称之为一个 token。这种生成是顺序的，这就像一个结构性瓶颈，无法克服，因为它内建在自回归公式中。神经网络被训练来预测下一个 token，这就是你在推理时向它提问时的使用方式。顺序计算通常很难加速，因为如果你必须逐个处理 token，就没有太多可以并行处理的事情。

### (5:00 - 10:00) 扩散模型训练原理
> 通过去噪任务训练，可同时修改多个token

另一方面，扩散语言模型是一次性生成整个答案。神经网络被训练来完善其答案，修正错误。然后在推理时，你使用这种程序，每次神经网络评估都能同时修改多个 token，而不仅仅是一个。我们看到这些模型相比类似质量的自回归模型可以快得多。这就是关键优势。我们能够大幅加速生成速度，基本上是加速 LLM 生成答案所需的时间，并降低成本。这是另一个重大优势。计算更加并行，我们可以更有效地利用 GPU 的并行计算，并且可以有效地为用户降低成本。

主持人：另一个问题，我不知道这是否算优势，但 LLM 的一个问题是我一直在与人们讨论后 transformer 架构，但 LLM 的一个问题是上下文，上下文窗口所需的计算呈二次方增长。所以很快就变得太昂贵了，然后上下文窗口太大会出现各种问题，LLM 会变得困惑或迷失。这是扩散 LLM 会解决的问题吗？

Stefano Ermon：这是个很好的问题，我非常熟悉这些问题。我是 flash attention 论文的原始作者之一，这个技术被广泛用于加速在 transformer 中进行注意力所需的非常昂贵的二次方计算。

我们过去分享过，我很高兴确认我们的扩散语言模型在底层也是基于 transformer 的。我们使用的架构仍然是带有自注意力的 transformer。这意味着在上下文长度方面，我们得到与自回归模型相同的优缺点。我们目前在生产中提供的模型大约有 13 万个 token 的上下文。我们认为可以使用人们为自回归模型使用的技术来扩展它，但在优缺点方面，与自回归模型相比没有真正的优势或劣势。我们也使用了不同的架构，就像在自回归 LLM 领域的人们一直在实验状态空间模型、Mamba 和替代架构一样，这在扩散语言模型的背景下也是可能的。

### (10:00 - 15:00) 预测与重建机制
> 利用双向上下文进行全局重建，而非仅左到右预测

你可以在扩散语言模型内部使用任何你想要的神经网络架构，我们有一些原型，用这些替代架构替换了 transformer，你会得到这些架构的优势和劣势。所以扩散与架构，这些是你可以用来加速计算或减少内存占用或权衡你可能关心的不同性能特征的正交轴。

主持人：对于听众和我自己来说，你能解释扩散是如何工作的以及 transformer 在其中扮演什么角色吗？我知道在训练期间注入噪声，然后将噪声逆转到输出，但如果你能为外行人解释一下，那就太好了。

Stefano Ermon：当然。自回归 LLM 和扩散模型在底层都依赖神经网络。有一个神经网络接受句子作为输入，然后输出某些内容。在自回归 LLM 的背景下，这个神经网络（通常是 transformer）接受句子作为输入，然后预测下一个 token，下一个应该附加到该句子以使其有意义的词。你可以通过向这些模型提供大量文本或代码来训练这个神经网络做得相当好，然后在经过大量训练迭代后，它们在理解什么样的 token 有意义以及应该如何完成句子方面变得相当好。

扩散语言模型在底层也使用神经网络，在我们的情况下也是 transformer 神经网络。但模型训练的任务完全不同。模型不是被训练来预测下一个 token，而是被训练来去除噪声。同样，你从可能从互联网上获取的句子或代码开始，然后人为地添加噪声。也许你掩盖一些 token 或改变一些 token 的值，然后训练神经网络接受这个损坏的句子作为输入并尝试修复错误，尝试给你回原始句子。所以这是一个不同的训练目标。它不是下一个 token 预测，而更像是句子的全局转换，你尝试修复错误。

在推理时，这两种模型的使用方式完全不同。传统的自回归模型只是预测下一个 token，然后你输入它，然后预测下一个 token，然后输入它，预测下一个 token，如此往复。扩散语言模型相反，它从对答案应该是什么的猜测开始，理论上可以是任何东西，然后你将整个猜测通过神经网络。神经网络会修复一些错误。

### (15:00 - 30:00) Part 2

### (15:00 - 20:00) 控制性与训练方法
> 扩散模型更易控制，支持RLHF等传统训练技术

扩散语言模型的优势在于它不仅仅是按顺序逐个处理 token。它实际上能够利用左侧和右侧的上下文来修复错误。比如说，如果句子的第一个 token 缺失，但其他所有内容都是已知的，扩散语言模型可以尝试预测第一个 token 应该是什么，基于它知道的所有其他信息。这有点像是对自回归模型通常执行任务的泛化，在自回归模型中，你总是只从左到右预测一个 token。

在扩散语言模型中，你有一套更丰富的任务需要在训练期间能够解决，这可能只是预测下一个 token，但也可能是在你知道右侧所有内容的情况下预测第一个 token。所以这更像是从右到左的预测。实际上情况甚至更复杂，因为句子中可能存在各种各样的错误，模型需要弄清楚如何同时修复所有这些错误。但在某种意义上，它们是相似的，因为在两种情况下，你都试图重建输入的一部分。

在自回归语言模型的情况下，你总是从左到右逐个 token 地重建。在扩散语言模型的情况下，你一次性全局地重建整个句子。所以这是一个更复杂的重建去噪任务，但我们仍然可以训练神经网络来解决它。

主持人：图像扩散模型众所周知很难控制。它们确实会给你输出，但可能不是你想要的，即使它符合提示。这个问题在语言方面是如何体现的？

研究员：在我看来，扩散模型实际上比典型的自回归模型更容易控制。原因是在扩散模型中，你从一开始就可以访问整个对象。所以你从一开始就知道它是否与提示一致，是否满足某些安全约束或你想要用来引导生成的任何控制信号。在整个生成过程中，你总是知道是否朝着正确的方向前进，是否满足了约束。

### (20:00 - 30:00) 性能与商业化进展
> Inception是唯一实现商用级扩散LLM的公司

这就是为什么扩散模型相对容易控制，人们想出了很多技术来引导生成过程朝着特定方向发展。因为你可以检查是否满足了约束，并且可以将生成推向你想要的方向。相反，如果你考虑自回归模型，你需要等到最后一个像素才能知道是否满足了约束。所以我认为扩散模型比自回归模型更容易控制。

我们正在开始探索这些优势在扩散语言模型的背景下的应用。仍然有一些问题会出现，你希望模型以某种方式表现，当然你希望模型是安全的，但也许你希望模型符合品牌形象。有更复杂的方式来定义你希望模型做什么，我们开始探索使用扩散语言技术的方法，让我们的客户能够以更细粒度的方式控制语言模型。

主持人：训练这些模型的过程是怎样的？是否与训练自回归模型相同？

研究员：在高层次上，正如我讨论的，我们训练这些神经网络来去噪，训练它们从句子中去除噪声。我们没有透露很多细节，比如确切的训练目标或模型的确切大小，或我们使用的训练数据量。有一些公开可用的论文研究了扩散语言模型与自回归模型的扩展定律和性能。

学术文献中有一些证据表明，扩散语言模型在数据效率方面要高得多。直觉是，因为你训练模型解决许多不同类型的任务，而不仅仅是预测下一个 token，比如以任何顺序进行预测，它们往往更具数据效率。所以你需要更少的训练数据来达到某种质量水平。当然，一切都取决于细节，你可以做各种事情，但我们有自己的配方，我们看到它表现得非常非常好。

除此之外，也可以进行基于人类反馈的强化学习。也可以使用可验证的奖励进行强化学习，如果你在编程或数学等有可验证奖励的环境中，有唯一正确或错误的答案可以用来检查结果的质量。在内部，我们已经能够使用所有这些技术，并将它们适应到我们正在处理的不同类型的生成模型中。

主持人：在评估方面，你们是使用其他扩散模型进行评估，还是使用自回归 LLM？

研究员：评估 LLM 是出了名的困难，我们内部跟踪性能的方式是使用基准测试。有各种社区开发的基准测试，试图衡量模型在人们关心的不同任务上的表现，比如数学能力、指令遵循能力、问答能力、用不同语言编写代码的能力。

在内部，我们监控模型在各种基准测试中的性能，然后不断调整我们的训练方法、架构和推理方案，试图获得尽可能好的性能，同时保持计算成本低，也试图让模型尽可能高效。所以在评估方面，实际上完全相同，与传统实验室的做法非常相似。

主持人：到目前为止，你们模型的参数规模是多少？

研究员：不幸的是，我们不能透露模型的确切大小。我们处在一个非常竞争的领域，这是我们目前无法透露的信息。

主持人：有趣的是竞争领域。还有其他人在研究扩散 LLM 吗？

### (30:00 - 45:00) Part 3

### (30:00 - 40:00) Mercury模型与代码应用
> 在Copilot Arena基准测试中排名第一

研究员：是的，我们从代码开始是因为，首先我们都是计算机科学家，所以代码对我们来说很亲近，我们能够理解模型是否运行良好。这对扩散语言模型来说也是一个非常有趣的应用，因为它的自回归性质相对较少。从左到右的偏向在写文档时确实有意义，这有点像我们的写作方式，虽然我认为可能仍然存在一些粗糙的生成过程，我们从大纲开始然后填充细节。

但至少如果你考虑代码，从左到右的偏向要少得多。如果你考虑不同的文件，它们不一定按特定顺序排列，有很多任务你真正需要能够查看整个代码库并填充你要开发的特定函数或功能。这正是扩散模型可以发光发热的地方，因为它们不受约束，不需要从左到右进行，这种从左到右的想法并没有被内置进去。

这就是我们最初专注于代码的原因，因为这是我们期望看到巨大改进的领域，而且从商业角度来看也非常重要。这是我们看到 LLM 大量使用的地方，人们从中获得最大价值，我想说今天人们使用 LLM 来更快地编写代码、查找编码问题等等，这些都是人们非常兴奋的事情，所以这对我们来说是很自然的第一步。

主持人：这些模型可以像自回归模型一样进行微调吗？

研究员：当然可以。是的，我们可以为特定任务微调它们，实际上这是我们与企业客户合作时的一种模式。有时如果他们能够提供内部数据或他们关心的特定数据，我们实际上能够在他们的数据上微调我们的模型，并在他们关心的任务上提供更高的性能。

主持人：你说你不能透露参数数量，但是否与其他模型相同，参数越大，模型越准确？

研究员：是的，通常是这样。模型越大，性能越高，只要你用比例更大的训练数据量来训练它，所以有一些注意事项，但通常你可以期望更大的模型表现更好，但它们也变得更昂贵、更慢，这就是我们面临的挑战。

主持人：从你的角度来看，优势是速度和成本或功耗效率，还是准确性？你如何衡量与自回归模型的准确性对比？

研究员：这取决于你如何考虑。如果你看看今天存在的不同 LLM，在质量（准确性）、成本和速度之间总是存在权衡。如果你想要最快的模型，它们通常不是最准确的，因为它们更小，人们需要使用各种技巧使它们尽可能高效。

我们能够实现的是，对于给定的速度配置，我们是最准确的模型。我们不是整体最准确的模型，所以有一些由 OpenAI、Anthropic 和 Gemini 训练的更大模型，与我们今天的水平相比，在绝对意义上更准确。但在相同速度类别中，我们是最准确的模型。

我们的想法是在质量和速度之间存在帕累托前沿，我们已经能够转移这个帕累托前沿，至少对于某个质量水平。公司的下一阶段是继续提高扩散语言模型的质量，直到我们真正赶上前沿级别的质量智能，这种智能目前只在少数自回归模型中可用。

主持人：你认为扩散语言模型是自回归 LLM 的替代品吗？还是你认为扩散 LLM 与自回归 LLM 竞争，希望有朝一日能够超越自回归 LLM？

### (40:00 - 45:00) 语音助手与延迟优化
> 在延迟敏感场景替代小型自回归模型

研究员：我们当然认为它们在功能上可以相互替代。无论何时你可以使用自回归 LM，你也可以使用扩散语言模型。从这个角度来看，它们通常是同一问题的竞争解决方案。即使现在，我们也在超越自回归 LLM，我们的客户正在从自回归模型转向扩散语言模型，因为有许多低延迟应用，你需要能够快速给开发者或客户答案，因为你在运行语音代理。

有各种延迟很重要的应用，对于这种延迟预算，我们的模型实际上比人们为了足够快速输出答案而使用的自回归模型更准确。当然问题是，随着我们扩展模型，随着扩散语言模型变得更好，会发生什么。我们不知道。

我可以看到未来所有模型都将基于扩散。通常如果我回顾计算机科学的历史，更并行的解决方案会获胜，而扩散语言模型从根本上就是为并行而构建的。这就是我们押注于此的原因。但我们不知道。也有可能多种技术将共存，也许一种更适合某些用例，另一种更适合其他用例。

它们完全可能都不完美，都会犯错误，但只要错误是不同的，彼此不相关，那么拥有基于自回归模型和基于扩散模型的代理就会有价值。它们可以相互交互，相互学习，通过拥有完全不同的智能栈来构建智能机器，你就会获得价值。

### (45:00 - End) Part 4

### (45:00 - 52:20) 未来发展方向
> 推理能力、多模态融合、世界模型集成

我们仍然只是触及了扩散语言模型所蕴含的可能性的皮毛。这就是我对这些技术如此兴奋的原因，因为一切都非常新颖、非常新鲜。我认为仍然有很多触手可及的机会。很多设计选择我认为都是次优的。我们经常只是采用在自回归模型中有效的任何方法，然后在可能的情况下将其应用到扩散语言模型的上下文中。但我认为这些选择不一定是最佳的，考虑到架构不同，考虑到模型根本不同，所以模型的研发仍然有很多机会。

我们正在研究的一个方向是推理扩散语言模型。这是自回归模型具备的能力，也是它们能够显著提升质量的方式之一——在输出答案之前先思考的能力。在扩散语言模型的上下文中，有一些有趣的方法可以实现这种能力，这是我们在研发方面大力投资的领域之一，目标是开发出第一个推理扩散语言模型。

主持人： 哇，这太有趣了。这会对它们在智能体系统中的应用产生巨大影响，对吗？

完全正确。因为如果你想到智能体，经常涉及大量的规划和推理，一旦你有一个推理模型作为底层支撑，这些能力就会变得更强大。

主持人： 你们正在研究多模态吗？我的意思是扩散已经是图像和视频生成领域的主导技术了。

这是我们对扩散语言模型如此兴奋的另一个原因，因为我们已经知道扩散是图像生成、世界模型、视频生成的最佳模型。一旦我们突破并找到如何进行文本和代码生成，这将为能够处理不同模态并在不同模式间学习的单一生成AI系统打开道路。这是我们非常兴奋的事情，我们正在研究这个方向，希望能有一些模型和公告发布。

主持人： 我将与Fei-Fei Li，你的同事或者说World Labs进行对话，我想她是这样称呼的。这个大理石模型非常出色。你能谈谈扩散LLM如何融入世界模型吗？因为世界模型很神奇，但真正神奇的是当你有一个能够感知世界并理解物理定律的世界模型，然后将其与语言结合时。

完全正确。我认为一旦你结合不同的模态，就会有非常有趣的机会，因为正如你所说，你可以有一个在物理书籍、工作原理和仿真代码上训练的模型，它可以编写代码来弄清楚将要发生什么，并对未来做出预测。我认为这将带来更强大的世界建模能力。

如果这个模型的底层不仅仅在视频数据或3D数据上训练，而是真正通过所有可获得的文献、所有应该获得的科学知识来理解世界。

主持人： 有什么我没有问到的我们应该谈论的吗？

我认为Inception令人兴奋和独特的一点是模型真正可用。这不仅仅是研究原型，而是开发者现在就可以使用的模型。企业可以构建应用程序，他们可以用扩散语言模型替代自回归LLM。现在一切都向后兼容，API是相同的，就像OpenAI兼容的端点一样。

所以这正在发生，我认为你的观众可能会有兴趣看到这一点，并意识到这种转变，为可能改变他们构建应用、开发应用程序等方式的变化做好准备。

主持人： 在我开始录制之前我们谈论了幻觉问题。你们如何处理幻觉？在自回归方面确实已经做了很多工作。扩散LLM是否采用相同的策略？这是完善这些模型的重点吗？


---

*生成时间: 2026-01-05 04:58:52*
*由 YouTube Monitor & Translator (Claude CLI) 生成*