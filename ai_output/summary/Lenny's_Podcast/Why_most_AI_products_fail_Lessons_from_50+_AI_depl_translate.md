# Why most AI products fail: Lessons from 50+ AI deployments at OpenAI, Google & Amazon

## 📹 视频信息

- **频道**: Lenny's Podcast
- **发布日期**: 2026-01-11
- **时长**: 1:26:21
- **原始链接**: [https://www.youtube.com/watch?v=z7T1pCxgvlA](https://www.youtube.com/watch?v=z7T1pCxgvlA)

---

Looking at this YouTube transcript, I need to analyze a podcast episode titled "Why most AI products fail: Lessons from 50+ AI deployments at OpenAI, Google & Amazon" from Lenny's Podcast channel. The episode features Kiti Bottom (works on Codex at OpenAI) and Aishwarya Ranti (early AI researcher at Alexa and Microsoft) discussing their experiences with over 50 AI product deployments.

The video is 1 hour 26 minutes long and covers critical insights about building successful AI products, including the key differences between building AI vs non-AI products, common pitfalls, and a practical framework for development.

Let me now generate the comprehensive summary following the required format.

---

## 视频开头信息

> 本文内容整理自 OpenAI Codex 工程师基蒂·鲍顿（Kiti Bottom）和前亚马逊 Alexa、微软 AI 研究员艾什瓦利亚·兰蒂（Aishwarya Ranti）在 Lenny's Podcast 频道的技术访谈。

---

## TL;DR（一句话核心洞察）

> 成功的 AI 产品开发需要从"高控制、低自主"逐步过渡到"低控制、高自主"的迭代模式，通过持续校准（Continuous Calibration）和持续开发（Continuous Development）的双循环框架，在不损害用户体验的前提下逐步赋予 AI 更多决策权，而"痛苦是新的护城河"——企业需要经历反复试错的痛苦过程才能真正掌握 AI 产品的构建之道。

---

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-06:00 | AI产品构建的根本差异 | AI产品具有非确定性和自主性-控制权权衡两大核心特征 |
| 06:00-20:00 | 渐进式自主权框架 | 从建议→草稿→自主执行的三步演进路径 |
| 20:00-32:00 | 成功企业的三角模型 | 领导力、文化、技术进步共同决定AI转型成功 |
| 32:00-45:00 | 评估系统的真相 | Evals和生产监控缺一不可，需平衡使用 |
| 45:00-60:00 | CCCD开发框架详解 | 通过双循环持续校准用户行为和AI响应 |
| 60:00-75:00 | 多智能体系统误区 | 监督者模式优于点对点通信的多智能体架构 |
| 75:00-86:21 | 2026年AI展望 | 主动式后台智能体和多模态体验将成主流 |

---

## 📊 核心论点

### AI产品与传统软件的根本性差异

- **核心内容**：传统软件像Booking.com有确定的决策引擎和工作流程，用户通过点击按钮、填写表单完成预订。而AI产品使用自然语言界面，用户可以用无数种方式表达意图，AI（LLM）的输出也是概率性的黑箱。这种输入、输出和处理过程的三重不确定性，使得开发者无法预测用户行为和系统响应，必须在部署后持续观察和校准。
- **关键概念**：非确定性API、概率性输出、自然语言界面、黑箱模型、行为校准
- **实际意义**：企业必须彻底改变开发流程，从瀑布式转向迭代式；产品经理和工程师需要更紧密协作，共同查看AI轨迹；传统的需求文档和技术规范已经不够用。

### 自主性-控制权权衡原则（Agency-Control Tradeoff）

- **核心内容**：每当赋予AI系统更多决策能力（自主性），就必然失去一些控制权。这不是简单的二选一，而是需要通过渐进式演进来平衡。以客服系统为例：V1仅做路由分类（高控制、低自主），V2提供草稿建议（中等控制、中等自主），V3自主处理和解决工单（低控制、高自主）。只有当AI在前一阶段证明了可靠性，才能进入下一阶段。
- **关键概念**：渐进式自主、信任构建、人机协作层级、决策权转移、可靠性验证
- **实际意义**：避免了Air Canada式的灾难（AI擅自创造退款政策导致法律纠纷）；让企业能够在控制风险的前提下逐步释放AI潜力；为不同风险级别的应用场景提供了清晰的实施路径。

### "痛苦是新的护城河"理念

- **核心内容**：在AI时代，成功的企业护城河不再是先发优势或独特功能，而是经历过的痛苦迭代过程。企业需要经历反复的试错：尝试不同的提示词策略、处理各种边缘情况、理解用户的意外行为模式、修复数据层的混乱分类体系。这些痛苦的经验积累形成了组织级的隐性知识，竞争对手即使拥有相同的技术栈也无法快速复制。
- **关键概念**：迭代学习、组织知识、经验积累、试错成本、隐性护城河
- **实际意义**：一键式AI解决方案大多是营销噱头；真正的AI价值需要4-6个月的持续优化；企业应该拥抱而非回避实施过程中的困难。

### CCCD框架：持续校准与持续开发

- **核心内容**：CCCD框架包含两个循环。持续开发循环：确定能力范围→整理数据集→搭建应用→设计评估指标→部署运行。持续校准循环：分析用户行为→发现错误模式→应用修复→设计新指标。两个循环相互作用，通过不断发现意外行为模式并将其纳入开发流程，实现系统的渐进式改进。这是AI版本的CI/CD。
- **关键概念**：双循环模型、行为校准、错误模式识别、飞轮效应、迭代改进
- **实际意义**：解决了74%企业面临的AI可靠性问题；提供了系统化的方法论来管理AI的不确定性；让团队能够在保护用户体验的同时持续改进系统。

### 评估系统的多维视角

- **核心内容**：业界对"Evals"存在严重的语义扩散问题。数据标注公司说的Evals是错误分析，产品经理写的Evals是预期行为定义，工程师的Evals是LLM判断器。真相是：离线评估只能捕捉已知错误，生产监控才能发现新兴模式。成功的团队同时使用显性信号（点赞/点踩）和隐性信号（重新生成=不满意），在高流量场景下通过信号筛选需要人工审查的案例。
- **关键概念**：语义扩散、离线评估、生产监控、显性/隐性信号、错误模式发现
- **实际意义**：单纯依赖任一方法都会失败；需要建立完整的反馈闭环；Codex团队的实践证明了平衡方法的有效性。

### 成功企业的三角支撑模型

- **核心内容**：AI转型成功需要三个支柱。领导力：CEO必须重建直觉，如Rackspace前CEO每天4-6点"追赶AI进展"，周末写代码保持hands-on。文化：从"AI会取代你"的恐惧文化转向"AI增强你10倍能力"的赋能文化，让主题专家愿意分享知识而非自我保护。技术：深度理解现有工作流程，识别哪些适合AI、哪些需要确定性代码、哪些必须人工处理。
- **关键概念**：直觉重建、赋能文化、工作流解构、技术债务处理、主题专家参与
- **实际意义**：自上而下的推动是必需的，底层工程师无法独自推动变革；消除员工恐惧能释放10倍生产力；80%的AI工程工作是理解业务流程而非构建模型。

### 企业数据的混沌现实

- **核心内容**：企业数据远比想象的混乱。以零售公司为例：分类体系中"鞋子"下同时存在"女鞋/男鞋"和"为女性/为男性"两套并行分类，后者自2019年后未更新但仍存在系统中。函数命名混乱如"get_customer_data_v1"和"get_customer_data_v2"并存。人类员工凭经验知道哪个该用，但AI需要明确的上下文。这种技术债务是AI部署的最大障碍。
- **关键概念**：分类混沌、遗留系统、隐性规则、技术债务、上下文工程
- **实际意义**：一键部署的AI agent在企业环境下几乎不可能成功；数据清理和上下文构建占据大部分实施时间；需要主题专家帮助AI理解那些"不成文的规则"。

### 编码智能体的独特定位

- **核心内容**：编码智能体与其他领域AI根本不同：它们为工程师构建，强调可定制性而非解决固定工作流。Codex成功的秘密在于平衡approach：有核心功能的评估集防止退化，但更依赖快速的用户反馈循环。代码审查功能的爆发式增长证明了这一点——用户如果被错误的代码建议惹恼会直接关闭功能，这种即时反馈比任何评估集都有效。
- **关键概念**：工程师工具、可定制性、快速反馈、A/B测试、社交媒体监听
- **实际意义**：2025-2026年编码智能体仍被低估，渗透率极低；成功的关键是倾听用户而非预设场景；"vibes"在某种程度上确实重要。

### 用户行为的动态演化

- **核心内容**：某银行为承销商构建的AI系统前3个月运行良好，帮助处理30-40页贷款申请。3个月后，用户开始提出意外请求："对于这样的案例，以前的承销商是怎么处理的？"这看似自然的功能扩展，背后需要完全不同的技术架构：理解"这样的案例"的语义（收入范围？地理位置？），检索历史文档，分析决策模式。用户期望的自然演进可能需要系统的根本性重构。
- **关键概念**：期望演进、功能蔓延、语义理解、历史数据挖掘、架构重构
- **实际意义**：AI产品不能"一次构建永久使用"；需要预留架构弹性应对用户期望的增长；成功会带来新的挑战。

### 多智能体系统的架构选择

- **核心内容**：将复杂任务分解给多个专门agent看似合理，但点对点通信的"闲聊协议"架构极难控制。如客服场景中，多个agent自主通信可能导致不可预测的回复，安全护栏需要复制到每个agent。成功的模式是监督者架构：一个主agent协调多个子agent，保持清晰的控制链。当前模型能力还不足以支撑完全自主的多agent协作。
- **关键概念**：监督者模式、点对点风险、控制链、护栏复制、架构模式选择
- **实际意义**：避免过度工程化；集中式协调优于分布式自主；在客户端应用中尤其要谨慎使用多agent。

### 主动式后台智能体的未来

- **核心内容**：2026年的关键趋势是AI从被动响应转向主动预测。GPT的定期摘要只是开始，未来的编码agent会在你醒来前修复5个Linear工单并准备好代码审查。关键突破点是agent能"看到"更多工作流上下文，理解你优化的指标和日常活动模式。这不是取代人类，而是让AI像优秀的助手一样预测需求、提前准备。
- **关键概念**：主动式AI、上下文感知、工作流集成、预测性协助、后台处理
- **实际意义**：AI集成的深度比广度更重要；最有价值的AI是那些深度理解你工作模式的；未来的竞争在于谁能构建更好的上下文理解。

### 多模态体验的爆发前夜

- **核心内容**：人类是多模态生物，语言只是最后进化出的表达方式。访谈中说话者会观察听众的肢体语言实时调整表达。2025年在生成和理解上都有进展，但仍有大量手写文档和混乱PDF无法被最好的模型解析。Demis Hassabis透露DeepMind正在整合图像模型、LLM和世界模型（Genie），这将解锁海量未被利用的数据，创造更接近人类的交互体验。
- **关键概念**：多模态融合、非语言信号、文档理解、世界模型、交互丰富度
- **实际意义**：大量"无聊但重要"的任务（文档处理）将被自动化；人机交互将超越纯文本；掌握多模态能力的企业将获得数据优势。

---

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| OpenAI | Codex开发方，嘉宾工作单位，GPT系列产品经验 | ⭐⭐⭐ |
| Google/DeepMind | 嘉宾曾就职，Demis谈多模态方向，Genie世界模型 | ⭐⭐⭐ |
| Amazon/Alexa | 嘉宾早期AI研究经历，语音助手经验 | ⭐⭐ |
| Microsoft | 嘉宾研究背景，35篇论文发表 | ⭐⭐ |
| Databricks | 74%企业AI可靠性调研来源 | ⭐⭐ |
| Air Canada | AI创造虚假退款政策的失败案例 | ⭐⭐ |
| Rackspace | 前CEO的AI学习模式案例 | ⭐ |
| Whisper Flow | 概念化转录工具推荐 | ⭐ |
| Raycast | 生产力工具推荐 | ⭐ |

---

## 💬 经典金句

> "如果不把点连起来向前看，就只能在回头时才能看清。"
> — Kiti Bottom（引用乔布斯）

> "他们说这不可能完成，但傻瓜不知道，所以他还是做到了。"
> — Aishwarya Ranti（引用父亲）

> "痛苦是新的护城河。"
> — Kiti Bottom

> "如果未经审视的生活不值得过，那么未曾活过的生活值得审视吗？"
> — Aishwarya Ranti（引用《当呼吸化为空气》）

> "建立AI产品80%的时间是理解工作流程，而不是构建花哨的模型。"
> — Aishwarya Ranti

---

## 👤 主要人物

#### Kiti Bottom（基蒂·鲍顿）

**身份**：OpenAI Codex团队工程师
**背景**：过去十年在Google和Kumo构建AI/ML基础设施，深度参与编码智能体开发
**核心观点**：强调平衡approach，认为纯评估集或纯生产监控都不够，需要结合使用。提出"痛苦是新的护城河"理念——企业通过痛苦的迭代过程积累的知识才是真正的竞争优势。坚信持续性比聪明更重要，2025-2026年编码智能体仍被严重低估。

#### Aishwarya Ranti（艾什瓦利亚·兰蒂）

**身份**：AI产品专家，Maven平台排名第一的AI课程讲师
**背景**：前Amazon Alexa和Microsoft早期AI研究员，发表超过35篇研究论文，领导和支持过50+企业AI部署
**核心观点**：AI产品构建的关键在于理解非确定性和自主性-控制权权衡。成功需要领导力、文化、技术的三角支撑。反对过度工程化，强调问题优先而非技术优先。认为设计和判断力在AI时代比执行力更稀缺。

---

## 📺 视频类型判断

**访谈对话**：技术深度访谈，主持人与两位嘉宾的三方对话形式

---

## 📝 完整翻译

### (0:00 - 8:00) Part 1

**(0:00 - 1:20)**

我们之前合作写了一篇客座文章，其中有一个非常关键的洞察：构建 AI 产品与构建非 AI 产品是截然不同的。

Aishwarya Radanti：大多数人往往忽视非确定性。你不知道用户会如何使用你的产品，你也不知道 LLM 可能会如何回应。第二个区别是代理控制权衡。每当你将决策能力交给代理系统时，你实际上是在放弃一定程度的控制权。

**(1:20 - 2:40)**

这显著改变了你构建产品的方式。所以我们建议循序渐进地构建。当你从小处开始时，它迫使你思考我要解决的问题是什么。在 AI 的所有进步中，一个容易滑向的陷阱是不断思考解决方案的复杂性，而忘记你试图解决的问题。

这不是关于在你的竞争对手中成为第一个拥有智能体的公司。而是你是否建立了正确的飞轮，以便能够持续改进。

**(2:40 - 4:00)**

Lenny Rachitsky：在成功构建 AI 产品的公司中，你看到了什么样的工作方式？

Kiti Bottom：我曾与现在 Rackspace 的 CEO 合作过。他每天早上都会有一个时间段，从早上4点到6点，专门用来"了解 AI"。领导者必须重新变得亲力亲为。你必须接受这样的事实：你的直觉可能不对，你可能是房间里最愚蠢的人，而你想要向每个人学习。

**(4:00 - 5:20)**

Lenny Rachitsky：你认为 AI 的下一年会是什么样子？

Aishwarya Radanti：坚持是极其宝贵的。目前在任何新领域成功构建的公司都在经历学习、实施和理解什么有效、什么无效的痛苦。痛苦是新的模式。

今天我的嘉宾是 Aishwarya Radanti 和 Kiti Bottom。Kiti 在 OpenAI 从事编解码器工作，过去十年在 Google 和 Kumo 构建 AI 和机器学习基础设施。Ash 是 Alexa 和微软的早期 AI 研究员，发表了超过35篇研究论文。

**(5:20 - 6:40)**

他们共同领导和支持了50多个 AI 产品部署，涉及亚马逊、Databricks、OpenAI、Google 以及初创公司和大型企业。他们还在 Maven 上教授排名第一的 AI 课程，向产品领导者传授他们在构建成功 AI 产品方面学到的所有关键经验。

这一集的目标是为你和你的团队节省大量痛苦、折磨和试图构建 AI 产品时浪费的时间。无论你是否已经在努力让产品运行，或者想要避免这种困扰，这一集都适合你。

**(6:40 - 8:00)**

Lenny Rachitsky：Ash 和 Kiti，非常感谢你们的到来，欢迎来到播客。

Aishwarya Radanti：谢谢。谢谢邀请我们，我们对此非常兴奋。

Lenny Rachitsky：让我为今天的对话设定一个基础。你们两位自己构建了很多 AI 产品，与许多构建 AI 产品、努力构建 AI 产品、构建 AI 智能体的公司进行了深度合作。你们还教授关于成功构建 AI 产品的课程，你们的使命就是减少人们在构建 AI 产品时不断经历的痛苦、折磨和失败。

为我们今天的对话奠定一些基础，你们在尝试构建 AI 产品的公司中看到了什么？什么进展顺利？什么进展不顺利？

### (8:00 - 16:00) Part 2

**(8:00 - 9:15)**

确实。我想确保我们强调正确的要点。构建 AI 系统和软件系统有很多相似之处，但也有一些根本性的差异会改变你构建软件系统与 AI 系统的方式。其中一个大多数人往往忽视的是非确定性。你基本上是在使用非确定性 API，而不是传统软件。这意味着什么？为什么这会影响我们？在传统软件中，你基本上有一个非常明确的决策引擎或工作流程。想想 Booking.com 这样的网站，你有一个意图，比如你想在旧金山预订两晚酒店等等。产品已经构建好，你的意图可以转化为特定的操作，你点击一堆按钮、选项、表单等等，最终实现你的意图。

**(9:15 - 10:30)**

但现在在 AI 产品中，那一层已经完全被一个非常流畅的界面所取代，主要是自然语言，这意味着用户可以字面上想出无数种方式来表达或传达他们的意图。这改变了很多事情，因为现在你不知道用户将如何行为。这是输入端的情况。而输出端，你也在使用非确定性概率 API，也就是你的 LLM (大语言模型)。LLM 对提示措辞非常敏感，而且它们基本上是黑盒。所以你甚至不知道输出表面会是什么样子。

**(10:30 - 11:45)**

所以你不知道用户可能如何与你的产品交互，你也不知道 LLM 可能如何响应。现在你正在处理输入、输出和一个过程，而你对这三者都不太理解。你试图预测行为并为之构建。对于智能体系统，这变得更加困难。这就是我们谈到的第二个差异，即代理控制权衡。我们的意思是，我很震惊这么多人不谈论这一点。他们极其痴迷于构建自主系统，能为你工作的智能体。但每次你将决策能力或自主权移交给智能体系统时，你就在放弃一些控制权。

**(11:45 - 13:00)**

当你这样做时，你想确保你的智能体赢得了你的信任，或者它足够可靠，你可以允许它做决策。这就是我们谈论的代理控制权衡，即如果你给你的 AI 智能体或 AI 系统更多代理权（做决策的能力），你也在失去一些控制权，你想确保智能体或 AI 系统已经赢得了那种能力，或者随着时间建立了信任。

**主持人**：总结一下你们在这里分享的内容，本质上人们长期以来一直在构建软件产品。我们现在处于一个世界，你构建的软件是非确定性的，可以做不同的事情。正如你所说，你去 Booking.com 找酒店，每次都会是相同的体验，你会看到不同的酒店，但这是可预测的体验。

**(13:00 - 14:15)**

而对于 AI，你无法预测它每次都会是完全相同的，是你计划的那样。另一个是代理权和控制权之间的权衡，AI 为你做多少与人应该仍然负责多少之间的权衡。我听到的重点是，这显著改变了你应该构建产品的方式，我们将讨论对产品开发生命周期应该如何改变的影响。在我们深入讨论之前，你还有什么要补充的吗？

**Aishwarya**：这种区别确实需要在你开始构建时存在于你的脑海中，这是关键点之一。例如，想想如果你的目标是徒步半圆顶，你不会每天都开始徒步，而是从小的部分开始训练自己，然后慢慢提高，最终达到目标。我觉得这与你想要构建 AI 产品的方式极其相似。

**(14:15 - 15:30)**

当你不从第一天就开始使用拥有公司所有工具和所有上下文的智能体，并期望它能工作，或者甚至在那个级别上修修补补。你需要刻意地从影响最小、人类控制更多的地方开始，这样你就能很好地掌握当前的能力是什么，我能用它们做什么，然后慢慢倾向于更多的代理权和更少的控制权。这给你信心，让你知道这是我面临的特定问题，AI 可以解决这种程度的问题，然后让我接下来思考我需要引入什么上下文，我需要添加什么样的工具来改善体验。

**(15:30 - 16:00)**

我觉得这既是好事也是坏事。好的一面是你不必看到外部世界的复杂性，比如所有这些花哨的 AI 智能体，觉得我做不到。每个人都是从非常简约的结构开始，然后逐步发展的。坏的一面是，当你试图将这些一键式智能体构建到你的公司时，你不必被这种复杂性所压倒，你可以慢慢升级。这极其重要，我们一再看到这种重复模式。

### (16:00 - 24:00) Part 3

**(16:00 - 17:15)**

太棒了。我认为这里的更高层次想法是，对于AI系统，一切都关乎行为校准。提前预测你的系统如何表现是极其困难的。那么你该怎么办呢？你要确保不会破坏客户体验或最终用户体验。你保持现有体验不变，然后减少人类拥有的控制权，而且没有单一的正确做法。你可以决定如何约束这种自主性。

一个不同的约束自主性的例子是预授权用例。保险预授权是一个非常适合AI的用例，因为临床医生花费大量时间预授权血液检查、MRI等项目。有些情况是比较容易处理的，比如MRI和血液检查，因为一旦你了解患者信息，就更容易批准，AI可以处理这些，而像侵入性手术等则风险更高。你不想自主地做这些事情。所以，你可以确定哪些用例应该通过人工干预层，哪些用例AI可以方便地处理。在整个过程中，你还要记录人类在做什么，因为你想建立一个飞轮，可以用来改进你的系统。所以你本质上是在不破坏用户体验、不侵蚀信任的同时，记录人类本来会做的事情，这样你就可以持续改进你的系统。

**(17:15 - 18:30)**

让我给你一些你推荐的这种渐进方式的更多例子。我花这么多时间在这里的原因是，这是你建议的一个真正关键部分，帮助人们构建更成功的AI产品。这个想法就是从高控制、低代理权的缓慢开始，然后随着时间推移，一旦你建立了信心，知道它在做正确的工作，就逐步提升。

比如你在帖子中分享的几个例子。假设你在构建一个编码助手。V1版本只是建议内联补全和样板代码片段。V2版本是生成更大的代码块，比如测试或重构，供人类审查。然后V3版本是直接应用更改并自主开启PR。

另一个例子是营销助手。V1版本是起草邮件或社交文案，就像"这是我会做的"。V2版本是构建多步骤活动并运行活动然后启动，V3版本是直接启动、A/B测试并跨渠道自动优化活动。

**(18:30 - 19:45)**

太棒了。再次总结一下我们到目前为止分享的建议。首先，理解AI产品是不同的，它们是非确定性的，这很重要。你指出了一个我忘记反映的要点，就是在输入和输出方面，用户体验都是非确定性的。人们会看到不同的东西、不同的输出、不同的聊天对话，如果它在为你设计UI，可能还有不同的UI，显然输出也将是非确定性的，所以这是一个问题和挑战。

如果你仔细想想，这也是AI最美妙的部分，我们都更习惯于交谈，而不是遵循一堆按钮之类的东西。所以使用AI产品的门槛要低得多，因为你可以像与人类交流一样自然。但这也是问题所在，我们有无数种交流方式。你要确保意图得到正确传达，并采取正确的行动，因为你的大多数系统都是确定性的，你想要实现确定性的结果，但却要使用非确定性技术，这就是问题变得有点混乱的地方。

**(19:45 - 21:00)**

太棒了。我喜欢这个乐观版本，解释了为什么这是好事。然后另一个要点是在设计产品时自主性与控制之间的权衡理念。我想象你看到的是，人们试图立即跳到理想状态，比如立即到V3版本，这就是他们遇到麻烦的时候。这可能更难构建，而且就是不工作，然后他们就说这是个失败，我们到底在做什么？

确实如此。我觉得在到达V3之前，你实际上必须对一堆事情建立信心，很容易被压倒，觉得我的AI智能体在100种不同的方式上做错了事情，你不会真正列出所有这些问题并修复它们。即使你已经学会了如何处理评估实践等等，如果你从错误的地方开始，你实际上很难从那里纠正事情。

当你从小处开始，当你开始构建一个极简版本，高人类控制和低代理权时，它也迫使你思考我要解决的问题是什么。我们使用这个叫做"问题优先"的术语，对我来说这似乎很明显，是的，我确实需要思考问题，但令人难以置信的是，它与人们产生了很好的共鸣。在我们看到的所有AI进步中，一个容易的滑坡就是不断思考解决方案的复杂性，而忘记你试图解决的问题。所以当你试图从较小的自主性规模开始时，你开始真正思考我试图解决的问题是什么，以及我如何将其分解为我以后可以构建的自主性级别。这在我们与每个人交谈时都非常有用，我们不断重复这种模式。

**(21:00 - 22:15)**

限制自主性还有很多其他好处，因为让系统为你做太多事情也很危险，可能会搞砸你的数据库，发送所有这些你从未预期的邮件。有很多原因说明这是个好主意。

是的。我最近读了一篇来自加州大学伯克利分校一群人的论文，基本上是Zahara Stoker和Databricks的人写的，它说他们接触的企业中大约74%或75%的最大问题是可靠性，这也是为什么他们不愿意将产品部署给最终用户或构建面向客户的产品，因为他们只是不确定，或者他们不愿意这样做并让用户面临这些风险。这也是为什么他们认为今天的很多AI产品必须与生产力有关，因为它的自主性要低得多，相比于那些会取代工作流程的端到端智能体。我也喜欢他们的其他工作，但我认为这与我们在我的初创公司看到的情况非常一致。

**(22:15 - 23:30)**

好的，非常有趣。在这次对话之前会有一集节目，我们深入探讨了这种方法避免的另一个问题，那就是提示注入和越狱攻击，以及这对AI产品来说是多么大的存在风险，这本质上是一个未解决且可能无法解决的问题。我不会深入探讨这个话题，但这是我们进行的一次相当可怕的对话，会在这次对话之前播出。

我认为一旦系统成为主流，这将是一个巨大的问题。我们现在还在忙于构建AI产品，所以我们不担心安全问题，但这将是一个巨大的问题，特别是再次涉及到这个非确定性API。所以，你有点被困住了，因为有无数种指令可以注入到你的提示中，然后是的，这会很糟糕。

让我们实际花一点时间在这里，因为这对我来说真的很有趣，没有人在谈论这些东西，就像我们进行的对话一样，让AI被欺骗去做它不应该做的事情是相当容易的，人们建立了所有这些护栏系统，但结果这些护栏实际上并不是很好，你总是可以绕过它们。正如你所说，随着智能体变得更加自主和机器人化，你可能让AI做它不应该做的事情，这变得相当可怕。

**(23:30 - 24:00)**

我认为这绝对是一个问题。但我觉得在当前客户采用AI的范围内，公司实际上能够从AI中获得优势，或者改进他们的流程，或者简化他们现有流程的程度，我觉得仍然处于非常早期的阶段。2025年对于AI智能体和客户试图采用AI来说是极其繁忙的一年。但我觉得渗透率仍然没有达到你实际能从中获得优势的程度。

### (24:00 - 32:00) Part 4

**(24:00 - 25:15)**

所以通过正确的人工干预点，我认为我们实际上可以避免很多这些问题，更专注于简化流程。我更倾向于乐观的一面，认为你需要先尝试采用这项技术，而不是只关注可能出现的负面问题。我强烈认为公司必须采用这项技术。在 OpenAI，我们接触的公司中，从来没有一家说"AI 在这种情况下帮不了我"。总是会有一些事情可以优化，然后他们会考虑如何采用。

很好，我总是喜欢乐观的视角。我很期待你听听这个，看看你的想法，因为这确实很有趣。正如你所说，有很多事情需要关注，这只是众多需要担心和思考的事情之一。

**(25:15 - 26:30)**

好，让我们回到正轨。我们已经分享了很多专业建议和重要意见。让我问一下，你在成功构建 AI 产品的公司和团队中看到了哪些其他模式和工作方式？最常见的陷阱是什么？我们可以先从公司如何成功构建 AI 产品开始说起。

我几乎将其视为一个成功三角形，有三个维度。这从来不仅仅是技术问题。每个技术问题首先都是人的问题。对于我们合作过的公司来说，有这三个维度：优秀的领导者、良好的文化和技术进步。

**(26:30 - 27:45)**

在领导层面，我们与许多公司合作进行 AI 转型、培训、战略等工作。我觉得很多公司的领导者在过去 10 到 15 年中建立了直觉，他们因这些直觉而备受推崇，但现在有了 AI，这些直觉必须重新学习，领导者必须愿意变得脆弱。

我曾与现在 Rackspace 的 CEO Gajen 合作。他每天早上都有一个时间段，从凌晨 4 点到 6 点叫做"跟上 AI"，他不安排任何会议，这就是他获取最新 AI 播客或信息的时间，他还会有周末编程会议等。

**(27:45 - 29:00)**

我认为领导者必须回到实践中，不是因为他们必须实施这些东西，而是重建他们的直觉，因为你必须接受你的直觉可能是错误的这个事实。你可能是房间里最愚蠢的人，你想向每个人学习。这是我看到的成功构建产品的公司的一个非常显著的区别因素，因为你在带来自上而下的方法。自下而上几乎总是不可能的。如果领导者不信任技术或对技术有错误期望，你不能让一群工程师去获得领导者的支持。

我听到很多正在构建的人说，我们的领导者就是不理解 AI 能解决特定问题的程度，或者他们只是编写一些代码就假设很容易投入生产。你真的需要了解 AI 今天能解决的范围，这样你就能指导公司内的决策。

**(29:00 - 30:15)**

第二个是文化本身。我与一些企业合作，AI 不是他们的主要业务，但他们需要将 AI 引入他们的流程，因为竞争对手在这样做，也因为确实有意义，因为有些用例非常成熟。在这个过程中，我觉得很多公司有一种 FOMO 文化和"你会被取代"的想法，人们变得非常害怕。

领域专家是构建有效 AI 产品的重要组成部分，因为你真的需要咨询他们来了解你的 AI 是如何表现的，或者理想的行为应该是什么。但我与许多公司交谈过，领域专家就是不想和你说话，因为他们认为自己的工作会被取代。这又回到了领导者本身。你想要建立一种赋权文化，将 AI 增强到你自己的工作流程中，这样你就能 10 倍提升你正在做的事情，而不是说如果你不采用 AI 可能会被取代。

**(30:15 - 31:30)**

这种赋权文化总是有帮助的，你希望让整个组织团结在一起，让 AI 为你工作，而不是试图保护自己的工作。对于 AI 来说，它确实比以前开启了更多机会。你可以让员工做比以前更多的事情，10 倍提升他们的生产力。

第三个是我们谈论的技术部分。我认为成功的人对理解他们的工作流程非常痴迷，增强那些可能适合 AI 的部分，而不是那些可能需要人工干预的部分。每当你试图自动化工作流程的某个部分时，从来不是说你可以使用 AI 智能体就能解决你的问题。总是你可能有一个机器学习模型来完成工作的某些部分，你有确定性代码做工作的某些部分。所以你真的需要痴迷于理解那个工作流程，这样你就能为问题选择正确的工具，而不是痴迷于技术本身。

**(31:30 - 32:00)**

我看到的另一个模式是人们真正理解与非确定性 API（也就是你的 LLM）合作的想法。这意味着他们也理解开发生命周期看起来非常不同，他们迭代得很快，就是我能否构建一些东西，以不破坏客户体验的方式快速迭代，同时给我足够的数据，这样我就能评估行为。所以他们很快建立了这个飞轮。到今天为止，这不是关于成为竞争对手中第一个拥有智能体的公司，而是你是否建立了正确的飞轮，这样你就能随着时间的推移而改进。

### (32:00 - 40:00) Part 5

**(32:00 - 33:20)**

拥有今天这种信息获取能力，你需要选择合适的信息渠道，因为每个人都有自己的观点。所以你要相信谁的意见呢？我觉得拥有一个高质量的信息源群体真的很重要。所以他有一个包含两三个固定信息源的清单，总是关注这些渠道，然后带着一堆问题回来，和一群 AI 专家讨论，看看他们怎么想。我是那个群体的一员，所以我知道他提出的问题类型。这很酷。

主持人：确实很酷。我想问为什么你要花这么多时间在这上面？然后他说这会渗透到我们做出的一系列决策中。

**(33:20 - 34:40)**

好，让我谈论另一个话题，这在这个播客中是个热门话题，在 Twitter 上也一度很热。评估（Evals）。

很多人痴迷于评估，认为它们是解决 AI 中很多问题的方案。很多人认为它们被高估了，觉得你不需要评估，只要凭感觉就能做好。你对评估有什么看法？在解决你谈到的那些问题方面，评估能帮助人们走多远？

我觉得存在一个错误的二分法，要么评估能解决一切，要么在线监控或生产监控能解决一切。我没有理由完全信任其中一个极端，将我的应用完全押注在其中任何一个上面。如果你退一步思考，评估基本上是你信任的产品思维，或者说是关于产品的知识，这些知识会融入到你要构建的数据集中。这意味着这对我很重要，这是我的智能体不应该出现的问题类型，让我构建一个数据集清单，确保在这些方面表现良好。

**(34:40 - 36:00)**

而在生产监控方面，你做的是部署应用程序，然后有一些关键指标实际上会向你反馈客户如何使用你的产品。比如你可以部署任何智能体，如果客户对你的交互给出好评，你最好要知道这一点。这就是生产监控的作用。生产监控在产品中已经存在很长时间了，只是现在有了 AI 智能体，你需要监控更多的细节。不只是客户总是给你明确的反馈，还有很多隐含反馈你可以获得。比如在 ChatGPT 中，如果你喜欢答案，你可以点赞，如果你不喜欢答案，有时客户不会点踩，但实际上会重新生成答案。这清楚地表明你最初生成的答案没有满足客户的期望。

**(36:00 - 37:20)**

这些就是你始终需要思考的隐含信号类型，这种范围在生产监控方面一直在扩大。现在让我们回到最初的话题，是评估还是生产监控？这有什么关系？我觉得我们又回到了问题导向的方法，你要构建什么？你试图为客户构建一个可靠的应用程序，它不会做坏事，总是做正确的事情，或者如果它做错了，你会很快收到警报。

我把这分成两部分：首先，没有人会在没有测试的情况下就部署应用程序，这种测试可能是大致的感觉测试，或者是"我有这10个问题，无论我做什么改变，它都不应该出错"的具体测试，让我构建这个，我们称之为评估数据集。

**(37:20 - 38:40)**

假设你构建了这个并部署了，然后你发现，好，现在我需要了解它是否在做正确的事情。如果你是高吞吐量或高交易量的客户，你实际上不能坐下来评估所有的追踪记录，你需要一些指标来了解我应该关注什么，这就是生产监控发挥作用的地方。你无法预测你的智能体可能出错的方式，但所有这些隐含信号和明确信号会向你反馈哪些追踪记录你需要查看，这就是生产监控的帮助。

一旦你获得了这种追踪记录，你需要检查在这些不同类型的交互中看到的失败模式，是否有我真正关心的不应该发生的事情。如果这种失败模式正在发生，那么我需要考虑为此构建一个评估数据集。

**(38:40 - 40:00)**

假设我为尝试提供退款的智能体构建了一个评估数据集，而我明确配置它不应该这样做。我构建了这个评估数据集，然后在工具或提示或其他方面做了改变，然后部署了产品的第二个版本。现在，不能保证这是你将看到的唯一问题，你仍然需要生产监控来实际捕获你可能遇到的不同类型的问题。所以我觉得评估很重要，生产监控也很重要，但这种认为只有其中一个就能为你解决问题的观念，在我看来是完全可以忽略的。

主持人：非常合理的答案，这里的重点不是简单地"两者都做"。更重要的是有不同的东西需要捕获，一种方法不会捕获所有你需要关注的东西。

完全正确。太棒了。

我想退两步，谈谈"评估"这个术语在2025年下半年承载了多少重量，因为你遇到一家数据标注公司，他们告诉你我们的专家在写评估。然后你有所有这些人说产品经理应该写评估，它们是新的产品需求文档。然后你有人说评估几乎就是一切，是你应该构建的反馈循环来改进你的产品。现在退一步，作为初学者思考什么是评估？为什么每个人都在说评估？这些实际上是流程的不同部分，从某种意义上说没有人是错的，是的，这些都是评估，但当数据标注公司告诉你我们的专家在写评估时，他们实际上指的是错误分析，或者专家只是在正确的内容上留下注释。律师和医生写评估并不意味着他们在构建 LLM 评判员或构建整个反馈循环。当你说产品经理应该写评估时，并不意味着他们必须写一个足够好用于生产的 LLM 评判员。我认为也有非常规范的方法来做这件事，而且同意 KD 的观点，你无法预先预测是否需要构建 LLM 评判员，还是需要使用生产监控的隐含信号等。我想马丁·福勒在2000年代某个时候有个术语叫"语义扩散"，意思是有人提出一个术语，每个人都开始用自己的定义来曲解它，然后你就失去了它的实际定义。这就是今天评估正在发生的事情。每个人都看到它的不同方面。但如果你让一群从业者坐在一起，问他们为 AI 产品构建可操作的反馈循环是否重要，我认为所有人都会同意。

### (40:00 - 48:00) Part 6

**(40:00 - 41:15)**

现在如何实现这一点真的取决于你的应用本身。当你处理复杂用例时，构建 LLM 评判员是极其困难的，因为你会看到大量新兴模式。如果你构建了一个评判员来测试冗长性之类的东西，结果发现你看到的新模式是你的 LLM 评判员无法捕捉到的，然后你就会构建太多评估，在那一点上，查看你的用户信号、修复它们、检查是否有回归并继续前进，而不是实际构建这些评判员，就更有意义了。所以这完全取决于情况。我认为每个机器学习从业者都会告诉你的一个声明是，这真的取决于上下文。不要沉迷于处方。它们会改变。

**(41:15 - 42:30)**

主持人：这是一个非常重要的观点。尤其是评估对不同的人来说意味着很多不同的东西。这就像是很多事情的术语。当你把它看作是数据标注公司给你的东西时，谈论评估变得复杂。还有基准测试，人们也称基准测试为评估。

KD：我最近与一个客户交谈，他告诉我我们做评估，我说好的，你能给我展示你的数据集吗？他说："不，我们只是检查了 LM arena 和 artificial analysis。这些是独立的基准测试，我们知道这个模型适合我们的用例。"我说："你们不是在做评估。那不是评估。那些是模型基准测试。"

主持人：这是有道理的。这个词，你知道，可以在那种上下文中使用。我理解为什么人们这样想，但是，现在这只是让它更加混乱。

**(42:30 - 43:45)**

主持人：我想到的另一个问题是，这之所以成为一个大辩论的原因是，Claude Code 的负责人 Boris 说："不，我们在 Claude Code 上不做评估。全都是直觉。" 关于 Codex 和 Codex 团队如何处理评估，Kita，你能分享什么吗？

Kita：所以 Codex 我们采取这种平衡的方法，你需要有评估，你也绝对需要倾听你的客户。我认为 Alex 最近上了你的播客，他一直在谈论我们如何极度专注于构建正确的产品，其中很大一部分基本上是倾听你的客户。编程代理与其他领域的代理相比是极其独特的，因为这些实际上是为可定制性而构建的，这些是为工程师构建的。

**(43:45 - 45:00)**

所以编程代理不是一个要解决前五个工作流程或前六个工作流程之类的产品，它意味着在多种不同方式上是可定制的。这意味着你的产品将在不同的集成、不同类型的工具和不同类型的事物中使用。所以为客户将使用你产品的所有类型交互构建评估数据集变得非常困难。但话虽如此，你也需要理解，如果我要做出改变，它至少不会损害产品真正核心的东西。所以我们有评估来做这件事。

同时，我们极其小心地理解客户如何使用它。例如，我们最近构建了这个代码审查产品，它获得了极大的关注，我感觉 OpenAI 内部的许多错误以及外部客户的错误都被这个产品捕获了。

**(45:00 - 46:15)**

现在假设如果我要对代码审查进行模型更改，或者训练不同类型的强化学习机制，现在如果我要部署它，我绝对想要进行 A/B 测试，识别它是否真的找到了正确的错误，用户如何反应。有时，如果用户被你不正确的代码审查惹恼了，他们会走到关闭产品的程度。所以这些是你想要查看的信号，确保你的新更改在做正确的事情。对我们来说，提前想到这些类型的场景并为此开发评估数据集是极其困难的。

所以我觉得两者都有一点，有很多直觉，有很多客户反馈，我们在社交媒体上非常活跃，以了解是否有人遇到某些类型的问题并快速修复。所以我觉得这是一个你在这里做的事情的领域。

**(46:15 - 47:30)**

主持人：这非常有道理。我听到的是 Codex 支持评估，但这还不够。你需要关注客户行为和反馈，还有一些直觉，比如这感觉好吗？当我使用它时，它是否生成了让我兴奋的、我认为很棒的代码。

Kita：我不认为如果有人来说我有这套具体的评估，我可以把生命赌在上面，然后我不需要考虑任何其他事情，那是行不通的。每个我们要发布的新模型，我们都会作为团队聚在一起，测试不同的东西，每个人都专注于不同的事情，我们有这个困难问题清单，我们把它扔给模型，看看它们的进展如何。所以这就像每个工程师的自定义评估，只是了解产品在这个新模型中在做什么。

**(47:30 - 48:00)**

[广告]如果你是一位创始人，创业最困难的部分不是有想法，而是在不被后勤工作埋没的情况下扩展业务。这就是 Brex 发挥作用的地方。Brex 是为创始人打造的智能金融平台。通过 Brex，你可以获得高限额企业卡、便捷银行服务、高收益资金管理，以及为你处理手动财务任务的 AI 代理团队。他们会做所有你不想做的事情，比如归档费用、搜索交易中的浪费、运行报告，所有这些都按照你的规则。通过 Brex AI 代理，你可以移动得更快，同时保持完全控制。美国三分之一的初创公司已经在使用 Brex。你也可以，访问 brex.com。

### (48:00 - 56:00) Part 7

**(48:00 - 49:15)**

这就是为什么我们提出了持续校准、持续开发的想法。这个概念很简单，我们有循环的右侧部分叫做持续开发，在这里你定义能力范围并策划数据，本质上是获得一个数据集，包含你期望的输入和应该产生的输出。这是在开始构建任何AI产品之前非常好的练习，因为很多时候你会发现团队中的很多人对产品应该如何表现根本没有达成一致，这就是你的产品经理和主题专家能够提供大量信息的地方。所以你有了这个数据集，知道你的AI产品应该在这上面表现得很好。

**(49:15 - 50:30)**

虽然它不是全面的，但它能让你开始，然后你设置应用程序，设计正确类型的评估指标。我故意使用"评估指标"这个术语，虽然我们说eval，但我就是想非常具体地说明它是什么，因为评估是一个过程，评估指标是你想要在过程中关注的维度。然后你进行部署，运行你的评估指标，第二部分是持续校准，这是你理解一开始没有预期到的行为的部分。因为当你开始开发过程时，你有这个数据集在优化，但往往你会意识到这个数据集不够全面，因为用户开始以你没有预测到的方式与你的系统交互。

**(50:30 - 51:45)**

这就是你想要做校准的地方。我已经部署了我的系统，现在我看到有一些我没有真正预期的模式，你的评估指标应该给你一些对这些模式的洞察。但有时你会发现这些指标也不够，你可能有新的错误模式是你没有想到的，这就是你分析行为、发现错误模式的地方。你对看到的问题应用修复，但你也设计新的评估指标来发现新出现的模式。这并不意味着你应该总是设计评估指标，有些错误你可以直接修复而不需要再回来，因为它们是非常孤立的错误。

**(51:45 - 53:00)**

例如，有一个工具调用错误只是因为你的工具定义得不好之类的，你可以直接修复然后继续。这基本上就是AI产品生命周期的样子。但我们特别提到的是，当你进行这些迭代时，尝试在开始时考虑较低代理权的迭代和较高控制的迭代。这意味着限制你的AI系统可以做出的决策数量，确保有人类在循环中，然后随着时间推移增加这种能力，因为你正在构建行为的飞轮，你正在了解什么样的用例正在出现，或者你的用户如何使用系统。

**(53:00 - 54:15)**

我们在newsletter中给出的一个例子是客户支持，这是一个很好的图像，显示了你如何将代理权和控制作为两个维度来思考，你的每个版本都不断增加你的AI系统的代理权或做决策的能力，并随着进展降低控制。我们给出的一个例子是客户支持代理，你可以将其分解为三个版本。第一个版本只是路由，就是你的代理能够分类并将特定票据路由到正确的部门。当你读到这个时，你可能会想，路由真的那么难吗？为什么代理不能轻易做到这一点？

**(54:15 - 55:30)**

当你去企业时，路由本身可能是一个超复杂的问题。任何零售公司，任何你能想到的流行零售公司都有分层分类法。大多数时候分类法都非常混乱。我在一些用例中工作过，你可能有分类法说某种层次结构，然后说鞋子，然后女鞋和男鞋都在同一层，理想情况下你应该有鞋子，然后女鞋和男鞋应该是子类。然后你想好吧，我可以合并那个，你继续往下看，发现鞋子下面还有另一个部分说"女性用"和"男性用"，由于某种原因它没有被聚合，没有被修复。

**(55:30 - 56:00)**

所以如果代理看到这种分类法，它应该做什么？它应该路由到哪里？很多时候我们不知道这些问题，直到你真正开始构建一些东西并理解它。当真正的人类代理看到这些问题时，他们知道接下来要检查什么。也许他们意识到鞋子下面说"女性用"和"男性用"的节点最后更新是在2019年，这意味着它只是一个躺在那里没有被使用的死节点。所以他们知道我们应该查看不同的节点等等。

### (56:00 - 1:04:00) Part 8

**(56:00 - 57:15)**

构建大量评估指标并将其投入生产的问题是，评估指标只能捕捉你已经知道的错误。但是有很多新出现的模式，只有在你将系统投入生产后才能理解。对于这些新出现的模式，你要创建一个低风险的框架，这样你就能理解用户行为，而不会处于有大量错误并试图一次性修复所有错误的位置。这不是唯一的方法，有很多不同的方式。

**(57:15 - 58:30)**

你想决定如何约束你的自主性。它可以基于代理采取的行动数量，这就是我们在这个例子中所做的。它可以基于主题。有些领域对于某些决策来说，让系统完全自主是相当高风险的，但对于其他一些主题，让它们完全自主是没问题的。这取决于问题的复杂性，这就是你真正希望产品经理、工程师和主题专家就如何构建系统并持续改进它达成一致的地方。关键是行为校准，在进行行为校准时不失去用户信任。

**(58:30 - 59:45)**

主持人：我们会为大家提供这篇文章的链接，如果他们想深入了解的话。你基本上逐步讲解了所有这些步骤和大量示例。正如你所说，你描述的一切都是关于让它持续和迭代，沿着更高自主性、更少控制的进展路线前进。甚至将其称为持续校准、持续开发的想法也在传达这是一种迭代过程。明确一点，这个命名有点类似于 CI/CD，持续集成、持续部署套件。这里的想法是，这是 AI 版本的 CI/CD，不是仅仅集成到单元测试并持续部署，而是运行评估、查看结果、迭代你正在观察的指标、找出它在哪里出错，并对此进行迭代。

**(59:45 - 1:00:50)**

很棒。好的，如果人们想更深入了解，我们会再次为他们指向这篇文章。这是一个很好的概述。在我转向不同主题之前，关于这个框架有什么其他重要的事情你认为人们应该知道的吗？

我们得到的最常见问题之一是，我如何知道是否需要进入下一个阶段，或者这是否已经校准得足够好？实际上没有可以遵循的规则手册，但关键在于最小化意外，这意味着假设你每一两天校准一次。如果你发现没有看到新的数据分布模式，你的用户与系统的交互行为相当一致，那么你获得的信息量就非常低，这时你就知道可以实际进入下一阶段了。这完全取决于时机。你知道你准备好了吗？你没有接收到任何新信息。

**(1:00:50 - 1:01:55)**

但理解有时会有事件完全搞乱你系统的校准也很有帮助。一个例子是 GPT-4 不再存在或将在 API 中被弃用。所以大多数使用 GPT-4 的公司应该切换到 GPT-4o，而 GPT-4o 有非常不同的特性。所以这就是你的校准再次偏离的地方。你想回去再次进行这个过程。

有时用户开始以不同的方式与系统交互，或者用户行为随时间演变，即使是消费者产品也是如此。你现在与 ChatGPT 对话的方式与两年前的方式不同，仅仅因为你知道能力已经增强了很多，而且当这些系统能解决一个任务时，人们会变得兴奋，想在其他任务上也试试。

**(1:01:55 - 1:02:50)**

我们在某个时候为承保人构建了这个系统。承保是一项痛苦的任务。有些协议就像贷款申请一样，有 30 或 40 页。这家银行的想法是构建一个系统，可以帮助承保人选择政策和关于银行的信息，这样他们就可以批准贷款。整整三四个月，每个人都对系统印象深刻。我们让承保人实际报告了他们花费时间等方面的收益。

**(1:02:50 - 1:04:00)**

三个月后我们意识到，他们对产品如此兴奋，以至于开始提出我们从未预料到的非常深入的问题。他们会把整个申请文档扔给系统，然后问："对于像这样的案例，之前的承保人是怎么做的？"对用户来说，这似乎是他们正在做的事情的自然延伸，但背后的构建应该显著改变。现在你需要理解"像这样的案例"在贷款本身的背景下意味着什么。它是指特定收入范围的人，还是指特定地理位置的人等等？然后你需要查找历史文档，分析这些文档，然后告诉他们这是什么样子的，而不仅仅是说有政策 X、Y、Z，你想查找那个政策。对最终用户来说可能看起来很自然的事情，对产品构建者来说可能很难构建，你会看到用户行为也会随时间演变，这时你知道要回去重新校准。

### (1:04:00 - 1:12:00) Part 9

**(1:04:00 - 1:05:20)**

这种用例极难控制什么样的Agent在回复你的客户，因为你需要在各个地方调整防护措施等等。

主持人：好的，很棒的选择。Ash，你有什么想法？

Ash：我能说邮件吗？这会被批判吗？

主持人：在哪个类别？它们属于哪个桶？

Ash：被高估了。

主持人：被高估了，好的，继续说。我们不会让你被批判的。

Ash：开玩笑的。我觉得EVA是被误解的，它们是重要的工具，我不是说它们不重要。但我认为这种不断跳转工具、不断学习新工具的做法是被高估的。我仍然很老派，觉得你真的需要对你试图解决的业务问题保持执着。AI只是一个工具，试着这样想。当然，你需要了解最新最好的技术，但不要如此痴迷于快速构建。

**(1:05:20 - 1:06:40)**

今天构建成本真的很便宜，设计成本更昂贵。真正思考你的产品、你要构建什么、它是否真的能解决痛点，这在今天更有价值，在不久的将来这一点只会更加真实。所以真正专注于你的问题和设计是被低估的，而盲目构建是被高估的。

主持人：很棒。从产品角度来看，你认为AI的下一年会是什么样子？给我们一个愿景，你觉得到2026年底事情会发展到哪里？

Ash：我觉得在后台Agent或主动Agent方面有很大前景，它们基本上能更好地理解你的工作流程。如果你想想AI今天在哪里未能创造价值，主要是因为不理解上下文。它不理解上下文的原因是没有插入到实际工作发生的正确位置。

**(1:06:40 - 1:08:00)**

当你做得更多时，你可以给Agent更多上下文，然后它开始看到你周围的世界，理解你正在优化的指标集合是什么，或者你试图做的活动类型是什么。从那里实际获得更多价值是一个很容易的扩展，然后让Agent反向提示你。我们已经在ChatGPT Plus中这样做了，它会给你每日更新你可能关心的事情，实际拥有这种功能来启发你的大脑是非常好的，比如"这是我没有想到的事情，也许这很好"。

当你将这扩展到更复杂的任务时，比如编码Agent说"好的，我已经修复了你的五个Linear工单，这是补丁，请在你一天开始时审查它们"，我觉得这将极其有用，我认为这是2026年产品构建的强劲方向。

**(1:08:00 - 1:09:20)**

主持人：太酷了。所以本质上Agent能够预测你想要做什么，提前行动，"我为你解决了这些问题"或者"我认为这会让你的网站崩溃，也许你应该修复这个"或者"我看到这里有峰值，让我们重构数据库"。太神奇了，多么美妙的世界。

Ash，你有什么想法？

Ash：我对2026年的多模态体验全力支持。我认为我们在2025年已经取得了相当大的进步，不仅在生成方面，还在理解方面。直到现在，LLM是我们最常用的模型，但作为人类，我们是多模态生物。我会说语言可能是我们进化的最后形式之一。当我们三个人在交谈时，我们不断获得许多信号，比如"Lenny在点头，所以我可能会朝这个方向走"或者"Lenny看起来很无聊，让我停止说话"。

**(1:09:20 - 1:10:40)**

你的思维链背后有一个思维链，你不断用语言改变它，但表达的那个维度没有被很好地探索。如果我们能构建更好的多模态体验，这会让我们更接近人类对话的丰富性。而且考虑到现有的模型，还有一堆无聊的任务也适合AI，如果多模态理解变得更好，有很多手写文档和真正混乱的PDF，即使是今天最好的模型也无法解析，如果这成为可能，我们可以利用如此多的数据。

主持人：很棒。我刚看到Google DeepMind的Demis在谈论这个，他认为这将是他们发展方向的重要部分，结合图像模型工作、LLM和他们的世界模型Genie。

**(1:10:40 - 1:12:00)**

这将是一个疯狂的时代。最后一个问题：如果有人想要提高构建AI产品的能力，你认为他们应该专注发展哪一两个技能？

Ash：我认为我们确实涵盖了AI产品的很多最佳实践，比如从小开始、让迭代良好运行、构建飞轮等等。但如果你从一万英尺的高度来看今天的任何构建者，就像我说的，实现在未来几年将变得极其便宜。所以真正掌握你的设计、判断力、品味等等。一般来说，如果你也在建立职业生涯，我觉得在过去几年里，你职业生涯的前几年总是专注于执行机制等等，现在我们有了AI可以帮助你快速提升，之后每个人的工作都变成了关于你的品味、判断力和独特的你。我认为要专注于这部分，试图弄清楚如何带入这种视角。

这不一定意味着你必须明显更年长，拥有多年经验。我们最近雇佣了某个人，我们使用这个非常流行的应用来跟踪任务，我们已经使用了多年并支付高额订阅费。这个人只是带着自己白板编码的应用来开会，他让我们都上手使用，说"让我们开始使用这个"。我认为这种主动性和重新思考体验的主人翁精神是区分人们的关键。我并不盲目地认为白板编码的应用维护成本很高，也许随着公司规模扩大我们必须替换它或考虑更好的方法，但考虑到我们现在是小型公司，我真的很震惊，因为我从未想过这点。

如果你习惯于以某种方式工作，你会将成本与构建联系起来，我觉得在这个时代成长起来的人在心中与构建相关的成本要低得多，他们不介意构建某些东西并继续使用它，他们也非常热衷于尝试新工具。这也可能是为什么AI产品有留存问题，因为每个人都对尝试这些新工具感到兴奋，但本质上拥有主动性和主人翁精神，我认为这也将是繁忙工作时代的终结。你不能坐在角落做一些对公司没有推动作用的事情。你真的需要思考端到端工作流程，如何带来更多影响。我认为所有这些都将非常重要。

主持人：这让我想起，我刚在播客上采访了Jason Lumpkin，他在销售、市场推广方面非常聪明，运营Zaster，他用Agent替换了整个销售团队。他曾有10个销售人员，现在有1.2个人和20个Agent。其中一个Agent只是跟踪每个人对Salesforce的更新，并根据他们的通话自动更新。其中一个销售人员说"好吧，我辞职"，结果发现他实际上什么都没做，只是闲坐着，他想"这会抓住我，我得离开这里"。

### (1:12:00 - 1:20:00) Part 10

**(1:12:00 - 1:13:15)**

是的，关于你说的不能再闲坐着无所事事这一点，我认为确实如此。我想补充的是，坚持不懈也是极其宝贵的品质，特别是考虑到现在任何想要构建产品的人，信息都触手可及，比过去十年更加容易获取。你可以在一夜之间学会任何东西，采用那种钢铁侠式的方法。我觉得拥有这种坚持不懈的精神，经历学习、实施、理解什么有效什么无效的痛苦过程，在开发多种方法并解决问题的痛苦过程中，这种痛苦将成为个人真正的护城河。我喜欢称之为"痛苦是新的护城河"，我觉得这在构建AI产品时特别有用。

**(1:13:15 - 1:14:30)**

主持人：请详细说说这个概念。我很喜欢"痛苦是新的护城河"这个想法。还有更多内容吗？

我觉得作为一家公司，现在在任何新领域取得成功的公司，它们成功不是因为率先进入市场，或者拥有客户更喜欢的炫酷功能。而是它们经历了理解哪些是不可协商要素的痛苦，并准确权衡这些要素与可以用来解决问题的功能或模型能力。这不是一个直接的过程，没有教科书指导，没有直接的方式或已知的路径可循。我所说的痛苦很大程度上就是经历这种迭代过程——好的，我们试试这个，如果不行就试试那个。你在整个组织中或通过自己的亲身经历构建的知识，这种痛苦就是转化为公司护城河的东西，可能是产品评估或你构建的某些东西。我觉得这将是游戏规则的改变者。

**(1:14:30 - 1:15:45)**

这太棒了，就像将煤炭变成钻石一样。我觉得我们在帮助人们避免构建AI产品时经常遇到的最大问题方面做得很好。我们已经涵盖了很多陷阱以及正确做法。

在进入我们非常激动人心的快问快答环节之前，还有什么其他想要分享的吗？还有什么想要留给听众的？

要对你的客户着迷，对问题着迷。AI只是一个工具，要确保你真正理解你的工作流程。80%的所谓AI工程师、AI产品经理花费时间实际上是在深入理解他们的工作流程。他们不是在构建最炫酷、最复杂的模型或围绕它的工作流程。他们实际上在深入一线理解客户行为和数据。每当一个从未做过AI的软件工程师听到"查看你的数据"这个术语时，我认为这对他们来说是一个巨大的启示，但事实一直如此。你需要深入其中，查看你的数据，理解你的用户，这将是一个巨大的差异化因素。

**(1:15:45 - 1:16:30)**

这是一个很好的结尾。AI不是答案，它是解决问题的工具。说到这里，我们进入了非常激动人心的快问快答环节。我为你们两个准备了五个问题。准备好了吗？

好的！是的。

好的，你们都可以回答这些问题。你们可以选择想要回答的问题。随你们。你们最经常向别人推荐的两三本书是什么？

对我来说，有一本书叫《当呼吸化为空气》，Lenny。这是由Paul Kalaniti写的。我想他是一位印度裔神经外科医生，在31或32岁时被诊断出肺癌，整本书是他的回忆录，是在他被诊断后写的，非常美丽，特别是因为我在疫情期间读的这本书，疫情期间我们唯一想做的就是活下去。

**(1:16:30 - 1:18:00)**

书中有很多非常美好的引言，但我记得其中一个，他在反驳苏格拉底的一句著名话："未经审视的生活不值得过"，意思是你真的需要思考自己的选择，理解自己的价值观、使命等等。Paul说："如果未经审视的生活不值得过，那么未曾生活的人生值得审视吗？"意思是你花费太多时间理解自己的使命和目标，是否忘记了生活？我认为每个在AI时代生活、构建产品、不断经历自我重塑阶段的人都需要暂停一下，好好生活一段时间。我想他们需要停止过度评估生活。

主持人：我刚想说这正是我想到的，为你的生活生成一些电子邮件。天哪，我们已经走得太远了。

是的，这是我最喜欢的书。

我更喜欢科幻小说。所以我真的很喜欢《三体》系列，这是一个三本书的系列。它包含了比科幻更宏大的元素，地外生命以及它如何影响人类决策过程，也包含地缘政治元素以及抽象科学对人类进步的重要性，当这种进步被阻止时，在日常生活中可能不明显，但可能造成毁灭性影响。我觉得AI在这些领域的帮助将是极其关键的，那本书很好地展示了否则可能发生的情况。

**(1:18:00 - 1:20:00)**

完全同意，绝对喜欢，可能是我最喜欢的科幻书系列，必须读完全部三本。我发现它在一本半之后才变得真正精彩。如果有人尝试过但觉得"这到底在讲什么"，请继续阅读，读到第二本的中间部分，然后就会让人叹为观止。

如果你喜欢科幻并且从事AI工作，你必须读Vernon Vinge的《深渊上的火》。

我在Noah Smith的通讯中看到他推荐这本书，有整个系列的续集，但这是那一本。它如此不可思议，实际上讲的是AGI和超级智能这些东西，非常史诗级，但没有人听说过它。

谢谢。我也给你推荐一本。好的，下一个问题。你们最喜欢的最近的电影或电视节目是什么？

我开始重新观看《硅谷》，我觉得它太真实了，太永恒了。一切都在重复发生。几年前看过的人应该开始重新观看，你会发现它与AI浪潮中现在发生的一切惊人地相似。

重新观看是个好主意。我喜欢他们整个业务就像一个压缩算法。这在某种程度上可能是LLM的先驱。非常好。GT，你有什么？

我要偏个题，不是电影或电视节目，而是我最近玩的一个游戏叫《远征33》。它与AI无关，但在游戏玩法、电影、故事和音乐方面制作得非常精良，令人惊叹。

我很高兴你有时间玩游戏，这是个好兆头。我只是想象你除了编码什么都不做。

是的，很难找到时间玩游戏。

这很好，我很高兴听到这个。好的，你们最近发现并真正喜爱的产品是什么？

对我来说是Whisper Flow。我一直在大量使用它，我不知道我如此需要它。最好的部分是它是一个概念性转录工具，意思是如果你去代码编辑器开始使用Whisper Flow，它开始识别变量等等，在从转录到指令方面非常无缝。你可以说"我今天很兴奋，加三个感叹号"，它会无缝切换并添加这三个感叹号，而不是写"加三个感叹号"。我觉得这很酷，如果你没在使用就应该试试。我要打个广告，成为我通讯的年度订阅者可以免费获得Whisper Flow一整年。

### (1:20:00 - End) Part 11

**(1:20:00 - 1:21:15)**

一年免费，成为我通讯年度订阅者就可以。这就是我如何获得访问权限的。Lenny，就是这样。我觉得我提出了这个交易。我认为人们并没有真正理解这有多不可思议。他们会说"不可能，这是真的吗？"这是真的。还有18个其他产品。Lenny的产品包lenny's productbass.com。去看看吧。继续。

**(1:21:15 - 1:22:30)**

很棒。我实际上是个生产力狂热者。我一直在尝试新的CLI工具和能让我更快的东西。所以我觉得Raycast非常棒。我发现了所有这些新的快捷键，可以用来打开不同的东西，输入快捷命令等等。Caffeinate是我最近从队友那里发现的另一个东西。它可以防止Mac进入睡眠状态。所以你可以本地运行一个真正长时间的代码任务，比如四五个小时。让它构建，然后你就可以醒来说"好的，这很好。我喜欢这个。"

**(1:22:30 - 1:23:45)**

这太搞笑了。那个组合——代码和咖啡因。你们需要使用它。自己构建一个。一个开放的版本或者代码代理应该直接防止你的Mac睡眠。这太有趣了。顺便说一下，Raycast也是Lenny产品包的一部分。Raycast免费一年。

Lenny没有告诉我们这些朋友们，这些实际上是我们的最爱。这只是19个产品中的两个。不过没有咖啡因。我不知道那个是否需要付费。好的，让我们继续。

**(1:23:45 - 1:25:00)**

你有什么最喜欢的人生格言，在工作或生活中经常回想起来吗？

Ashwaryia：对我来说，这是我小时候我爸爸告诉我的，一直深深印在我心中：他们说这不可能完成，但这个傻瓜不知道，所以他还是去做了。我认为要足够愚蠢地相信，如果你全心投入，你可以做任何事情。特别是现在，因为你手头有太多数据可能指向你可能不会成功的事实。有多少播客达到了超过一千个订阅者，或者有多少公司达到了超过一百万年收入，总是有数据显示你不会成功，但有时候就是要愚蠢地继续下去。

**(1:25:00 - 1:26:15)**

Kir：这很棒。对我来说，我更倾向于过度思考，所以我真的很喜欢Steve Jobs的这句话：你只能回过头来连接这些点。很多时候有无数种选择，你并不真正知道最佳选择是什么，但生活就是这样运作的，你实际上可以回头看并说："哦，从我如何转变的角度来看，这些实际上很美。"所以，我觉得这在继续前进、继续实验方面非常有用。

**(1:26:15 - 1:27:30)**

最后一个问题。每当我同时邀请两位嘉宾上播客时，我喜欢问这个问题。你欣赏对方什么地方？

Ashwaryia：我觉得对于Kir来说，他非常冷静，非常踏实。他一直是我的智囊团。我可以向他抛出大量想法，他总是能预见可能遇到的问题，而且他非常善良，让工作自己说话，而不是做很多言辞表达。但如果我必须选一个的话，我认为他是最不可思议的丈夫。

**(1:27:30 - 1:28:45)**

这是很少有人知道的事。我们已经结婚四年了，这是我生命中最美好的四年。

哇。好的。你怎么接这话？

Kir：是的，这很难接。我要说的是，我在硅谷与真正聪明的人在优秀公司工作方面非常幸运。我觉得Ashwaryia与我合作过的任何其他聪明人相比，独特之处在于她有这种真正令人惊叹的教学能力，能够以非常易懂和易于理解的方式解释事情。这与坚持不懈相结合，在我们所处的快速发展的AI世界中非常有用。有太多新事物出现，感觉很压倒性，但当我听到她谈论如何理解整个事情、它如何插入时，我觉得"哦，这太简单了，我也可以做到。"所以她通过简化事情和以最易懂的方式解释事情来赋能很多人。我觉得这是一个不可思议的品质。

**(1:28:45 - 1:30:00)**

太棒了。多么甜蜜。我得一直这么做。我需要更多。是的，那很棒。好的，最后的问题。人们在哪里可以找到你们正在做的事情？在线找到你们。谈论分享你们的课程链接，然后听众如何能对你们有用？

Ashwaryia：我在LinkedIn上写很多东西。如果你想听取一直在AI产品开发一线工作的实用主义者的意见以及他们所看到的，你可以关注我的工作。我们还有一个拥有约2万颗星的GitHub仓库，那个仓库全部是关于学习AI的好资源。完全免费，如果你喜欢我们今天说的，我们还运营一个超级受欢迎的关于构建企业AI产品的课程。我们会在链接中留下。这个课程很大程度上是关于摒弃思维定式，遵循问题优先的方法，而不是工具优先或炒作优先的方法。你也可以看看。如果你不想上课程，我们写很多东西。我们提供很多免费资源。我们有免费会议。所以一定要关注我们的工作。

**(1:30:00 - 1:31:15)**

Kir：是的，我还要补充的是，你也可以在LinkedIn上找到我。我不太写很多东西，但我对谈论你正在构建的任何复杂产品非常兴奋，如果你对如何使用编码代理让你的生活更好或者你看到什么问题有想法，我的私信总是开放的，我们可以进行很好的讨论。

太棒了。好的，Kir和Ashwaryia，谢谢你们来到这里。

Ashwaryia：非常感谢。

Kir：谢谢Lenny。这太有趣了。

**(1:31:15 - 1:32:00)**

太有趣了。再见大家。

非常感谢收听。如果你觉得这很有价值，你可以在Apple Podcasts、Spotify或你最喜欢的播客应用上订阅这个节目。另外，请考虑给我们评分或留下评论，因为这真的有助于其他听众找到播客。你可以在lennispodcast.com找到所有过往剧集或了解更多关于节目的信息。下一期见。

---

*生成时间: 2026-01-12 23:31:47*
*由 YouTube Monitor & Translator (Claude CLI) 生成*