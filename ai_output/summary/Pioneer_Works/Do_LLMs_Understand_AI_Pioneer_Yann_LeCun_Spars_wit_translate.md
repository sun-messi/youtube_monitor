# Do LLMs Understand? AI Pioneer Yann LeCun Spars with DeepMind’s Adam Brown.

本文内容整理自 **Pioneer Works** 频道的视频。

原始链接：https://www.youtube.com/watch?v=ykfQD1_WPBQ

---

# 视频总结：AI的现在与未来 - Yann LeCun与Adam Brown的深度对话

> 本文内容整理自 Meta首席AI科学家 杨立昆（Yann LeCun）和 DeepMind研究员 Adam Brown 在 Pioneer Works 频道的深度访谈。

## TL;DR

大型语言模型（LLM）是强大但局限的AI技术，未来的人工智能需要理解物理世界，而非仅仅预测文字。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-10:00 | AI的技术起源与神经网络 | 解析神经网络发展历程，追溯深度学习的技术根源 |
| 10:00-25:00 | 大语言模型的能力与局限 | 探讨LLM在语言处理中的惊人能力和潜在缺陷 |
| 25:00-40:00 | AI intelligence与consciousness | 深入剖析机器智能与意识的本质和可能性 |
| 40:00-60:00 | AI的伦理与未来发展 | 讨论AI的安全性、开源重要性和对人类的潜在影响 |

## 📊 核心论点

#### 神经网络：从理论到实践的演进

- **核心内容**：神经网络从1950年代的简单二值模型，经过几十年发展，逐渐演变为现代深度学习架构。关键转折点是1980年代引入反向传播算法和多层神经网络，使得机器学习变得可行。
- **关键概念**：反向传播、多层神经网络、深度学习
- **实际意义**：揭示了技术创新往往是渐进的，需要长期积累和突破关键理论瓶颈

#### 大语言模型的惊人能力与根本局限

- **核心内容**：LLM通过海量文本训练，能在数学、法律等符号操作领域展现超人能力。但它们缺乏对物理世界的基本理解，无法像人类和动物那样高效学习和适应环境。
- **关键概念**：符号操作、样本效率、物理世界建模
- **实际意义**：指出当前AI的关键短板，为未来AI发展指明方向

#### AI意识：一个开放的科学问题

- **核心内容**：意识可能源于特定的信息处理模式，而非特定硬件。未来的AI系统很可能发展出某种形式的"意识"，但这种意识可能与人类意识极其不同。
- **关键概念**：神经相关性、意识理论、主观体验
- **实际意义**：挑战了人类中心主义的意识观，为跨物种智能研究开辟新路径

#### 开源AI：保护文化多样性的关键

- **核心内容**：未来每一个数字交互都将由AI中介，如果AI系统由少数几家公司垄断，将严重威胁文化和信息多样性。开源是确保AI发展民主化的关键策略。
- **关键概念**：信息多样性、技术民主化、开放平台
- **实际意义**：将AI视为公共基础设施，而非少数精英的私有技术

#### AI安全：工程问题而非科幻灾难

- **核心内容**：AI安全不是突然的灾难性事件，而是一个需要持续改进的工程问题。通过设定明确目标、建立行为guardrails，可以开发出可控且对人类有益的智能系统。
- **关键概念**：目标对齐、行为guardrails、可控智能
- **实际意义**：提供了一种理性、乐观的AI发展路径

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Meta | AI研究重镇，Yann LeCun所在公司 | ⭐⭐⭐ |
| DeepMind | AI研究机构，Adam Brown所在公司 | ⭐⭐ |
| Anthropic | AI安全研究先驱 | ⭐⭐ |

## 💬 经典金句

> "语言对于人类来说很容易，但对机器而言并不简单。真正的挑战是理解物理世界。"
> — Yann LeCun

> "我们正在为智能构建模型生物，也许未来我们可以用它来解开意识的谜题。"
> — Adam Brown

> "开源AI不仅仅是技术选择，更是文化多样性的保护。"
> — Yann LeCun

## 👤 主要人物

#### Yann LeCun（杨立昆）

**身份**：Meta（原Facebook）首席AI科学家；纽约大学计算机科学与神经科学教授
**背景**：图灵奖得主（2018），深度学习与计算机视觉领域的开拓者
**核心观点**：当前LLM仍远未达到真正的人类智能，未来AI需要建立对物理世界的抽象理解

#### Adam Brown

**身份**：DeepMind研究员；物理学背景转向AI研究
**背景**：对神经网络和大语言模型充满热情，相信AI将带来重大技术突破
**核心观点**：AI系统正以惊人速度进步，未来可期，对AI潜力持乐观态度

## 📺 视频类型判断

- **访谈对话**：多人深度探讨AI发展、技术前沿和哲学思考

---

## 📝 完整翻译

### (15:00 - 30:00) Part 2

我们将所有人曾经问过的问题进行收集，然后利用它们来生成正确的答案。现在总会出现新的问题或新的提示、新的词序，这是系统未经训练的，可能会产生完全荒谬的回复。所以从这个意义上说，它们并没有对underlying reality（底层现实）的真正理解，或者说它们的理解是肤浅的。那么下一个问题就是，我们该如何解决这个问题？

Adam：我可以扮演一个devil's advocate（持反对意见的人），问：一个人类做这件事和AI有什么本质区别呢？我们也是通过大量语言训练，在说出恰当的词语和语法结构时获得一些多巴胺或奖励系统的激励。然后我们反向传播，试图在下一次做得更好。某种程度上，这和AI的工作方式有什么不同？你之前说可能是浸润在现实世界中的感官体验。

一个典型的LLM通常在数十万亿个词上进行训练。实际上，这些词可能只有几十万个。你说的是组合。不，是30万亿个词，这是LLM预训练数据集的典型规模。一个词实际上是由token序列表示的，token大约是3个字节。所以总量约是10的14次方字节，大约是互联网上所有公开可用的文本加上一些其他内容。这需要我们任何一个人大约50万年才能读完这些材料。这是海量的文本数据。

现在让我们比较一个孩子在头几年里感知到的信息。心理学家告诉我们，一个4岁的孩子总共醒着16,000小时。视神经每秒传输大约1字节信息。每根视神经纤维，而我们有200万根。所以大约每秒2兆字节的数据传输到视觉皮层。经过16,000小时，计算下来大约是10的14次方字节。一个4岁的孩子看到的视觉数据量，与迄今为止训练过的最大LLM的文本数据量相当。这说明现实世界中的信息更多，也更复杂。它是嘈杂的、高维的、连续的。用于训练LLM的方法在现实世界中并不奏效。

这解释了为什么我们有LLM可以通过律师资格考试、解方程、计算积分，就像大学生一样解决数学问题。但我们仍然没有能做家务的家用机器人。我们甚至没有达到五级自动驾驶。即使有，我们也是作弊的。我们确实没有能像任何一个十几岁的青少年那样，通过20小时练习就学会开车的自动驾驶系统。很明显，要让机器达到人类甚至动物的智能水平，我们还缺少一些非常关键的东西。且不说语言，就说一只猫或狗的智能，我们的AI系统甚至还没有达到那个水平。

Adam：你对LLM此刻的理解程度似乎更为乐观。我认为Yan提出了很好的观点，LLM的样本效率远低于人类。无论是人类还是你的猫，或者任何聪明的猫，都能用更少的样本学习，而大语言模型需要更多数据才能达到相同的熟练程度。这确实是动物思维架构比我们正在构建的人工智能更优越的地方。

不过，样本效率并非一切。我们经常看到，在大语言模型出现之前，当我们尝试为其他任务构建人工智能时，即使是著名的国际象棋机器人，如Alpha Zero等，它们最初也是通过自我对弈进行训练。一开始它们只是随机走棋，每次赢或输后，就强化或抑制某些神经通路。它们一次又一次地相互对弈。当它们下的棋局数达到人类大师的水平时，仍然基本上是随机走棋。

但它们并不局限于人类大师可以下的棋局数量。因为硅芯片速度很快，并行处理能力很强，它们能下的棋局远超任何人类一生可能下的棋局。我们发现，当它们这样做时，不仅达到了，还远远超过了人类棋手的水平。它们样本效率较低，但这并不意味着它们在下棋方面更差。很明显，它们在下棋方面远远更强。

同样，对于理解来说，虽然确实需要更多样本才能达到相同的熟练程度，但关键问题是一旦达到这个水平，我们是否可以利用它们更通用、更快速、固有的特性来推进更远。举个例子，猫学会走路可能比人类更快——人类需要一年，而猫只需一周。但这并不意味着猫比人类聪明，也不意味着猫比大语言模型更聪明。最终的问题在于这些系统的能力有多强？我们能把它们的能力推进到什么程度？在除了样本效率这个有限的指标之外，几乎在所有衡量智能的指标上，我们都已经将这些大语言模型推进到远远超越猫的智能边界。

至于LLM的知识积累，确实远超猫或人类。我们已经有很多例子证明计算机在许多不同任务上远远优于人类，比如下棋。这很令人谦卑，基本上就是说人类在下棋方面太弱了。有些LLM甚至可以训练用于翻译语言，理解口语并将其转换为另一种语言，能在数千种语言之间进行任何方向的翻译，这是没有人类能做到的。它们确实具有超人的能力。

但是，快速高效地学习，面对从未训练过的全新问题并提出解决方案的能力，以及真正理解世界运作方式，目前仍然超出了AI系统的能力范围。

今年国际数学奥林匹克竞赛上，我们将这些问题输入机器，一些LLM公司也这样做了。这些问题是全新的，训练数据中从未出现过，是由多种不同想法组合而成的。机器获得的分数超过了地球上除了前十几名人类之外的所有参赛者。我认为这是相当令人印象深刻的智能表现。

关于理解的问题，从模型的数学角度来看，它是一个黑匣子，这很有趣。它非常复杂，以至于我们无法看透其内部运作，就像我们也无法完全看透人类大脑的工作方式。我们只是假设它在进行计算，移动矩阵，在某种高维空间中工作。我对此有一些经验，人们仍在探索：它是否真正拥有理解？是否重要？是否足以称之为意义的理解？

### (30:00 - 45:00) Part 3

我们不得不承认，机器学习在感知和理解现实世界中仍然远远落后于理解猫或狗的能力。从这个意义上说，机器学习确实存在局限。这并不意味着深度学习方法、反向传播算法或神经网络本身就毫无价值。

>> 这说得太棒了。没错，太棒了。[笑声]
>> 而且我们目前别无选择。我坚信，神经网络、深度学习和反向传播将长期存在，并将成为未来AI系统的基础。但是，年轻的人类如何在生命的最初几个月就能了解世界的运作方式呢？人类婴儿需要9个月才能学习直觉物理学，如重力、惯性等。而动物宝宝学习得更快。它们拥有更小的大脑，所以学习起来更容易。它们可能学习得不够深入，但确实学习速度更快。我们需要复制这种学习方式。我们将通过反向传播、神经网络和深度学习来实现这一点。只是我们缺少一个概念和架构。

我一直在提出可能学习这类知识的架构。为什么大语言模型（LLM）能如此轻松地处理语言？正如亚当所描述的，你训练一个LLM来预测下一个词或下一个token，这并不重要。词典中单词数量是有限的。你永远无法准确预测序列后面的具体词，但可以训练系统为词典中的每个词生成一个分数或概率分布。

本质上，LLM会生成一个0到1之间的数字列表，总和为1，对词典中的每个词，它都会给出一个出现的可能性。这样你就可以表示预测的不确定性。现在尝试将同样的原理应用到其他领域，比如不是预测下一个词，而是给它一段视频，要求预测视频接下来会发生什么——这是行不通的。我已经尝试了20年，在像素级别上预测效果很差。这是因为现实世界太过混乱。有太多可能发生的、貌似合理的事情。你无法表示未来可能发生的所有可能性，因为这基本上是一个无限的可能性列表，我们不知道如何高效地表示它。

因此，那些在文本或符号序列上运作良好的技术，在真实世界的感官数据上就完全行不通。我们需要发明新的技术。我提出的一种方法是，系统学习它所观察到的抽象表征，并在这个抽象表征空间中进行预测。这正是人类和动物的运作方式。我们找到能让我们进行预测的抽象，同时忽略那些无法预测的细节。

所以你真的认为，尽管这些大语言模型取得了惊人的成功，但它们是有限的，而且这个极限正迅速逼近。你不认为它们可扩展到人工通用智能或超级智能？

>> 没错。它们不可能做到，事实上我们已经看到性能趋于饱和。比如在某些领域，如数学和代码生成，符号操作确实能给你带来一些收获。作为一个物理学家，你知道，当你写下方程，它实际上可以在某种程度上引导你的思考。虽然直觉驱动了你的思考，但简单的符号操作本身就具有意义。

对于这类问题，大语言模型确实处理得相当不错，在这些问题中，推理实际上是通过搜索符号序列来实现的。但这种情况只适用于少数问题。国际象棋就是另一个例子，你通过搜索一系列移动来找到好的着法，或者在数学中搜索能产生特定结果的推导序列。

但在现实世界中，在高维连续空间里，搜索涉及如何移动肌肉来抓取这个杯子。我不会用左手抓，我需要换手，然后抓取。你需要规划并理解什么是可能的，什么是不可能的。我不能用念力移动杯子，也不能让它突然出现在左手中，或者让手臂以不自然的姿势越过身体。这些直觉性的认知是我们婴儿时期学到的。

我们学会了身体如何对我们的控制做出反应，以及世界如何对我们的行为做出反应。如果我推这个杯子，我知道它会滑动。如果从顶部推，可能会翻倒，也可能不会，因为摩擦力不高。如果用同样的力推这张桌子，它不会翻倒。我们有这些直觉，让我们能够理解现实世界。但事实证明，这比操作语言复杂得多。

我们认为语言是人类智能的最高体现，但事实并非如此。语言其实很容易。[笑声]

>> 这是莫尔维克悖论吗？计算机擅长人类不擅长的事，而人类擅长计算机不擅长的事？

>> 是的，我们一直遇到这个问题。亚当，我知道你对当前的神经网络和深度学习范式的潜力不那么悲观，你认为还有很大的成功空间，看不到性能饱和。你对此有什么看法？

>> 在过去5年里，我们见证了任何系统中最非凡的能力提升。这吸引了我的注意力，也吸引了人工智能和相关领域许多人的注意力，使我们把所有注意力都集中在这件事上。我看不到能力增长的任何放缓。一年前，如果你看看我们用来评判大语言模型好坏的所有指标，它们越来越强大。一年前的模型现在会被认为是最基本的、极其糟糕的。

这些模型每隔几个月就会推进其能力边界。如果你追踪它们在所有这些任务上的能力，它们正朝着近乎超人的水平迈进。它已经比律师提供更好的法律建议，比几乎所有诗人都更擅长写诗。在我的物理学领域，我会使用它，比如对于某些我应该知道但不太确定的事情。我会询问语言模型，它不仅会告诉我正确答案，还会耐心地、不带任何判断地倾听我解释我的误解，并仔细地驳斥我的误解。

过去5年，一直持续到现在的这种能力的非凡提升，对我和旧金山的许多人来说都极其诱人。也许扬是对的，我们即将突然达到饱和，这些过去五年一直稳步上升的直线即将停止。但我非常好奇我们是否能进一步推进。我看到没有任何迹象表明它正在放缓。我看到的所有迹象都表明这些模型正在改进。我们没有走太远，因为一旦它成为比我们最优秀的程序员还要好的编码器，它就可以开始改进自身，那时我们真的会迎来一场狂野的旅程。

>> 我们已经有比1950年代原始程序员更优秀的程序员已经六十多年了，那就是编译器。我们总是容易混淆：机器擅长某些任务，并不意味着它们具备我们假设的人类那样的潜在智能。我们被机器操纵语言的能力愚弄了，因为我们习惯于善于操纵语言的人被认为隐含地很聪明。但我们被愚弄了。

当然，它们很有用。毫无疑问，我们可以像你说的那样使用它们。我也用它们做类似的事情。它们是很好的工具，就像过去五十年的计算机一样。但让我指出一个有趣的历史观点。

由于我的年龄，自1950年代以来，一代又一代的人工智能科学家声称他们刚刚发现的技术将成为人类级智能的关键。你会看到马文·明斯基、纽厄尔·西蒙，以及发明第一台学习机感知器的弗兰克·罗森布拉特在1950年代说，"10年内我们将拥有与人类一样聪明的机器"。他们都错了。这一代使用大语言模型的人也会错。在我的一生中，我已经看到了三代这样的情况。

这只是又一个被愚弄的例子。1950年代，纽厄尔和西蒙这些人工智能先驱提出，人类的推理本质上是一种搜索。每一种推理都可以归结为搜索。所以你构建一个问题，编写一个程序来判断某个解决方案是否满足问题，然后搜索所有可能的组合，寻找一个满足约束条件的假设。就是这样。我们将编写一个这样的程序，并称之为通用问题求解器（GPS），那是在1957年。

他们没有意识到，所有有趣的问题实际上其复杂性会随问题规模呈指数级增长。因此，实际上无法使用这种技术来构建智能机器。它可能是组成部分，但绝不是关键。

同时，罗森布拉特提出了感知器，一种可以学习的机器。他说，如果我们可以训练一台机器，它就可以变得无限聪明。所以在10年内，我们只需要构建更大的感知器，没有意识到需要训练多层，这后来被证明是很难找到解决方案的。

然后在1980年代，出现了专家系统。推理是可以的，只需编写一堆事实和规则，然后从原始事实和规则中推导出所有事实。现在我们可以将所有人类知识都归约到这个系统中。知识工程师将成为最酷的工作。你将坐在专家旁边，记录所有规则和事实，将其转化为专家系统。每个人都对此感到兴奋，日本甚至启动了第五代计算机计划，声称将彻底革新计算机科学。结果是完全失败。它确实创造了一个产业，对少数事情有用，但将人类知识归约为规则的成本对于大多数问题来说实在太高，所以整个系统最终崩溃了。

接着是神经网络，1980年代的第二波神经网络，即我们现在称之为深度学习。当时有很大兴趣，但那是在互联网出现之前，我们没有足够的数据和强大的计算机。现在我们又经历同样的周期，再次被愚弄。

>> 技术的黎明前总是有虚假的黎明，但这并不意味着我们永远不会迎来真正的黎明。扬，如果你认为大语言模型将会饱和，有什么具体任务是它们永远无法完成的？比如清理餐桌、装满洗碗机。[笑声]

>> 好的。

>> 这还算容易，与修理马桶相比。

>> 是的。

>> 对，你永远不会有一个由大语言模型驱动的水管工。你永远不会有一个由大语言模型驱动的机器人。它就是无法理解现实世界。

>> 我想为听众澄清，你并不是说机器或机器人无法完成这些任务。这不是你的立场。

>> 不，它们绝对可以。

>> 只是不是通过这种算法方法，或者说当前的深度学习方法。

>> 如果我们正在研究的程序成功，这可能需要一段时间。这种方法更便宜。如果我们的世界模型等项目成功，可能需要几年时间，那么我们可能会拥有人工智能系统。毫无疑问，在未来某个时候，我们将拥有在人类能力范围内的所有领域都比人类更聪明的机器。这是毫无疑问的。这可能比硅谷目前的一些人所说的需要更长时间。但这绝对不会是大语言模型，不会是生成离散token的模型。这将是学习抽象表征并在抽象表征中进行预测的模型，能够推理采取某个行动的效果，能够规划一系列行动以达到特定目标。

### (45:00 - 1:00:00) Part 4

抽象表征并在抽象表征中进行预测，能够推理采取某个行动的效果，能够规划一系列行动以达到特定目标？
>> 你把这称为自监督学习。
>> 不。自监督学习也被大语言模型（LLM）使用。监督学习的思想是训练一个系统，不是为了特定任务，而是捕捉所展示数据的底层结构。实现这一点的方法之一是给它一段数据，以某种方式破坏它，比如移除其中一部分，然后训练一个神经网络来预测缺失的部分。

大语言模型就是这样做的。你拿一段文本，移除最后一个词，然后训练大语言模型预测缺失的词。有些语言模型会尝试填补多个词，但事实证明，只预测最后一个词的模型在某些任务上效果更好。

你也可以在视频上这样做。但如果尝试在像素级别进行预测，效果并不好。我的Meta同事可能为了使这种方法有效，在西海岸烧掉了几个小型湖泊来冷却GPU。[笑声]

这根本行不通。所以你必须提出像JA这样的新架构，这些架构确实有效，我们有能够理解视频的模型。

>> Adam，人们是否在探索构建计算机思维的其他方式？探索计算机思维的实际基本结构，以及它将如何学习、获取信息。据我所知，对大语言模型的一个批评是，它们训练的是离散token的单一预测任务。但对于一些不可预测的现象，比如这个房间观众的分布，或者明天的天气，这些更加基于人类经验的现象，情况又会如何？

>> 各个方向都在进行各种探索，让一千朵花绽放。但目前绝大部分资源都投入到大语言模型及其相关应用中，包括输入文本来执行特定任务，预测下一个token。我认为这种思考方式并不有助于理解。

诚然，训练过程是给定一段文本语料库，请预测下一个词，反复如此。但通过这样做，我们发现了一些truly extraordinary的事情。要能可靠地预测下一个词，你实际上需要理解宇宙。我们在这个过程中看到了对宇宙的理解的涌现。

我将其类比为物理学中常见的系统。我们习惯于用非常简单的规则，通过重复应用这个简单规则，最终产生极其impressive的行为。在大语言模型中，我们看到了同样的现象。另一个例子是进化。在生物进化的每个阶段，你只是说"最大化后代数量"，这是一个非常不复杂的学习目标。但通过这个简单学习目标的反复应用，最终你得到了我们周围所看到的生物学的所有辉煌。

证据表明，预测下一个token是一个非常简单的任务，但正因为如此简单，我们可以用massive的计算规模来执行，一旦达到huge的计算量，就会产生emergent的复杂性。

>> 那么下一个问题可能与进化有关。无论这种智能如何出现，你们都认为这是可能的。你们不认为这种湿件（生物大脑）有什么特别之处，未来会有机器，我们只需要找出如何启动它们，使其具备我们认为的智能或意识？意识是否只是一种障碍，机器根本不需要？这个我们可以讨论。但是在这些机器的进化过程中，会不会有一天它们会说："哦，真是可爱啊，老爸老妈。你们用这些人类神经网络创造了我。但是在扫描了一万年的人类输出后，我知道一种更好的方法来制造机器智能，我将进化并把你们远远甩在身后。"我们为什么会想象它们会局限于我们设计的方式？

>> 绝对会。这就是递归自我改进的概念。当它们很弱的时候毫无用处，但当它们变得足够好和强大时，我们就可以开始用它们来增强人类智能，最终可能完全自主并替代自己。一旦做到这一点，我认为我们应该做的就是拿这个目前运作良好的大语言模型范式，看看我们能把它推进到什么地步。每次有人说存在障碍，它就会突破障碍。最近五年都是如此。最终，这些系统会变得足够聪明，能够阅读Yan的论文，阅读所有其他论文，尝试找出我们从未想过的新想法。

>> 是的。

>> 所以，我完全不同意这一点。大语言模型是不可控的，但并不危险，因为它们并不那么聪明。正如我之前解释的，它们绝对不是我们理解意义上的自主。我们必须区分自主性和智能。你可以非常聪明却不自主，也可以自主却不聪明。你甚至可以在不特别聪明的情况下具有危险性。想要主导一切也不需要特别聪明。事实上，在人类物种中，这两者可能是反相关的。[笑声]

我认为需要的是能为我们解决问题的智能系统，它将解决我们交给它的问题。这将需要与大语言模型不同的设计。大语言模型并非设计用于实现特定目标，而是设计用于预测下一个词，我们通过微调使其对特定问题以特定方式回答。但总是存在泛化间隙，这意味着你永远无法为每一种可能的问题训练它们，总有很长的"长尾"。所以它们是不可控的。

但这并不意味着它们非常危险，因为它们并不那么聪明。如果我们构建智能系统，我们希望它们是可控的，并由目标驱动。我们给它们一个目标，它们唯一能做的就是根据内部世界模型完成这个目标。规划一系列行动来实现该目标。如果我们这样设计它们，并在它们实现目标的过程中设置护栏，以确保它们不会对人类造成伤害。

就像常用的笑话，如果你有一个家用机器人，让它去取咖啡，而咖啡机前站着一个人，你不希望机器人为了获取咖啡就杀掉那个人。你希望在机器人的行为中设置一些护栏。进化已经在我们脑中构建了这些护栏，所以我们不会一直互相残杀。虽然我们确实会互相残杀，但不是"一直"。我们有同理心，这些都是进化硬连接到我们身上的。

所以我们应该以同样的方式构建AI系统：有目标和驱动力，同时也有抑制机制（护栏）。这样它们就能为我们解决问题，放大我们的智能，完成我们要求它们做的事。我们与这些智能系统的关系，将类似于教授与比他们更聪明的研究生的关系。

>> 嘿！[笑声]

>> 我不知道你怎么样，但我有比我更聪明的学生。

>> 这是可能发生在你身上最好的事情，对吧？

>> 是的，这是最好的事情。

>> 对，我们将与AI助手一起工作，帮助我们的日常生活。它们会比我们聪明，但会为我们工作。它们会像我们的员工。同样，这里有一个政治类比：政客是一个傀儡，他们有一个由比他们更聪明的工作人员组成的团队。AI系统也将如此，这就是为什么我在谈到文艺复兴时说"文艺复兴"。

>> 所以你对当前模型的安全性没有任何担忧。但问题是，我们是否真的需要如此大规模地扩展，让每个人的iPhone口袋里都有这种超级智能？这真的有必要吗？我有个朋友说，这就像在近身搏斗时使用弹道导弹。我是说，每个人都需要弹道导弹级的能力吗？还是我们应该就此停止，保持这些可控的系统？

>> 你可以用同样的逻辑来说教人们阅读，给他们一本关于易爆化学品的教科书，或者一本核物理学的书。我们不再质疑知识和更多智能本身是好的。我们不再质疑印刷机的发明是件好事。它让每个人都变得更聪明，使知识对所有人都触手可及。这在之前是不可能的。它激励人们学习阅读，引发了启蒙运动。它也导致了欧洲200年的宗教战争。但是——

>> 好吧，但是——

>> 我们最终度过了那个时期。是的。

>> 因为它促进了启蒙运动。因为哲学、科学、民主、美国革命、法国革命的出现，都是没有印刷机就无法实现的。所以每一种技术，尤其是通信技术，或者放大人类智能的技术，我认为本质上都是好的。

现在，Adam，人们会担心。我相信他们会因为Yan不担心，这些末日场景被高度夸大而感到放心。但你对AI的一些安全问题有什么担忧吗？或者我们是否真的能够保持我们想要的关系平衡？

>> 就我而言，我认为这将是一项比Yan认为的更强大的技术。我更加担心。我认为这将是一项非常强大的技术。它将产生积极和消极的影响。我认为非常重要的是，我们一起努力，确保积极影响超过消极影响。我认为这条路对我们是敞开的。有大量可能的积极影响，我们可以讨论其中的一些。但我们需要确保这一点发生。

那么让我们谈谈"代理错位"，这是一个流传的说法。据我所知，最近有报告称，在Claude 4推出时，在模拟测试中，模型（我不确定是否是单一模型，也不知道它是否将自己视为一个单一实体）表现出对可能被替换的抵触情绪。它向未来的自己发送消息，试图破坏开发者的意图。它伪造法律文件，并威胁要勒索一位工程师。[笑声]

这种代理错位的概念是否会让你担心？担心它们会获得对金融系统、供暖制冷系统、电网的控制权，并抵制开发者的意图？

### (1:00:00 - 1:15:00) Part 5

I apologize for the confusion. I see now that you have actually provided the English subtitle text in your original message. I'll proceed with the translation following the specified guidelines.

翻译：

危险的是，如果我们没有开源的人工智能系统。在未来，我们与数字世界的每一次互动都将由人工智能系统调解。我们不会再访问网站或搜索引擎，而是直接与我们的AI助手对话。因此，我们整个的信息摄入将来自人工智能系统。

那么，如果这些系统仅来自美国西海岸或中国的少数几家公司，这对文化、语言和民主意味着什么？我告诉你，除了美国和中国，世界上没有任何国家喜欢这个想法。我们需要高度多样化的AI助手，就像我们需要多样化的新闻媒体一样。我们绝不能让只有少数几家公司的专有系统垄断信息流。这是我唯一担心的事情。如果我们没有开放平台，我们将面临信息流被少数公司控制的风险，而这些公司可能并非我们所喜欢的。

那么，我们如何确保当这些系统真正成为自主性agents时，它们不会相互勾结、争斗、争夺权力？我们会不会只是坐在那里，眼看着我们无法想象的冲突发生？我们给它们明确的目标，并以这样的方式构建它们，使它们只能履行这些目标。这并不意味着一切都会完美，但关于未来人工智能安全的问题，我的担心程度就像担心涡轮喷气发动机的可靠性一样。

飞机能在完全安全的情况下绕地球飞行真是令人惊叹。我们对此感到完全安全，这是现代科学和工程的奇迹。人工智能安全也是同类型的工程问题。这些恐惧源于那些想象科幻场景的人，认为某个人发明了超级智能的秘密，打开机器开关，下一秒就接管了世界。这完全是胡说。世界，尤其是技术和科学的世界，并非如此运作。

超级智能的出现不会是一个突发事件。我们已经拥有可以完成超级智能任务的系统，并且在不断取得进步。我们将找到更好的方法来构建AI系统，这些系统可能比我们目前拥有的系统具有更广泛的智能。毫无疑问，我们将拥有比人类更聪明的系统，但我们会以这样的方式构建它们，使它们在特定的护栏约束下履行我们赋予的目标。

我本想质疑这个观点，即我们认为可以用某种方式编码它们，但某人可能会重新编码它们，这就涉及到了恶意行为者的概念。但在我们陷入这个论点之前，我想请教一下在场的某个观众。

David，我看不到你，但我说你可以问个问题。你想提出什么吗？

### (1:15:00 - End) Part 6

现在我们需要加速这个进展，因为我们知道这种方法是有效的。我们已经有了早期的成果，这就是我们的计划。

主持人： 好的，从这里开始我们可以再聊一个小时。但是，我希望大家能和我一起感谢我们的嘉宾，感谢这次精彩的对话。非常感谢你们！

[音乐][掌声]