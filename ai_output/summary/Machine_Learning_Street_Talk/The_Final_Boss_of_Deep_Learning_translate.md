# The "Final Boss" of Deep Learning

## 📹 视频信息

- **频道**: Machine Learning Street Talk
- **发布日期**: 2025-12-22
- **时长**: 43:56
- **原始链接**: [https://www.youtube.com/watch?v=AWqvBdqCAAE](https://www.youtube.com/watch?v=AWqvBdqCAAE)

---

> 本文内容整理自 DeepMind 研究科学家佩塔尔·韦利奇科维奇（Petar Veličković）和加拿大高等研究院研究员安德鲁·达菲（Andrew Duffy）等在 Machine Learning Street Talk 频道的技术访谈。

---

## TL;DR

大型语言模型连基础算术都做不好，暴露了深度学习的根本局限性：缺乏统一的数学框架。范畴论有望成为深度学习的"元素周期表"，为神经网络提供系统化的理论基础，超越当前的试错式架构设计。

---

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-02:05 | LLM 算术能力的根本缺陷 | 大模型无法真正掌握算术，只是模式匹配，暴露了架构局限性 |
| 02:05-04:09 | 工具使用 vs 内在能力 | 外挂计算器不如内化算法，避免频繁调用和效率损失 |
| 04:09-07:15 | 几何深度学习的核心思想 | 通过对称性约束减少数据需求，Transformer 本质上是置换等变模型 |
| 07:15-09:18 | 从群论到范畴论的跃迁 | 群论无法描述非可逆计算，需要范畴论处理信息丢失的算法 |
| 09:18-13:26 | 范畴论：有色彩的代数学 | 用"有颜色的磁铁"比喻解释范畴论的组合约束和结构保持 |
| 13:26-15:31 | 深度学习缺乏统一框架 | 当前神经网络设计充满 ad hoc 选择，范畴论将成为统一理论 |
| 15:31-19:37 | 算法推理的信息丢失问题 | 最短路径算法等会压缩图信息，群论无法处理这种非对称性 |
| 19:37-23:41 | 神经符号结合的未来愿景 | 神经网络负责理解世界，算法组件负责可靠计算 |
| 23:41-28:18 | 结构主义数学的抽象框架 | 综合数学关注推理原则而非底层实现，为深度学习提供指导 |
| 28:18-33:28 | 二范畴与权重共享理论 | 二态射建模权重关系，超越传统权重拷贝的局限性 |
| 33:28-39:39 | 递归计算的范畴描述 | 列表折叠操作对应单子代数，连接函数式编程和神经网络 |
| 39:39-43:45 | 进位机制与神经网络 | 数字轮的进位行为启发 GNN 设计，Hopf 纤维化提供几何洞察 |

---

## 📊 核心论点

#### LLM 的算术能力是伪装的模式匹配

- **核心内容**：大型语言模型无法真正执行算术运算，它们只是学习了大量数据中的计算模式。当遇到训练分布外的问题时（如将"一串8加一串1末尾加2"中的一个8改为7），模型就会失败。这表明 LLM 缺乏对基础算法的真正理解，只是在进行统计模式匹配。即使是最先进的模型也需要数千亿次乘法运算才能产生一个token，却无法可靠地乘两个相对较小的数字。
- **关键概念**：模式匹配、分布外泛化、算法理解、统计学习局限性、token 生成成本
- **实际意义**：揭示了当前 AI 系统的根本性限制，表明仅靠扩大模型规模无法解决推理问题；对于需要精确计算的科学和工程应用，现有 LLM 存在可靠性风险；推动研究界思考神经符号结合的必要性。

#### 工具使用无法替代内在算法能力

- **核心内容**：虽然可以通过 MCP 服务器等工具为 LLM 外挂计算器功能，但这种方法存在效率问题。在复杂推理任务中，模型可能需要频繁调用外部工具，涉及多轮交互、结果验证和思路调整。相比之下，如果模型内在具备基础计算能力，就能在内部完成整个推理链，避免反复的外部调用开销。
- **关键概念**：工具使用、内在能力、计算效率、推理链连续性、外部调用开销
- **实际意义**：指导 AI 系统架构设计，平衡内在能力培养与外部工具集成；促进研究更高效的神经符号混合系统；为 Agent 框架的设计提供理论指导。

#### 几何深度学习通过对称性实现数据效率

- **核心内容**：几何深度学习的核心思想是将对称性变换的等变性直接构建到神经网络架构中。例如，卷积神经网络对图像平移具有等变性，图神经网络对节点排列具有置换等变性。这种设计能够指数级地减少训练所需的数据量，因为网络无需从数据中学习这些对称性。Transformer 在本质上就是一种置换等变模型，这解释了其在序列建模上的成功。
- **关键概念**：等变性、对称性变换、数据效率、置换等变性、CNN 平移不变性
- **实际意义**：为神经网络设计提供了系统性的指导原则，减少了对大规模数据的依赖；启发了图神经网络、3D 深度学习等领域的架构创新；证明了数学原理在深度学习中的重要价值。

#### 群论局限性催生范畴论需求

- **核心内容**：几何深度学习基于群论，但群论要求所有变换都是可逆的，这限制了其描述算法计算的能力。许多重要算法（如 Dijkstra 最短路径算法）会丢失信息—多个不同的图可能有相同的最短路径。这种信息压缩是不可逆的，无法用群论的对称性概念来描述。因此需要范畴论这样更一般的数学框架。
- **关键概念**：可逆性限制、信息丢失、算法计算、最短路径算法、从单子到范畴的泛化
- **实际意义**：拓展了深度学习的理论边界，使其能够处理更广泛的计算模式；为设计能够执行算法推理的神经网络提供数学基础；推动神经符号结合研究的发展。

#### 范畴论：深度学习的统一数学语言

- **核心内容**：范畴论可以比作"有颜色的代数学"—就像矩阵乘法需要维度匹配，范畴论描述了具有组合约束的结构。它提供了一个统一框架来描述各种数学结构：群作用、列表操作、树结构等。范畴论的系统性意味着它能自动产生有意义的定义，如等变映射和自然变换，而无需领域专家的创造性洞察。
- **关键概念**：组合约束、颜色匹配规则、统一框架、自然变换、系统性定义生成
- **实际意义**：为深度学习提供了类似"元素周期表"的理论基础，将零散的技巧统一为系统的科学；指导新架构的系统性发现而非偶然发明；建立概率、神经科学和梯度下降视角的统一框架。

#### 深度学习的"炼金术时代"问题

- **核心内容**：当前深度学习领域充满了无法形式化证明的 ad hoc 设计选择。神经网络架构有大量的"旋钮和调整"，缺乏统一框架来解释概率视角、神经科学视角和梯度下降视角。这类似于化学发展史上的炼金术时代—有实用的经验结果，但缺乏基础理论指导。范畴深度学习旨在成为神经网络的"元素周期表"。
- **关键概念**：Ad hoc 设计、理论缺失、多视角统一、架构发现 vs 发明、系统性 vs 偶然性
- **实际意义**：推动深度学习从经验科学向理论科学转变；减少架构设计中的试错成本；为自动化神经架构搜索提供理论指导；促进跨领域知识的系统整合。

#### 算法推理的信息丢失挑战

- **核心内容**：许多重要的计算算法会破坏或压缩输入信息，这与几何深度学习假设的可逆对称性根本冲突。例如，不同的加权图经过最短路径算法处理后可能产生相同的输出，原始图的细节信息被不可逆地丢失。这种计算模式在计算机科学中极其普遍，但传统的对称性理论无法处理。
- **关键概念**：信息压缩、不可逆变换、最短路径算法、计算等价类、对称性突破
- **实际意义**：揭示了几何深度学习的理论边界；为设计能够执行真实算法的神经网络开辟道路；促进神经符号系统的理论发展；指导图神经网络处理动态信息的架构设计。

#### 神经符号混合系统的分工愿景

- **核心内容**：未来的 AI 系统应该采用分工合作模式：神经网络负责理解复杂的现实世界场景并将其转换为抽象表示空间（可能是高维嵌入），而专门的算法组件负责在该空间中执行可靠的计算。这种架构能够结合神经网络的世界理解能力和算法的正确性保证，同时提供错误估计和计算复杂度感知。
- **关键概念**：神经符号分工、世界理解、算法正确性、错误感知、计算复杂度估计
- **实际意义**：为下一代 AI 系统架构提供设计蓝图；结合深度学习的泛化能力和符号推理的可靠性；推动 Agent 系统向更加模块化和可解释的方向发展。

#### 二范畴理论建模权重共享机制

- **核心内容**：二范畴通过引入"二态射"概念来建模神经网络中的权重关系，超越了传统的简单权重拷贝。二态射描述参数化映射之间的关系，能够表达任意的权重约束关系，不仅仅是复制。这为理解和设计权重共享机制提供了严格的数学框架，适用于流形、博弈论等多个领域。
- **关键概念**：二态射、参数化映射、权重重参数化、任意权重关系、跨领域权重共享
- **实际意义**：为权重共享提供理论基础，指导更复杂的参数约束设计；扩展到博弈论中的策略共享、流形学习中的参数约束；为神经架构搜索提供数学约束框架。

#### 递归计算的范畴化描述

- **核心内容**：函数式编程中的递归数据类型（如列表）可以用范畴论中的"内函子的代数"来精确描述。列表的折叠操作对应数学中的单子（monoid），这建立了编程语言理论与神经网络设计的桥梁。这种对应关系表明，递归计算的结构可以系统地嵌入到神经网络架构中，而不是依赖试错式的设计。
- **关键概念**：内函子代数、列表折叠、单子结构、递归数据类型、语法与语义对应
- **实际意义**：为递归神经网络提供严格的理论基础；指导处理变长序列和树状数据的架构设计；建立编程语言理论与深度学习的系统联系；促进可解释 AI 的发展。

#### 进位机制启发的神经计算

- **核心内容**：传统的图神经网络忽略了数学中最基本的"进位"概念。在构建能够执行多位运算的系统时，需要当低位从9跳到0时触发高位加1的机制。但这种信息不在状态本身，而在状态变化中，且需要区分不同的变化原因。在连续数学框架中实现这种离散机制极其困难，可能需要利用高维流形的几何微妙性，如Hopf纤维化。
- **关键概念**：进位机制、状态变化信息、离散到连续的挑战、Hopf纤维化、高维流形几何
- **实际意义**：为构建真正的"神经CPU"提供几何洞察；推动连续神经网络模拟离散计算的理论发展；启发新的递归神经网络架构设计；为算法推理能力的神经实现开辟新途径。

---

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| DeepMind | 核心研究机构、论文作者所在单位、前沿理论研发 | ⭐⭐⭐ |
| OpenAI / ChatGPT | 算术能力测试案例、LLM 局限性典型代表 | ⭐⭐⭐ |
| Meta | Petar 的工作背景、大规模深度学习实践 | ⭐⭐ |
| 几何深度学习 | 理论框架基础、现有方法的局限性分析 | ⭐⭐ |

---

## 💬 经典金句（3-5 句）

> "语言模型无法做加法。每次我看到这种说法，我都会再次去ChatGPT测试，它们确实不能。"
> — Petar Veličković

> "深度学习，尽管取得了显著成功，但是一个充满了临时设计选择的领域。神经网络架构有所有这些旋钮和调整，我们还无法正式证明。"
> — Andrew Duffy

> "范畴论为懒人而设，或者说为不够创造性的人。你不需要聪明，你只需要遵循规则并保持系统性，就能产生非常自然的定义。"
> — Andrew Duffy

> "我们需要一个统一框架来解释概率视角、神经科学视角和基于梯度的迭代更新视角。我们的主张是范畴论将成为统一的深度学习框架。"
> — Petar Veličković

---

## 👤 主要人物

#### 佩塔尔·韦利奇科维奇（Petar Veličković）

**身份**：DeepMind 研究科学家、剑桥大学计算机科学系副研究员
**背景**：图神经网络和几何深度学习领域的先驱之一，Graph Attention Networks（GAT）的主要发明者。在算法推理、图表示学习和神经符号AI方面有深入研究。是几何深度学习框架的核心贡献者之一。
**核心观点**：当前的深度学习模型缺乏真正的算法推理能力，需要通过范畴论等数学框架来构建更加原理化的神经网络架构。强调神经网络与经典算法相结合的重要性，认为单纯扩大模型规模无法解决根本的推理问题。

#### 安德鲁·达菲（Andrew Duffy）

**身份**：加拿大高等研究院研究员、DeepMind 合作研究者
**背景**：纯数学出身，专精代数学和范畴论。致力于将抽象数学理论应用到深度学习中，是范畴深度学习理论框架的共同创立者。在将高等数学概念转化为实际神经网络设计原则方面有独特贡献。
**核心观点**：范畴论提供了深度学习所需的统一数学语言，能够系统地指导神经网络架构设计，将深度学习从"炼金术时代"推进到具有坚实理论基础的科学。强调结构主义数学在理解和设计AI系统中的重要价值。

---

## 📺 视频类型判断

**访谈对话**：多位专家深度讨论深度学习理论基础和数学框架

---

## 📝 完整翻译

### (0:00 - 15:00) Part 1

语言模型无法进行加法运算。确实如此。我一直看到有人声称它们可以，每当我看到这种说法时，我就会再次去 ChatGPT 等平台测试，它们确实不行。它们能做的是学习在大部分时候有效的模式，但你总能通过一些方法让它们出错。

比如，如果你问 ChatGPT，一串 8 加上一串 1 末尾带个 2 等于多少？它会给出正确答案，因为它能识破这个技巧。它会说："啊，那就是 1 后面跟一串 0。"它知道你在试图欺骗它。

但如果现在你把其中一个 8 改成 7，它就必须真正知道自己在做什么。它必须实际地向上计算，碰到 7 时停下来，停止传播零。它就是做不到。它要么卡住编造一些胡言乱语，要么还是说结果是 1 后面跟一串 0。它绝对无法以我们所知的算法方式、以人类学会的基本方式进行加法运算。

所以在非常基础的层面上，比如牛顿的三大运动定律，它是否真正掌握了？无论是 VO 还是 Genie，这些模型是否 100% 准确地掌握了物理学？现在还没有，它们只是近似值。当你随便看看时，它们看起来很逼真，但还不够准确，无法在机器人技术等领域可靠应用。

仅仅因为我们可以通过将一个非常强大的工具连接到语言模型上来取得一定程度的进展，并不意味着我们不应该思考下一代模型会是什么样子，以及我们如何让它们本质上变得更好。因为即使你拥有世界上最好的工具，如果你无法预测该工具的正确输入，那也救不了你。

即使是目前一些最前沿的模型，正如你可能知道的，它们会执行数千亿次乘法运算，仅仅是为了产生一个输出 token。然而，它们甚至无法可靠地将相对较小的数字相乘而不出错。这在我看来暗示了我们训练这些系统的方式、构建它们的方式与我们可能想要在下游使用它们的目的之间存在巨大的不匹配，特别是如果我们要进行推理或科学研究。

但似乎如果你正确地教授一个 LLM，你就可以教它进行加法运算，直到它记忆能力的某个限制点——就像人类一样，我们可能会忘记一个数字或忘记进位，或者有时以某种概率错误地执行算法。到那个程度为止，它可以学习这种添加长数字的程序。

这是神经网络还是符号化的？它在做某种算法，某种符号化的东西，但它是用神经机制来做的。神经机制还允许它首先吸收大量的世界知识，处理概念的模糊性——事实上，事物大多数时候并不完全符合你传统的人工智能符号理论。

我们都使用过 MCP 服务器，我们知道可以将工具连接到这些模型上，为什么不直接调用计算器呢？

Andrew 认为工具使用还不够，我们仍然需要思考底层的实际架构，架构仍然很重要。

将功能内化我认为有机会变得更加稳定。神经网络与工具之间的关系有些复杂。我认为你可以用它做很多事情，可以进行很多有趣的搜索，但有一些缺点。一个缺点是你可能需要多次调用模型，因为它可能得到一个答案，但可能不是它期望的答案。所以它可能需要重新思考并返回。

想象一种情况，你有一些复杂的推理问题，在此过程中你必须进行一系列小的加法运算。你只需要"好吧，那么这个，然后这个，然后哦，有多少个这样的？好的。然后有多少个那样的"等等。不断地调用工具然后返回，再调用工具然后再返回，可能会相当复杂。如果你能够让模型本身就能在内部进行某些种类的基本计算或推理，似乎会有很大的效率提升。

所以，几何深度学习是一个相当有趣的章节，当然也是我自己研究和与团队合作的一个非常令人兴奋的时期。在我们与几何深度学习一起做的最初几集中，就已经暗示了几何深度学习可能不一定足够的讨论。它将要求我们可能需要扩大我们对几何深度学习含义的视角。这是我们已经非常积极思考的事情。

我认为我们的合作者之一 Taco Cohen 实际上对此思考得更深入。正如我暗示的，群论——几何深度学习的基础——可能不足以适应计算对齐的概念，这是我个人的动机。

对于那些不熟悉的人来说，几何深度学习从根本上建立在以我们称之为对称变换等变的方式构造神经网络的概念上。简单来说，这意味着如果我以某种方式变换我的输入，如果该变换是我认为不相关的东西，我应该从神经网络中得到可预测的输出。

一个标准的例子是图像的平移。如果我有一张猫的图片，我决定将它移动一定数量的像素，它仍然是一张猫的图片。这没有改变。我只是改变了看那只猫的方式。所以我想构建我的模型为我们所说的平移等变，这样当我确实应用这样的移动时，我仍然会得到相同的输出，即这确实是一只猫。

以类似的方式，图机器学习——这是我个人真正热衷的领域——处理提取图结构数据（如分子）的有用表示。但图具有这种固有属性，即有很多不同的方式可以向模型展示它们，通常你必须使用某种邻接矩阵来呈现它们。

但如果我决定置换你展示那些节点的顺序，所以我相应地置换矩阵的行和列，这仍然是同一个图，我仍然希望在该图上得到完全相同的输出。我可以在我的模型中构建这种置换等变的概念，它保证即使我置换我的节点，我仍然会得到可预测的输出，对于那些置换的图，基本上是相同的输出（直到置换）。

这是一个非常重要的属性，因为它几乎指数级地减少了你需要将这样的系统拟合到令人满意的行为量所需的数据量。毫不奇怪，Transformer 的核心是置换等变模型。一旦你放入了 token 嵌入、位置嵌入到 token 中，你可以随意置换它们。你会得到完全相同的响应。

如果你想用简单的 token MLP 来学习那种对称性，那将需要你比我们目前用于训练这些模型的数万亿数据指数级地更多的数据。所以很可能是你找不到的数据。

我们从群对称的角度研究几何深度学习，这是描述空间规律和空间对称的一种非常好的方式，但它不一定是谈论通用计算不变性的最佳方式，你会在算法中找到这种不变性。

就像我有满足某些前置条件的输入，我想说一些关于一旦我通过这个函数推送它就应该满足某些后置条件的事情——这不是我们可以很容易地用群论语言表达的事情。然而，这也许是我们可以用范畴论语言更好地表达的东西。

我确实认为一些非常高级的先验知识可能是个好主意，甚至可能是必要的。在我的博士期间，我在将关于对称性的知识构建到神经网络中做了很多工作。我认为对于许多问题，关于对称性的知识首先会给你很大的回报。

我们从物理学中已经知道，现在从机器学习的经验结果中也知道，将这些东西构建到神经网络中或基于对称性对你的物理理论施加约束，会给你很多信息，或者它真的限制了假设空间，同时如果你的问题确实具有这种对称性，它不会偏置你的模型。

所以我认为那是我们应该寻找的那种事情——这种非常高级的抽象先验，而不是试图编码——回到我刚才给出的例子——电灯开关让灯亮起的事实，我们可以从数据中找出这一点，从大规模阅读互联网上的文本中，从交互环境中的试错学习中。

也许 3D 空间的事实，以及你的 2D 图像是其投影这一点，也许那是一个有用的先验。

范畴论很大程度上是见仁见智的。我认为首先对我来说，范畴论——范畴是我所来自的纯数学中一个非常平凡的东西。范畴论意味着你为了范畴本身而研究范畴，但每个人都使用范畴。问题是它们究竟是什么？

我真正来自代数，我的很多动机来自于研究代数。你可以把范畴想象成有颜色的代数。所以你知道我们可以想象典型的代数。比如，假设我们在乘方阵。我可以把每个方阵想象成一个小磁铁，我只是把它们连接在一起，它们就粘在一起，你知道你得到一个越来越大的磁铁，一切都说得通。

### (15:00 - 30:00) Part 2

你说的很对。我们在这个高维空间中工作，这个空间不一定容易解释或组合，因为你没有简单的方法来说——例如在理论计算机科学中，如果你想组合两个算法，你是在一个非常抽象的空间中处理它们，这意味着你可以轻易地推理将一个算法的输出连接到另一个算法的输入，而你无法对两个神经网络的潜在空间做出如此简单的声明。

几何深度学习很强大，但它假设所有变换都是可逆的。当计算破坏信息时会发生什么？严格来说，我当时并没有计划谈论这些内容。这很大程度上还在进展中，只是作为一系列可能想法困在我脑海中，但不是我能远程知道如何执行的东西。

但是我当时对算法推理也非常热衷。我现在仍然如此，我仍然相信构建能够与经典计算对齐的机器学习模型对于解决那些不能轻易通过仅仅收集更好数据集来解决的缺陷将是非常重要的。从根本上说，其中一些东西很可能被限制为无法轻易泛化到你训练它们的分布之外，特别是对于推理问题来说确实如此。

当你想到迄今为止用大语言模型完成的所有重大科学进展时，我认为我个人熟悉的大多数进展都是大语言模型与后台算法程序仔细结合的结果，该程序实际上确保给它稳健性属性。

想想像 FunSearch、AlphaCode、AlphaGeometry 这样的系统。所有这些系统都在计算机科学、竞争性编程、甚至国际数学奥林匹克的几何问题中发现了新知识。但在所有情况下，你都将语言模型连接到遗传算法或某种聚类机制或定理证明器，这些都有非常好的正确性保证，然后如果你能运行这个模型足够多次来使用算法纠正自己，你最终可以得到真正好的解决方案。

几何深度学习的问题是，正如我所说，它谈论对称性。排列或循环移位通常是具有非常特定和刚性行为的东西。通常我们对对称性假设的一件事是它们是可逆的。

基本上，每当我排列节点时，我总能将它们排列回来。我没有丢失任何信息。通常对于图像，当我们进行移位时，我们实际上用零填充图像以确保不会丢失图像数据等等。所以基本上它总是假设它仍然是相同的输入。我们没有丢失任何信息。

现在，对于真正对将模型与经典算法计算对齐感兴趣的我来说，为什么这是一个问题？正如任何计算机科学家都知道的，你编写的许多程序都会删除一些数据或破坏一些数据。所以这不再是一个对称性。你无法逆转它。

也许一个简单的例子不只是那种幼稚的"我要拿一个列表并无故删除其一半元素"的例子。让我们从路径查找开始。我们在计算机科学课程中经常谈论像 Dijkstra 或 Bellman-Ford 这样的算法。简而言之，这些是从有向加权图开始预测该图内最短路径长度的算法。

问题是，有许多不同权重的不同图将具有完全相同的最短路径，甚至可能具有相同的最短路径长度。然而，这些图是不同的。一旦你应用了 Dijkstra 算法或 Bellman-Ford 算法的变换，你就会丢失该算法最终输出中包含的关于图的信息。

因为许多不同的图将被压缩到完全相同的输出。所以这不是我可以使用对称性描述的操作。然后这个旅程逐渐——我花了一段时间才意识到我们如何能对此正式化，我们如何能尝试在此基础上建立一些理论，甚至现在我们如何能使用这个构建一些实用模型。

我很幸运能够开始与 Andrew 聊天，他是我在 DeepMind 的同事，他有范畴论背景，他自己过去一直在思考这些问题中的一些。所以这是一个很好的匹配，因为与他一起，这是一条漫长的道路，但我们设法逐渐放松了群给我们的约束。

我们首先研究了去除可逆性部分，这导致我们到单群，然后我们推导出一些有趣的理论和使用单群的模型中的异步不变性。现在我们也在研究去除群的第二个约束，即每个计算都必须与每个其他计算片段组合的要求。

正如你在计算机科学中可能也知道的，你不能总是做到这一点。你必须使你第一个函数的输出匹配第二个函数的输入类型，否则它们无法组合。所以这现在导致我们到范畴，这就是现在导致我们到范畴深度学习的原因。

主持人：像意向性和规划、系统二、推理等等这样的东西。我认为你在假设这些东西有什么标准的地方。

Petar：是的，确实如此。我的意思是在某种程度上确实如此，因为对于许多这些算法，我们甚至有证明，如果你给它们足够的时间并将它们放在正确的上下文中，它们将达到最优解。我实际上认为这将是一种协同作用，因为正如你所说，通过现代大规模深度学习系统，我们实际上有机会将真正复杂的嘈杂现实世界场景映射到那些算法可能变得适用的空间中。

现在我们在这里试图提出的主要论点是，要求模型既进行翻译又稳健地调用算法可能有点要求过多，因为除其他外，你说的固定计算预算已经是一个失败的方法，随着输入变大。因为正如你所知，例如乘法——我提到的是一个算法，这是一个我们真的还没有超高效算法的问题，最好已知的是 n log n，那个依赖于复杂的数论构造。

所以大多数人只知道乘法的 n 平方二次算法。所以你可靠地乘两个数字所需的资源量会增长，有时基于这两个数字的大小超线性增长。

目前我们的系统可以隐式地处理这个问题，如果你添加像思维链这样的东西，给模型更多的思考时间等等。但从根本上说，所有这些东西都是补丁，可能对特定类别的问题有帮助，但然后在其他地方失败，只是因为整个计算问题空间的复杂性。

所以基本上我相信一个未来，神经网络将处理对世界的理解，将世界中正在发生的事情翻译成某个抽象空间，这可能只是高维嵌入，顺便说一下，这也是可信的。然后会有一些我们通过先验或通过非常仔细的损失或甚至通过将系统与工具结合而烘焙到系统中的组件。

这已经证明非常有用，然后实际执行该计算，以一种我们可以推理它的方式。我应该强调，我说的推理不是指每个输入100%的准确性。我发现人类可以推理，人类对你给他们的每个输入都不是100%准确的。正如你可以看到的，如果你要求我乘两个50位长的数字，如果你要求我这样做，我肯定会犯一些错误。

但你知道重点是，我想要的是一个系统理解进行某种计算需要投入的努力量，也许至少给我一些要么对犯错误可能性的估计，要么某种概念——甚至某种"对不起，你要求我做的问题对我的能力来说计算量太大"的概念。

我想退缩而不回答。目前系统没有被训练来做这个。它们被训练来总是试图给你一个答案，这与那个非常不同。所以基本上我可以接受犯错误，但我真的希望有一些对何时可能发生错误以及它们会有多大的意识，当你应用算法时你经常有这个。所以你可以有正确性保证以及收敛保证之类的东西。

但你可能在想为什么我们首先需要使用如此抽象的数学？嗯，这种结构化思维实际上可以帮助我们看到什么真正重要。

### (30:00 - End) Part 3

这是一个很大的问题。当我们开始添加更多层次时，会发生什么呢？更多就是不同。这就是我们在更高范畴中开始看到的东西。当我们添加不同种类的关系和态射时，你开始看到这些也许最恰当地描述为涌现效应的东西。因为如果你有两个东西，你想研究它们作为复合体的行为，你可以单独研究它们的行为并观察联合行为，或者你可以组合这些系统并观察复合体的行为。我们经常发现这些并不相同。

在许多组合情况下，这些是同构的，但通常存在一个方向的映射而没有另一个方向的。所以你必须跟踪，你开始跟踪，因为这不再是等式了，你必须开始跟踪这个更高的自我。但现在这本身可能是另一个系统的一部分，产生了这些大量的层次和层次的涌现效应。所以更高范畴理论的问题之一就是对我们来说太难了，以我们思维的复杂性来做这个。所以什么是最好的基底，如何编码这些东西，肯定是一个开放的问题。

人们在编程语言理论中做了很多工作，不仅编码数据结构和算法，还编码这些算法以某种范畴方式托管的类型理论。相信强涌现的人认为没有还原主义。但即使是基本的弱涌现，不同尺度理论之间的分析捷径在计算上也是难以处理的。在它们之间转换非常困难。但我们想要有某种理论框架来捕捉整个事物，你知道，捕捉涌现的组织以及下面发生的事情。

所以这是范畴理论的目标之一，就是找到一些基本抽象，产生这些非常简单的原则。通常当你看一些系统或现象时，它非常复杂。但如果你足够幸运，如果你建立了良好的基础，总是，哦等等，它一直都很简单。我以错误的方式看待它。所以我不知道这能走向何方。但我们肯定在努力让事情尽可能具有组合性。在许多这些领域，我们有大量的实验证据和缺乏好的理论，范畴理论有一个非常好的有利位置，只是让我们停下来看看这里什么是好的有利位置的很多实际益处。

主持人：两个态射是否允许我们思考权重绑定？

绝对可以。如果我们将参数态射视为从A到B的映射，带有参数B，我们经常想要改变参数空间，我们经常想要说做权重绑定，你知道在实践中意味着我们从一个较小的权重空间开始并复制权重。所以在参数函数的两范畴中的双态射是重新参数化，所以它是两个参数态射之间的映射，以某种方式是连贯的，有一些必须满足的图表，但本质上它们编码了一个通过预组合某种权重绑定形式获得的。

但它不必仅仅是复制，这就是我们发现的。它可以是权重之间的任意关系。所以这些二胞和双态射允许我们的事情之一是看到这种代数结构编码为权重之间的关系。然后这又回到范畴是什么。它是关于找到对象和这个之间的关系。所以绝对是的。

所以这里是编程的关键连接。在函数式语言中，我们递归地定义数据类型，如列表。列表要么是空的，要么是一个元素后跟另一个列表。在范畴上，这是一个内函子的代数。代数的结构映射将数据类型的所有构造函数打包在一起。从这个代数的同态正是程序员所说的折叠——一个通过递归应用某些操作来消费列表的函数。所以这个框架描述了递归计算的结构。

当你写代码时，你遇到语法错误。你不太遇到语义错误。所以语法真的很基于你实际输入的东西，当你写某些东西时，无论是普通算法、网络架构等。语义更多关于程序如何表现，你知道一个例子是我们有像列表类型这样的东西。所以列表由类型构造函数定义。给定类型t，你有另一种类型列表of t。那么列表的语义是什么？列表的语义真正是可折叠的东西，某种可折叠类型。

所以像带加法的数字，它们是某种可折叠类型。如果我有一个数字列表，我可以只是添加它们来某种程度上移除列表。现在这些可折叠类型，数学家对它们有一个非常不同的名字——单元体，这是一种更一般的群。但无论如何，那就是列表的语义。在我说一些关于语法的东西之前，让我说我们的论文真正主要是从语义角度探索事物。为什么是这样？因为两个不同的语法可以很容易地描述同一件事。你知道我们可以有一个算术理论，我们有加法，我们也有否定，或者我们可以有减法。

你可以在这两种不同的语言中描述同样的事情，但语言确实是不同的。它们给你同样的语义，但语法是不同的。所以当做数学分析时，当证明定理时，从语义角度工作通常真的很有益。但值得真正强调的是，如果你想将这项工作与其他关于等变性等的工作进行比较，那项工作通常从语法角度进行。

让我们暂停来陈述范畴深度学习的中心主张。这个提议基本上是神经网络层应该被视为同一内函子的两个代数之间的同态。内函子描述网络需要遵守的计算类型，无论是群作用、列表折叠还是自动机转换。代数描述该计算如何转换特定数据。同态然后是在这两个数据表示之间映射的函数，同时保持计算结构。当这个同态是群作用时，你恢复几何深度学习。但框架本身要广泛得多。

例如，群作用的语法是什么？你真的可以说好吧，我只想想我有一种东西，然后每个群元素取那个东西并使其发送回自身。所以我有某种单一类型，然后每个群作用对那种类型做某事。所以我可能在平面上有一些点，我的群可能是旋转和反射，移动那些点，但我仍然在平面上。但事实证明这种单一排序的语法不足以捕捉计算机科学中的基本类型构造函数。

例如，列表，你不能仅使用单一排序来处理列表的语法。你需要多排序语法。所以你可以这样做的方式是说好吧想想我们有零元组、一元组、二元组等。然后给定k元组，我可能能够通过取我的k元组的元素并将它们制成一些列表ls来制作其他种类的元组，l元组。所以那是与群元素是语法相同意义上的语法，它确实具有组合性，如果我有一种将东西从元组打包到一堆列表中的方式，那么我可以将这些列表打包到其他列表的元组中。

但很清楚这首先是多排序的，然后也不同于群情况，因为所有这些都是高度不可逆的，对吧？你不能通过将东西越来越多地打包到列表中最终撤销列表，对吧？你只是得到更多列表。但我们基本上为语法构造一个模型。在群的情况下，集合中的模型意味着被群作用的集合。而例如，向量空间中的模型将意味着群的向量表示。而在列表语法的情况下，你只是得到我之前称为可折叠类型或单元体的东西，能够以预期方式执行这些语法操作的东西。

就像有些东西，如果你稍微扩展你的对象宇宙，数学推理就会工作得更好，即使你只关心原始对象。所以这是数学家多次学到的教训，这就是为什么很多人更喜欢在语义方面工作，至少在第一个实例中。事实证明，在数学中有一些非常非常基本的东西，我们都在小学学过，在GNN的设计中某种程度上被忽视了，那就是进位的概念。

那么进位到底是什么？嗯，假设我能够实现一个设备，一个数字轮，可以做模10的算术。所以上面有0到9。现在我想建立一种可以做模100算术的复合轮。那么我需要做什么？

我需要这种小机制，当轮子从9到零时，它将下一个轮子转动一个。这非常简单，但与过去GNN的构想方式极其不符，因为过去你通常发送整个状态，但状态中没有信息，信息只在状态的改变中。但这甚至比那更糟，即使你发送了状态的改变，那也不够信息，因为如果我从9到零，是因为我加了一吗？是因为我加了11吗？是因为我减了9吗？事实证明，在梯度下降的存在下让这种事情工作是相当微妙的。

所以这某种程度上是我们如何从更简单的计算操作组装更复杂的计算操作的一个非常基本的方面。我是说，如果你描述CPU的第一件事之一是你必须描述一个加法器。这已经是我们在GNN术语中努力做的事情。事实证明，当你做离散数学时这种行为很容易得到，当你做连续数学时非常复杂。你可以很容易地给出这个数字轮的例子。每个人都理解它，因为他们知道如何做加法。但让它以一种一切都连续的方式发生，事实证明真的很有趣。

这种现象的最简单例子直到你处理三维流形时才出现。所以你需要在四维空间中思考事物。我们知道的最简单例子是所谓的Hopf振动。这是一种情况，你可以分解三维球面。所以那是四维中的球面。你可以将其投影到二维球面上，使得所有预像都是一维球面或圆。三维球面与一维和二维球面的乘积非常不同，就像Z mod 100与Z mod 10和Z mod 10的乘积非常不同一样。

这是我个人现在非常兴奋的事情，来自这种异步工作——有没有方法利用这种几何微妙性来创造进位现象，实际恰当地建模算法推理的这个方面，并开始在神经网络中构建实际的CPU。

所以他们的主张最终是相当直接的。深度学习有两种语言：约束和实现，我们缺乏一个干净地将它们联系在一起的单一框架。范畴深度学习产生了桥梁，使用参数映射的双范畴中的通用代数。它恢复几何深度学习作为特例，同时自然地表达递归、权重绑定和不可逆计算等东西。如果你想要正式的故事，去阅读他们的论文，链接在描述中，特别是关于参数权重绑定和恢复几何深度学习的章节。很酷。谢谢观看。

---

*生成时间: 2025-12-22 22:32:44*
*由 YouTube Monitor & Translator (Claude CLI) 生成*