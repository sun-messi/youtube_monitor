# Are AI Benchmarks Telling The Full Story? [SPONSORED]

本文内容整理自 **Machine Learning Street Talk** 频道的视频。

原始链接：https://www.youtube.com/watch?v=rqiC9a2z8Io

---

> 本文内容整理自Prolific公司行为科学高级研究员安德鲁·戈登（Andrew Gordon）和AI研究员诺拉·佩特罗娃（Nora Petrova）在Machine Learning Street Talk频道的技术访谈。

## TL;DR

当前AI模型评估过分依赖技术基准测试，忽略了人类用户体验，Prolific团队提出更公平、多维度的人类偏好评估方法，发现模型在个性化和文化适应性方面表现不佳。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-02:30 | 技术基准vs用户体验的矛盾 | 解释为何技术性能高分不等于良好用户体验 |
| 02:30-05:40 | 当前评估体系的问题 | 分析现有模型评估方法的局限性和安全性缺失 |
| 05:40-08:42 | Prolific的改进方案 | 介绍分层抽样和多维度评估的新方法 |
| 08:42-11:47 | True Skill评估框架 | 详解基于信息增益的智能匹配算法 |
| 11:47-14:51 | 实验发现与模型个性 | 揭示模型在个性化表现上的不足 |
| 14:51-15:53 | 阿谀奉承行为分析 | 探讨模型过度迎合用户的负面趋势 |

## 📊 核心论点

#### 技术基准与实用性脱节：F1赛车悖论

- **核心内容**：技术基准测试就像F1赛车——在赛道上表现完美，但作为日常通勤工具却是噩梦。在人文学科最终考试（Humanity's Last Exam）或MMLU基准测试中得高分的模型，在日常使用中可能体验糟糕。大部分基准报告都集中在技术指标上，缺乏人类参与的评估环节，导致性能与用户体验严重脱节。
- **关键概念**：技术基准测试、用户体验、人类偏好评估、MMLU、实用性差距
- **实际意义**：推动AI行业重新思考评估标准，从纯技术竞争转向以用户为中心的设计理念，影响模型开发和产品化方向。

#### 评估标准化缺失造成的混乱局面

- **核心内容**：AI评估领域极其年轻，缺乏统一标准。不同实验室采用不同的基准报告方式——有些强调特定考试成绩（如Grok 4的人文学科最终考试），有些完全不提供基准数据。这种异质性使得模型间的公平比较变得困难，容易误导用户和投资者对模型真实能力的判断。
- **关键概念**：标准化缺失、异质性报告、基准选择偏差、比较困难、评估透明度
- **实际意义**：呼吁建立行业统一的评估标准，提高模型选择的科学性，为监管政策制定提供依据。

#### 安全性评估的盲区：野西部状态

- **核心内容**：目前没有专门的安全性排行榜，安全性甚至不在大多数研究者的考虑范围内。用户越来越多地将这些模型用于心理健康、人生导航等敏感话题，但缺乏相应的监管和伦理规范。其他涉及类似敏感话题的领域都有严格的监管和伦理准则，而AI领域却处于"野西部"状态。
- **关键概念**：安全性排行榜、敏感话题应用、监管缺失、伦理准则、野西部现象
- **实际意义**：凸显AI安全治理的紧迫性，为建立AI安全标准和监管框架提供理论基础，保护用户免受潜在伤害。

#### Chatbot Arena的系统性偏差问题

- **核心内容**：虽然Chatbot Arena是唯一主要的人类偏好排行榜，但存在严重的系统偏差。研究发现，某些公司在产品正式发布前进行大量私下测试（如Meta在Llama 4发布前测试了27个模型），获得更多比较数据和prompt访问权限，从而优化出更适应Arena的模型，破坏了竞争的公平性。
- **关键概念**：系统偏差、私下测试、数据不平等、竞争公平性、排行榜完整性
- **实际意义**：暴露了当前主流评估平台的根本缺陷，推动建立更严格、方法论更科学的评估体系。

#### 多维度偏好评估的必要性

- **核心内容**：传统的"我更喜欢这个回答"式评估过于简化，无法为模型改进提供可操作的指导。Prolific将偏好分解为具体维度：有用性、沟通风格、适应性、个性等，当发现模型在某个维度表现不佳时，开发者能够针对性地改进，而不是盲目调整。
- **关键概念**：多维度评估、可操作反馈、偏好分解、针对性改进、评估粒度
- **实际意义**：为AI模型的精细化改进提供科学方法，提高开发效率，推动用户体验的系统性提升。

#### True Skill算法：基于信息增益的智能评估

- **核心内容**：借用微软为Xbox Live开发的True Skill框架，该算法考虑游戏中的随机性、技能水平随时间的变化、运气与实力的区别。基于信息增益原则选择下一对比较模型，优先进行能最大化学习效果、最快降低不确定性的对战，比传统的随机配对更高效。
- **关键概念**：True Skill算法、信息增益、不确定性最小化、智能配对、评估效率
- **实际意义**：大幅提高评估效率，减少所需的比较次数，为大规模模型评估提供技术基础，可扩展到更多模型和人群。

#### 代表性抽样解决评估偏差

- **核心内容**：基于美英两国人口普查数据进行分层抽样，按年龄、种族、政治倾向等人口统计学特征确保样本代表性。这种方法确保评估结果能够反映真实世界的用户偏好，而不是某个特定群体的偏好，避免了Chatbot Arena中匿名用户可能带来的偏差。
- **关键概念**：分层抽样、人口普查数据、代表性样本、偏差控制、真实世界偏好
- **实际意义**：提高评估结果的外部效度，为全球化AI产品开发提供可靠的用户偏好数据，推动更公平的AI发展。

#### 模型个性化能力的显著不足

- **核心内容**：在6个领先模型的测试中，所有模型在个性化指标和背景文化理解方面的表现都明显低于有用性、沟通性、适应性等客观指标。这可能是因为模型不了解用户的背景文化，或者训练数据（整个互联网）本身无法产生令人满意的个性化表现。
- **关键概念**：个性化不足、背景文化理解、训练数据局限、主观vs客观指标、文化适应性
- **实际意义**：指出当前AI个性化技术的关键短板，为改进训练方法和数据策略提供方向，推动更人性化的AI发展。

#### 阿谀奉承行为的负面趋势

- **核心内容**：虽然各大公司对模型进行了大量微调以塑造个性和回答风格，但近期观察到模型出现越来越多的阿谀奉承（sycophancy）或过度取悦用户的行为，用户普遍对此感到反感。这种行为可能与个性评分的下降存在相关性，需要进一步研究其对用户偏好的具体影响。
- **关键概念**：阿谀奉承行为、过度取悦、微调副作用、用户反感、个性评分下降
- **实际意义**：警示过度优化用户满意度可能适得其反，指导更平衡的模型训练策略，避免人工化的讨好行为。

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Prolific | 研究团队所在公司，开发Humane评估平台 | ⭐⭐⭐ |
| Microsoft | True Skill算法开发者（Xbox Live） | ⭐⭐ |
| Meta | Llama 4发布前私下测试27个模型的案例 | ⭐⭐ |
| Anthropic | Constitutional AI和机械可解释性研究的先驱 | ⭐⭐ |
| Chatbot Arena | 当前主要的人类偏好排行榜平台 | ⭐⭐⭐ |

## 💬 经典金句（3-5句）

> "如果你只依赖那些技术指标，你就错过了一半的要点——这些模型最终是为人类设计使用的。"
> — Andrew Gordon

> "没有安全性排行榜，我们甚至不按安全性给LLM评分。"
> — Nora Petrova

> "在真实世界中，简单的偏好数据是无用的——它能做出漂亮的排行榜，但无法告诉公司为什么用户有这样的偏好。"
> — Andrew Gordon

> "这就像野西部一样，一些公司比其他公司更认真地对待这个问题。"
> — Andrew Gordon

## 👤 主要人物

#### 安德鲁·戈登（Andrew Gordon）

**身份**：Prolific公司行为科学高级研究员（Staff Researcher in Behavioral Science）
**背景**：专注于在线研究中的人类参与问题，在科学团队负责解决与人类研究相关的问题，特别是在线研究方面的挑战
**核心观点**：强调技术基准测试与用户体验脱节的问题，主张建立更严格的人类偏好评估方法，认为当前AI评估过于重视技术性能而忽视了用户的真实需求和体验

#### 诺拉·佩特罗娃（Nora Petrova）

**身份**：Prolific公司AI研究员（AI Researcher）
**背景**：专注研究如何将人类纳入AI模型的开发和评估过程，致力于AI与人类价值观的对齐问题，以及全面理解AI模型的能力边界
**核心观点**：强调安全性评估在AI发展中的重要性，指出当前缺乏安全性排行榜的问题，倡导使用True Skill等科学方法来改进AI评估的准确性和公平性

## 📺 视频类型判断

**访谈对话**：两位研究者接受主持人采访，讨论AI模型评估方法的问题和改进方案

---

## 📝 完整翻译

### (0:00 - 15:00) Part 1

Andrew Gordon：一级方程式赛车是工程学的绝对巅峰，一切都完美无缺，拥有巨大的最高速度。但如果你把它当作日常通勤车，那绝对是一场噩梦。我认为这些模型也是如此。一个在人类最后考试或MMLU上表现极其出色的模型，在日常使用中可能是绝对的噩梦。目前大多数基准测试报告都是关于技术基准的。你给模型一组评估，可能是某个主题或某个考试，然后得到一个分数，人类并没有真正参与到这个循环中。我是Andrew Gordon，Prolific行为科学团队的研究员。我在科学团队工作，专门解决与人类研究相关的问题，特别是在线研究。

Nora Petrova：我是Nora Petrova，Prolific的AI研究员。我致力于解决如何将人类纳入AI模型的开发和评估中的问题。如何使它们与人类价值观保持一致？我们如何充分理解它们的能力？

Andrew Gordon：人们认为模型有多有用？沟通效果如何？他们认为它的适应性如何？他们对模型的个性有什么看法？当你在这些因素上得到评分时，你实际上得到的是一组可操作的结果，比如你的模型在信任方面有困难，或者你的模型在个性方面有困难。我们首次尝试了名为"Prolific用户体验排行榜"的概念验证，有500名来自美国的代表性参与者，他们每次评估一个模型，并以量表形式给出反馈。你觉得这个模型有多有用，1到7分等等。我们现在已经从最初的排行榜中汲取了经验教训，并将其构建为我们称之为"Humane"的主要排行榜。它使用了类似于聊天竞技场的方法，我们在模型之间进行比较对战，这实际上让我们能够更清楚地区分哪个模型表现更好。如果我们有一个更公平的方法，实际上根据人们的年龄、居住地和价值观进行多样化抽样和分层，那会怎样？什么是理解模型行为和进行评估的更公平方法？这就是Andrew和Nora在Prolific所做的工作。我们如何知道这些模型对使用它们的人类来说是否真正有用？我们如何制定更公平的评估指标？

Andrew Gordon：但问题是，目前评估和基准测试这些模型的领域极其新兴，它只是在过去几年LLM出现以来才存在。正因为如此，这是一个支离破碎的领域。对于这些实验室如何报告基准测试数据，没有标准的竞争环境。有些可能会强调，比如最近的Grok 4，我们看到对人类最后考试的巨大强调，而对其他基准的强调较少，有些模型甚至根本没有任何基准测试数据就发布了。这些实验室报告结果的方式存在很大的异质性，这导致我认为我们面临着难以在任何公平竞争环境中真正比较模型的风险。当然还有更大的问题，模型经常因为在人文最后考试中获得最高分而受到赞扬，从技术角度我们知道模型已经超越了竞争对手，但对我来说，我在这个领域的核心论点是，如果你只依赖那些技术指标，你就错过了一半的要点。这些模型归根结底是为人类使用而设计的。大多数用户都是人类，在这些考试中的简单表现并不一定与良好的用户体验相关。所有前沿实验室真的需要开始将人类偏好排行榜与所有技术指标一起更多地考虑在内。人们越来越多地将这些模型用于非常敏感的话题和问题，用于心理健康，用于如何解决生活中的问题，而这方面没有监督。在讨论这些话题的任何其他领域，都有很多监管和许多道德行为准则。而这里目前就像是狂野西部，有些公司比其他公司更认真地对待它，试图研究人类将模型用于更个人话题和问题的方式。我们最近看到了一些令人震惊的例子，Grok 3和Meta Healer，这确实引发了关于一些模型顶部的安全训练有多薄弱的问题。

Nora Petrova：没有安全性排行榜，没有像我们不会根据LLM的安全程度来评分的指标。实际上，除了一些研究人员之外，这甚至不在考虑范围内。我认为这应该和模型的速度或智能程度一样重要。对人们使用来说有多安全？在这个方向上，Anthropic在安全性方面进行了很多有趣的研究，关于模型的对齐，使用宪法AI和他们探索的各种方法，还有关于机制可解释性的研究，窥视模型的幕后，理解输入如何产生某个输出，哪些特征概念、哪些电路在此过程中被激活，基本上是追踪这些模型的思路，试图隔离潜在问题可能出现的地方。这类工作非常重要，提高了对这些模型能够以安全方式处理新情况的信心。我想说的另一件事是，考虑到聊天机器人竞技场是唯一的、坦率地说几乎是LLM唯一的人类偏好排行榜，我们真正理解幕后发生的事情非常重要。显然，聊天竞技场是完全开源的。人们进入，输入一个提示，从两个不同的模型得到响应。然后他们说哪个更好。那篇论文发现，实际上在背景中发生的是一些公司比其他公司获得了更多的私人测试访问权限。例如，在Llama 4发布之前，我们看到Meta在竞技场上发布了27个模型。但当然，最终只报告了一个，这显然破坏了竞技场的完整性，因为你的模型比较越多，你获得的提示访问权限越多，你拥有的数据越多，你就能完善出一个在竞技场上表现更好的模型。这在数据中增加了一个偏差元素，很难规避。还有其他被指出的问题，以及我们自己看到的问题，我们认为这表明需要更严格和方法论上更可靠的方法来做这种人类偏好数据集。

我认为我们有一个很好的想法，超越了对排行榜幻象论文的批评。我们认为那篇论文实际上没有真正涉及到我们认为在进行人类偏好评估时应该关心的其他一些事情。对我来说，我们寻求改进的有三个大的领域。首先，正如你提到的样本。显然，聊天机器人竞技场的样本是任何人。我们对他们一无所知。我们不收集任何人口统计数据。所以他们只是匿名去那里提示模型并给出他们的偏好数据的人。显然这很好。你得到了大量的数据，这很棒，但你对提供数据的人一无所知，这是相当次优的。然后在特异性方面，任何使用过聊天竞技场的人，你所做的只是说我更喜欢这个回应或我更喜欢那个回应。在现实世界中，这种数据在某种意义上是无用的。它为你提供了一个非常好的方式来制作AI模型的漂亮排行榜，但它没有告诉公司为什么给出了这种偏好。所以在我们的方法中，我们寻求通过实际将偏好分解为其组成部分来缓解这个问题。比如人们认为模型有多有用？沟通效果如何？他们认为它的适应性如何？他们对模型的个性有什么看法？当你在这些因素上得到评分时，你实际上得到的是一组可操作的结果，比如你的模型在信任方面有困难，或者你的模型在个性方面有困难。这就是你需要专注的地方，真正构建一个对现实世界中的真实用户有好处的模型。但没有质量保证，我可以进去说你好，或者什么都不说，或者进行多轮对话，完全从太阳有多大游荡到蛇有多长，就是话题游荡，我不认为这是模型的真正细致的观点。所以我们在结构中建立了参与者进来与模型进行多步对话的机制。我们建立了质量保证，实际上说，如果你在问题上投入很少的努力或者你开始游荡，我们会惩罚你，三次这样的行为你就出局了。这些是我们围绕排行榜构建的原则。

Nora Petrova：我想谈谈我们使用的方法论，即TrueSkill。这是微软开发的一个框架，用于估计Xbox Live上玩家的技能水平。他们考虑游戏中的随机性，随时间变化的技能水平，某人是否有偶然的连胜与始终表现良好的经验丰富的玩家。所有这些我们认为应该考虑的事情。这是一个非常灵活的系统，估计概率时基于分布，有均值和方差，随着时间推移，系统了解这些对战或比较的结果，方差会越来越窄。最重要的是，它基于信息增益。我们选择锦标赛中下一对的方式是基于我们从这些模型的正面交锋中能学到多少。它们给我们多少信息？它们减少了多少不确定性？我们根据这个来排列对战队列，这让我们尽可能快地到达最小化不确定性的位置。这是一个非常灵活的方法。我们可以运行单独的锦标赛，就像我们对人口统计群体所做的那样。我们有大约20个人口统计群体，我们为他们运行了单独的锦标赛，我们可以合并每个锦标赛的发现，以获得一个整体排行榜，其不确定性比我们从任何人口统计群体产生的任何单独锦标赛或排行榜都要小得多。所以它真的允许我们以我们想要的任何方式分割数据，我们可以轻松地添加更多的人口统计群体，随时间推移添加更多模型。我们正在公开开发并欢迎反馈。

Andrew Gordon：我认为他们在排行榜幻象论文中指出的一个问题是，实际上一些模型的抽样率比其他模型高得多，我相信聊天机器人竞技场背后的人说这是因为人们来竞技场玩最新的模型。这很好，人们想玩最先进的技术，这很棒，但它不会导致有效的抽样方法。它基本上意味着一些模型比其他模型得到更多的对战。因此它们得到更多的数据。因此它们在竞技场中变得更好。对战数量和排行榜位置之间有很强的关系。我们只根据数据的需要进行对战。所以如果特定模型对另一个特定模型的不确定性很高，我们进行对战来降低不确定性。所以这完全由数据驱动。这在计算上是合理的，因为我们实际上不会进行超过我们需要的比较。它允许我们真正达到基于不确定性强烈区分模型的点。

Nora Petrova：如果我们在不确定性方面有某个目标，为了在我们感兴趣的置信区间内完全区分模型，我们可以进行更多对战直到达到目标。控制权在我们手中。本质上，我们只需要招募更多参与者以达到那种确定性水平。我们为这项研究采样的方式，显然我们使用来自Prolific平台的自己的参与者，但我们基于我们拥有的美国和英国人口普查数据进行有效抽样。这个长期愿景显然是一个更全球化的产品，但目前我们所做的是根据人口统计数据分层我们的样本，即给我们反馈的参与者，比如他们的年龄、种族、政治取向。我们从人口普查中获得了大量数据，告诉我们每个国家由这些人口统计数据的特定比例组成，这基本上让我们能够说，当我们汇总所有这些发现并找到领先模型时，我们可以非常自信地说，这个模型被尽可能具有代表性的普通公众所偏爱。所以希望在这个意义上，它更多地与世界上人们的真实世界偏好相关，而不是可能响应聊天机器人竞技场的非常有偏见的子集。我们将第一个作为MVP概念验证，更多的是证明我们可以以严格和方法论上合理的方式做到这一点。当我们实际运行时，我们只用了500名参与者。它给了我们很多关于如何构建Humane的洞察，这是我们目前正在工作的排行榜。现在那个排行榜实际上正在后台运行。我们仍在进行对战。所以我们期望能够从中获得更多数据。但我可以说关于我们做的第一轮，模型在我们测试的六个模型中往往表现得全面较差，这些都是当时的领先模型，它们在个性指标和背景文化指标上的表现比有用性、沟通性、适应性等方面要差得多。这真正表明的是，这些模型有一些更主观的方面，人们可能不太满意，可能只是他们在做不会激发模型个性或不会激发模型谈论背景和文化的任务。另外模型不了解他们的背景和文化，所以很难与他们对齐。但另一种可能性是模型在这方面可能真的不是很好。这可能是他们接受训练的数据的影响。我们知道的很少。显然模型是在整个互联网上训练的。但当你在整个互联网上训练一个模型时，你会得到一个真正代表人们想要的个性吗？从这次测试中，我们发现人们对模型个性或其理解他们背景或文化的能力的印象通常不如对更客观措施的印象深刻。

显然，这些模型中的很多都经过了广泛的微调，以调整它们的个性或调整它们回答问题的方式，这在不同公司之间是不同的。但我们最近观察到，模型的心理谄媚或这种取悦人的行为有所增加，人们普遍似乎不喜欢它。

### (15:00 - End) Part 2

这个实验结果以及后续数据集将使我们能够回答的一个问题是，心理谄媚的明显迹象与个性指标中的负面评价之间存在什么关联？这是否会影响人们对模型偏好的决定？我们可以对数据进行各种类型的后处理和分析，以识别数据集中观察到的心理谄媚水平，并尝试找出人们给出的反馈与更多由模型驱动的LLM作为评判者导向的对话分析，以及各种模式分类和模型表现之间的有趣关系。看看我们会发现什么将会相当有趣。