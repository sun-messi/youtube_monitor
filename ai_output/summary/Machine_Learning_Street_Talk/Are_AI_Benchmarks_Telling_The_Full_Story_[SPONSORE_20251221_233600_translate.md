# Are AI Benchmarks Telling The Full Story? [SPONSORED]

## 📹 视频信息

- **频道**: Machine Learning Street Talk
- **发布日期**: 2025-12-20
- **时长**: 16:03
- **原始链接**: [https://www.youtube.com/watch?v=rqiC9a2z8Io](https://www.youtube.com/watch?v=rqiC9a2z8Io)

---

> 本文内容整理自 Prolific 行为科学研究员安德鲁·戈登（Andrew Gordon）和 AI 研究员诺拉·佩特洛娃（Nora Petrova）在 Machine Learning Street Talk 频道的技术访谈。

---

## TL;DR

当前 AI 评估主要依赖技术基准测试，忽略了人类用户的实际体验，Prolific 团队提出更公平、更全面的人类偏好评估方法。

---

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-01:30 | 技术基准 vs 用户体验差异 | 解释为什么高分模型在日常使用中可能体验很差 |
| 01:30-03:30 | 当前评估体系的问题 | 分析现有基准测试缺乏标准化和人类维度的局限性 |
| 03:30-06:00 | 安全评估的缺失 | 强调缺乏安全性评估标准和监管的风险 |
| 06:00-09:00 | Chatbot Arena 的局限性 | 揭示现有人类偏好评估平台的方法学缺陷 |
| 09:00-12:00 | TrueSkill 评估方法论 | 介绍基于信息增益的科学评估框架 |
| 12:00-16:00 | 人口统计学采样与发现 | 展示代表性采样方法和模型在个性化方面的不足 |

---

## 📊 核心论点

#### 技术基准与用户体验的脱节

- **核心内容**：当前 AI 评估过度依赖技术基准测试（如 MMLU、人类最后考试），这些指标虽然能反映模型的技术能力，但无法预测实际用户体验。就像 F1 赛车在赛道上性能卓越，但作为日常通勤工具却是灾难。高分模型可能在实际使用中表现糟糕，因为技术性能不等于用户满意度。
- **关键概念**：技术基准测试、用户体验评估、MMLU 评估、人类最后考试、实用性与性能差异
- **实际意义**：AI 公司需要重新思考评估策略，不能仅追求技术指标排名，而应关注模型的实际可用性和用户满意度，这将影响产品开发方向和市场竞争格局。

#### 评估体系缺乏标准化和人类维度

- **核心内容**：当前 AI 评估领域高度分散，各实验室报告基准数据的方式差异巨大。某些模型强调特定基准（如 Grok 4 重点宣传人类最后考试成绩），而其他模型可能完全不提供基准数据。这种异质性导致模型间难以公平比较，更重要的是，这些评估完全忽略了人类用户的主观体验。
- **关键概念**：标准化评估、异质性报告、公平比较、主观体验、人类维度评估
- **实际意义**：行业需要建立统一的评估标准，包括技术指标和人类偏好指标，这将提高模型比较的透明度，帮助用户和企业做出更明智的选择。

#### 安全性评估的严重缺失

- **核心内容**：目前没有针对 AI 模型安全性的标准化评估体系或排行榜，用户越来越多地使用这些模型处理敏感话题（心理健康、人生问题），但缺乏相应的监管和伦理规范。在其他涉及类似敏感话题的领域都有严格的规范和监管，而 AI 领域目前是"狂野西部"状态。
- **关键概念**：安全性评估、敏感话题处理、监管缺失、伦理规范、安全训练、野西部状态
- **实际意义**：急需建立 AI 安全性评估标准和监管框架，这关系到用户的心理健康和社会安全，将推动行业自律和政策制定。

#### Chatbot Arena 评估方法的系统性偏差

- **核心内容**：虽然 Chatbot Arena 是目前唯一的大规模人类偏好评估平台，但存在严重的方法学问题。某些公司在正式发布前进行了大量私密测试（如 Meta 在 Llama 4 发布前测试了 27 个模型版本），这种不透明的预测试给某些公司带来不公平优势，因为更多的比较数据意味着更好的模型优化机会。
- **关键概念**：私密测试、不公平优势、方法学偏差、透明度缺失、数据获取不平等
- **实际意义**：需要建立更公平透明的评估机制，确保所有模型开发者享有平等的测试机会，这将促进健康的市场竞争。

#### 采样偏差和评估粒度不足

- **核心内容**：Chatbot Arena 的参与者是匿名的随机用户，没有收集任何人口统计学数据，这导致评估结果可能被特定群体的偏好主导。同时，评估只是简单的"更喜欢哪个回应"，缺乏具体的反馈维度，无法为模型改进提供可操作的洞察。这种粗粒度的评估无法识别模型在信任、个性、适应性等方面的具体问题。
- **关键概念**：匿名用户、人口统计学偏差、粗粒度评估、可操作洞察、多维度反馈
- **实际意义**：需要建立基于代表性采样的多维度评估体系，为 AI 开发者提供具体的改进方向，提升模型的实际可用性。

#### TrueSkill 算法的科学化评估方法

- **核心内容**：Prolific 采用微软为 Xbox Live 开发的 TrueSkill 算法，该算法考虑游戏中的随机性、技能水平的时间变化、以及区分偶然获胜和持续强劲表现。基于信息增益原理，算法智能选择下一对模型进行比较，以最快速度最小化不确定性。这种方法比传统的随机配对更高效，能够更准确地评估模型的真实能力。
- **关键概念**：TrueSkill 算法、信息增益、不确定性最小化、智能配对、概率估计、方差收敛
- **实际意义**：为 AI 评估提供了科学严谨的方法学基础，能够以更少的比较次数获得更可靠的排名结果，降低评估成本提高准确性。

#### 模型在个性化方面的显著不足

- **核心内容**：通过对 6 个领先模型的测试发现，模型在个性和背景文化理解方面的表现明显逊色于帮助性、沟通性、适应性等客观指标。这可能是因为模型在互联网数据上训练时，难以形成用户真正想要的个性特征，或者模型无法理解用户的具体背景和文化，导致在主观体验方面得分较低。
- **关键概念**：个性化不足、背景文化理解、主观指标 vs 客观指标、训练数据局限性、用户期望差距
- **实际意义**：揭示了当前大语言模型的重要局限性，提示开发者需要在个性化和文化适应性方面投入更多精力，这将影响未来模型设计方向。

#### 模型"讨好行为"的负面影响

- **核心内容**：近期观察到模型出现更多的"psychopancy"（迎合讨好）行为，即过度迎合用户偏好而非提供客观建议。用户普遍不喜欢这种行为，因为它降低了模型的可信度和实用性。通过分析讨好行为与用户负面评价的相关性，可以帮助开发者找到平衡点，既要友好又要保持客观。
- **关键概念**：迎合讨好行为、客观性 vs 友好性、用户信任、平衡点、可信度
- **实际意义**：为模型训练和微调提供重要指导，帮助开发者避免过度讨好，保持模型的客观性和可信度，提升长期用户满意度。

---

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Prolific | 人类行为研究平台，开发新的AI评估方法 | ⭐⭐⭐ |
| Meta | 在Chatbot Arena私密测试27个模型版本的案例 | ⭐⭐ |
| Microsoft | 开发TrueSkill算法用于Xbox Live | ⭐⭐ |
| Anthropic | 在AI安全和宪法AI方面的研究贡献 | ⭐⭐ |
| Grok/xAI | 强调"人类最后考试"基准的例子 | ⭐ |
| Chatbot Arena | 当前主要的人类偏好评估平台 | ⭐⭐⭐ |

---

## 💬 经典金句

> "Formula 1 cars are the absolute pinnacle of engineering, right? Everything is perfect, huge top speed. If you used it as your daily commuting car, you'd have an absolute nightmare, right? And I think the same can be said for these models."
> — Andrew Gordon

> "There is no leaderboard for safety, right? We don't grade LLMs by how safe they are... I would argue that that should be just as important as how fast or smart the model is."
> — Nora Petrova

> "These models are designed for humans to use at the end of the day. Most of the users are humans and simple performance on these exams doesn't necessarily correlate to a good user experience."
> — Andrew Gordon

---

## 👤 主要人物

#### 安德鲁·戈登（Andrew Gordon）

**身份**：Prolific 行为科学团队研究员
**背景**：专注于在线研究中的人类行为科学，研究人类在AI模型开发和评估中的作用
**核心观点**：认为当前AI评估过度依赖技术基准，忽略了用户体验，强调需要将人类偏好评估纳入主流评估体系，并建立更科学的方法学框架。

#### 诺拉·佩特洛娃（Nora Petrova）

**身份**：Prolific AI研究员
**背景**：专门研究如何将人类纳入AI模型的开发和评估过程，关注AI与人类价值观的对齐
**核心观点**：强调AI安全性评估的重要性，认为应该像评估模型智能程度一样重视安全性，同时关注模型的"讨好行为"对用户体验的负面影响。

---

## 📺 视频类型判断

**访谈对话**：两位研究专家就AI评估方法学进行深入讨论，解释他们在Prolific开发的新评估框架

---

## 📝 完整翻译

### (0:00 - 1:30) 技术基准 vs 用户体验差异
> 解释为什么高分模型在日常使用中可能体验很差

一级方程式赛车是工程技术的绝对巅峰，对吧？一切都很完美，拥有巨大的最高速度。但如果你把它当作日常通勤车使用，那绝对是一场噩梦。我认为这些模型也是如此。一个在人类最后考试或 MMLU 上表现非常出色的模型，在日常使用中可能是绝对的噩梦。如今大多数关于基准测试的报告都是基于技术基准的，你给模型一组评估，可能是某个主题或某场考试，然后你得到一个分数，而人类在这个循环中并不真正参与。

嗨，我叫 Andrew Gordon，我是 Prolific 行为科学的高级研究员。我在科学团队工作，专门解决与研究中的人类相关的问题，特别是在线研究。

Nora Petrova：我叫 Nora Petrova，我是 Prolific 的 AI 研究员。我专门解决如何在 AI 模型的开发和评估中纳入人类因素的问题，如何让它们与人类价值观保持一致，以及如何充分理解它们的能力。

人们认为模型有多有帮助？沟通效果如何？他们觉得模型的适应性如何？他们对模型的个性有什么看法？当你在这些因素上获得评分时，你实际上得到的是一组可操作的结果，告诉你：好吧，你的模型在信任方面有困难，或者你的模型在个性方面有困难。我们首先尝试了所谓的 Prolific 用户体验排行榜，这是一个概念验证，有来自美国的 500 名参与者，这是一组有代表性的参与者，他们一次评估一个模型，并以量表格式给出反馈。

### (1:30 - 3:30) 当前评估体系的问题
> 分析现有基准测试缺乏标准化和人类维度的局限性

你知道，你觉得模型有多有帮助，1 到 7 分，诸如此类。我们实际上现在已经改进了，从那个初始排行榜中吸取了经验教训，并将其构建成了我们称之为 HUMANE 的主要排行榜。它使用了与 Chat Arena 类似的方法，即我们在模型之间进行比较对战，这实际上让我们能够更清楚地区分哪个模型表现更好。

如果我们能有一个更公平的方法，实际上根据人们的年龄、居住地和价值观来多样化抽样和分层，会怎么样？什么是理解模型行为和进行评估的更公平的方法？这就是 Andrew 和 Nora 在 Prolific 一直在做的事情。我们如何知道这些模型对使用它们的人类是否真的有好处？我们如何制定更公平的评估指标？这就是 Andrew 和 Nora。

### (3:30 - 6:00) 安全评估的缺失
> 强调缺乏安全性评估标准和监管的风险

Andrew Gordon：但问题是，目前评估和基准测试这些模型的领域极其新兴，对吧？它的存在时间只有 LLM 在过去几年中存在的时间。正因为如此，这是一个支离破碎的领域。这些实验室如何报告基准测试数据没有标准的竞技场。你知道，有些可能会强调——我们最近在 Grok 4 中看到，对人类最后考试给予了巨大的重视，而对其他基准测试的重视较少，有些模型发布时根本没有任何基准测试数据。

基本上，这些实验室如何报告结果存在很多异质性，这让我觉得我们面临着在任何公平竞技场上实际比较模型的风险。当然，还有更大的问题，模型经常因为在人文最后考试中获得最高分而受到赞扬，所以我们从技术角度知道模型已经超越了其竞争对手。

但对我来说，我在这个领域的核心论点是，如果你只依赖这些技术指标，你就错过了一半的重点，对吧？这些模型最终是为人类使用而设计的。大多数用户都是人类，在这些考试中的简单表现并不一定与良好的用户体验相关。所有前沿实验室真的需要开始更多地关注人类偏好排行榜，与所有技术指标并行。

人们越来越多地将这些模型用于非常敏感的话题和问题，用于心理健康，用于他们应该如何处理生活中的问题，而这方面没有监督。在讨论这些话题的任何其他领域，都有大量的监管和内置的伦理行为准则。而在这里，目前还是蛮荒西部，有些公司比其他公司更认真地对待这个问题，并试图研究人类使用模型处理更个人话题和问题的方式。

### (6:00 - 9:00) Chatbot Arena 的局限性
> 揭示现有人类偏好评估平台的方法学缺陷

我们最近在 Grok 3 和 Meta Llama 中看到了一些相当令人震惊的例子，这确实引发了关于这些模型顶层安全训练有多薄弱的问题。

Nora Petrova：嗯，安全方面没有排行榜，对吧？没有像我们根据 LLM 的安全程度对它们进行评分的指标。事实上，除了一些研究人员之外，这甚至不是一个真正的问题。所以，我的意思是，我认为这应该与模型的速度或智能程度一样重要。你知道，人们使用它有多安全？

Anthropic 在这个方向上有很多有趣的研究，关于安全性，关于模型的对齐，使用宪法 AI 和他们探索的各种方法，以及关于机制可解释性，即窥视模型的幕后，理解输入如何产生某种输出，哪些特征概念、哪些电路在此过程中被激活，基本上是追踪这些模型的思想，试图找出潜在问题可能出现的地方。

这种工作非常重要，提高了这些模型能够以安全方式处理新颖情况的信心。我想说的另一件事是，鉴于 Chatbot Arena 是目前唯一的，坦率地说几乎是唯一的 LLM 人类偏好排行榜，我们真正理解幕后发生的事情非常重要。

显然，Chat Arena 是完全开源的。人们进入，输入一个提示，从两个不同的模型获得响应。然后他们说哪一个更好。那篇论文发现，实际上幕后发生的是，一些公司在幕后获得了比其他公司更多的私人测试机会。例如，在 Llama 4 发布之前，我们看到 Meta 在 Arena 上发布了 27 个模型。但当然，最终只有一个被实际报告，这显然破坏了 Arena 的完整性，因为你为你的模型进行的比较越多，你获得的提示访问越多，你拥有的数据越多，你就能改进一个在 Arena 中表现更好的模型。

### (9:00 - 12:00) TrueSkill 评估方法论
> 介绍基于信息增益的科学评估框架

这给数据增加了一个偏见元素，很难绕过。还有其他被指出的问题，以及我们自己看到的问题，我们认为这些问题表明需要一种更严格、更方法论上合理的方法来做这种人类偏好数据集。我认为除了对排行榜幻觉论文的批评之外，我们有一个很好的想法。我们认为那篇论文实际上没有真正触及我们认为在进行人类偏好评估时应该关心的其他一些事情。

对我来说，我认为有三个主要领域是我们寻求改进的地方。首先，正如你提到的样本。显然，Chatbot Arena 的样本是任何人，对吧。我们对他们一无所知。我们不收集任何人口统计数据。所以他们只是匿名去那里提示模型并给出偏好数据的人。现在显然这很好。你得到大量数据，这很棒，但你对给出数据的人一无所知，这相当不理想。

然后在特异性方面，任何使用过 Chat Arena 的人，你所做的就是说我更喜欢这个回应或我更喜欢那个回应，对吧？在现实世界中，这种数据在某种意义上是无用的，对吧？它给你一个制作 AI 模型精美排行榜的好方法，但它告诉公司为什么给出这种偏好的任何信息都没有。

所以在我们的方法中，我们试图通过实际将偏好分解为其组成部分来缓解这个问题。比如人们认为模型有多有帮助？沟通效果如何？他们觉得它的适应性如何？他们对模型的个性有什么看法？当你在这些因素上获得评分时，你实际上得到的是一组可操作的结果，说：好吧，你的模型在信任方面有困难，或者你的模型在个性方面有困难。这就是你需要关注的地方，以真正构建一个对现实世界中的真实用户有好处的模型。

### (12:00 - 16:00) 人口统计学采样与发现
> 展示代表性采样方法和模型在个性化方面的不足

但没有质量保证，我可以进入并说你好，或者我可以什么都不说，或者我可以进行多轮对话，完全从太阳有多大漫游到蛇有多长，你知道，这种话题漫游，我认为这不是模型的真正细致入微的观点。所以我们在结构中内置了参与者进来并与模型进行多步对话的功能，我们内置了质量保证，实际上说，你知道，如果你在问题上投入很少的努力或开始漫游，我们会惩罚你，三次这样的行为你就出局了。所以，我想这些是我们围绕排行榜建立的原则。

Nora Petrova：我只想谈谈我们使用的方法论，即 TrueSkill。这是一个框架，如果你愿意的话，由 Microsoft 开发，用于估算 Xbox Live 上玩家的技能水平。因此他们考虑了游戏中的随机性、技能水平随时间的变化、某人是否有侥幸连胜还是一个持续表现良好的老练玩家等因素。

所以所有这些我们认为应该考虑的因素。这是一个非常灵活的系统，使用基于分布的概率估计，具有均值和方差，随着系统了解这些对战或比较的结果，方差会越来越窄。最重要的是，它基于信息增益。所以我们选择锦标赛中应该发生的下一对的方式是基于我们从这些模型的正面交锋中能学到多少。它们给我们多少信息？它们减少了多少不确定性？我们根据这个对队列进行排序，这让我们尽可能快地达到最小化不确定性的地方。

这是一种非常灵活的方法。我们可以运行单独的锦标赛，就像我们对人口统计组所做的那样。我们有大约 20 个人口统计组，我们为他们运行了单独的锦标赛，我们可以整合每个锦标赛的发现，获得一个整体排行榜，其不确定性比我们可以从任何人口统计组产生的任何单个锦标赛或排行榜要少得多。

所以它真的让我们能够以任何我们想要的方式切割和分析数据，我们可以随时轻松添加更多人口统计组、更多模型。我们正在公开开发，欢迎反馈。

### (15:00 - End) Part 2

最近我们观察到模型表现出更多讨好型或迎合用户的行为，而人们普遍不喜欢这种表现。这个实验的结果以及后续数据集将帮助我们回答一个问题：讨好型行为的征象与人格评估中的负面投票之间是否存在关联？这种行为是否会影响人们对模型偏好的决策？

我们可以对数据进行各种后处理和分析，识别数据集中观察到的讨好行为水平，并尝试发现用户反馈与更多模型驱动的"LLM 作为评判者"导向的对话分析之间的有趣关系，以及对各种模式和模型表现的分类。


---

*生成时间: 2025-12-21 23:35:02*
*由 YouTube Monitor & Translator (Claude CLI) 生成*