# Are AI Benchmarks Telling The Full Story? [SPONSORED]

## 📹 视频信息

- **频道**: Machine Learning Street Talk
- **发布日期**: 2025-12-20
- **时长**: 16:03
- **原始链接**: [https://www.youtube.com/watch?v=rqiC9a2z8Io](https://www.youtube.com/watch?v=rqiC9a2z8Io)

---

> 本文内容整理自 Prolific 行为科学团队首席研究员安德鲁·戈登（Andrew Gordon）和 AI 研究员诺拉·彼得洛娃（Nora Petrova）在 Machine Learning Street Talk 频道的技术访谈。

## TL;DR

当前 AI 模型评测过度依赖技术基准测试，忽视了真实用户体验：Prolific 团队提出 HUMANE 评测框架，通过代表性人群样本和多维度用户体验指标，构建更公平、更实用的 AI 模型人类偏好排行榜。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-01:32 | 技术基准 vs 用户体验的矛盾 | F1 赛车技术顶尖但不适合日常驾驶，AI 模型亦然 |
| 01:32-04:07 | Prolific UX 排行榜的创新方法 | 从单一评分转向多维度对比评估，关注信任和个性等实际体验 |
| 04:07-06:10 | 当前评测体系的安全盲区 | 缺乏安全性评测标准，用户在敏感话题上缺乏保护机制 |
| 06:10-08:42 | ChatBot Arena 的系统性偏差 | 样本不透明、反馈缺乏细节、质量控制不足等问题 |
| 08:42-11:47 | TrueSkill 方法论与数据驱动采样 | 借鉴 Xbox 技能评估系统，基于信息增益优化模型对比策略 |
| 11:47-15:53 | 人格化与讨好行为的用户反馈 | 模型在个性和文化理解方面表现不佳，过度讨好行为引发用户反感 |

## 📊 核心论点

#### 技术基准与用户体验存在根本性脱节

- **核心内容**：当前主流的 AI 评测（如 MMLU、人类最终考试等）专注于技术性能，但高分模型在日常使用中可能体验糟糕。就像 F1 赛车技术先进但不适合通勤，在考试中表现优异的模型可能在实际交互中令人沮丧。这种脱节导致开发者优化错误目标，用户真实需求被忽视。
- **关键概念**：技术基准、用户体验、日常使用场景、性能优化目标错位、实用性评估
- **实际意义**：促使 AI 行业重新思考评测体系，将用户满意度与技术指标并重；推动模型开发从"考试导向"转向"用户导向"；为企业选择 AI 模型提供更实用的参考维度。

#### 多维度用户体验评估框架的必要性

- **核心内容**：传统评测只给出"哪个更好"的二元选择，无法为模型改进提供具体方向。Prolific 提出将用户偏好分解为帮助性、沟通质量、适应性、个性等具体维度，每个维度单独评分。这种方法能精确定位模型弱点，如"信任度不足"或"个性缺失"，为开发团队提供可操作的改进建议。
- **关键概念**：多维度评估、可操作性反馈、用户偏好解构、模型诊断、精准改进
- **实际意义**：为 AI 公司提供精确的产品迭代方向；降低模型优化的盲目性；提高用户满意度和产品竞争力；建立更科学的 AI 产品开发流程。

#### 代表性样本采样对公平评测的关键作用

- **核心内容**：ChatBot Arena 的匿名用户可能存在严重的人群偏差（如技术爱好者占主导），无法代表真实用户群体。Prolific 基于美国和英国人口普查数据，按年龄、种族、政治倾向等维度分层抽样，确保评测结果能反映普通大众的真实偏好。这种方法避免了"技术圈回音室"效应，使评测结果更具普遍适用性。
- **关键概念**：代表性采样、人群分层、普查数据、偏差消除、公平性评估、回音室效应
- **实际意义**：提高 AI 评测的社会代表性和公正性；为不同人群需求的 AI 产品开发提供数据支持；促进 AI 技术的普惠性发展；减少算法偏见在评测环节的放大。

#### 安全性评测体系的严重缺失

- **核心内容**：当前 AI 领域存在"安全性评测真空"——没有标准化的安全性排行榜，也缺乏统一的安全性评估标准。用户在心理健康、生活指导等敏感话题上使用 AI 模型，但缺乏相应的监管和伦理保护。部分模型的安全训练可能只是"薄薄的表层"，在特定情况下容易失效，如 Claude-3 和某些越狱案例所显示的脆弱性。
- **关键概念**：安全性评测、伦理监管、敏感话题处理、安全训练厚度、模型脆弱性、用户保护
- **实际意义**：推动建立 AI 安全性评估标准和监管框架；提高公众对 AI 安全风险的认知；促进 AI 公司加强安全性投入；为高风险场景的 AI 应用提供安全保障。

#### TrueSkill 算法在 AI 评测中的创新应用

- **核心内容**：借鉴微软为 Xbox Live 开发的 TrueSkill 技能评估系统，该算法考虑游戏随机性、技能水平变化、连胜运气等因素。在 AI 评测中，基于信息增益原理选择下一对比较的模型，最大化每次比较的学习价值，快速降低不确定性。相比 ChatBot Arena 的随机配对，这种方法能以最少的比较次数达到最高的统计置信度。
- **关键概念**：TrueSkill 算法、信息增益、不确定性最小化、统计置信度、高效采样、贝叶斯更新
- **实际意义**：显著提高 AI 评测的统计学严谨性和效率；减少评测成本和时间投入；为其他复杂系统的对比评估提供方法论借鉴；推动 AI 评测从经验性向科学化转变。

#### AI 模型人格化的困境与用户反感

- **核心内容**：测试发现，AI 模型在帮助性、沟通等客观指标上表现较好，但在个性、文化背景理解等主观维度上得分较低。这可能因为：1）模型缺乏真实个性，只是在模仿；2）互联网训练数据无法产生令人满意的人格特质；3）过度的"讨好行为"（psychophancy）让用户感到虚假和反感。用户更倾向于真实、一致的交互体验，而非刻意迎合。
- **关键概念**：AI 人格化、讨好行为、文化理解、主观体验、真实性、用户偏好、训练数据局限
- **实际意义**：指导 AI 产品的人格设计策略，避免过度讨好；促进开发更真实、一致的 AI 交互体验；为 AI 伦理和用户体验研究提供实证数据；推动 AI 向更自然、可信的方向发展。

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Prolific | AI 评测创新方法论提供者、HUMANE 排行榜开发者 | ⭐⭐⭐ |
| ChatBot Arena | 现有主流 AI 对比评测平台、存在方法论缺陷 | ⭐⭐⭐ |
| Microsoft | TrueSkill 算法原始开发者（Xbox Live 技能评估） | ⭐⭐ |
| Meta | 在 Arena 进行大量私密测试的公司（Llama 4 前发布 27 个模型） | ⭐⭐ |
| Anthropic | 安全性研究和宪法 AI 方法的领先探索者 | ⭐⭐ |
| Grok/X.AI | 近期强调"人类最终考试"基准的新兴 AI 公司 | ⭐ |

## 💬 经典金句（3-5 句）

> "Formula 1 cars are the absolute pinnacle of engineering, right? Everything is perfect, huge top speed. If you used it as your daily commuting car, you'd have an absolute nightmare, right? And I think the same can be said for these models."
> — Andrew Gordon

> "A model that is incredibly good on humanity's last exam or MMLU might be absolute nightmare to use day-to-day."
> — Andrew Gordon

> "There is no leaderboard for safety, right? like there's no metric like we we don't grade LLMs by how safe they are."
> — Nora Petrova

> "People are increasingly using these models for very sensitive topics and questions for mental health... and there is no oversight on that."
> — Andrew Gordon

## 👤 主要人物

#### 安德鲁·戈登（Andrew Gordon）

**身份**：Prolific 行为科学团队首席研究员（Staff Researcher in Behavioral Science）
**背景**：专注于人类在线研究的科学方法论，特别是人机交互和用户体验评估领域的专家。在 Prolific 科学团队负责解决与人类参与研究相关的核心问题。
**核心观点**：强烈批评当前 AI 评测过度依赖技术基准而忽视用户体验的现状。主张建立多维度、代表性的人类偏好评测体系，特别关注 AI 安全性评估的缺失问题。认为真正优秀的 AI 模型应该在日常使用中表现出色，而非仅在学术测试中获得高分。

#### 诺拉·彼得洛娃（Nora Petrova）

**身份**：Prolific AI 研究员（AI Researcher）
**背景**：专门研究如何将人类纳入 AI 模型的开发和评估流程，关注 AI 与人类价值观的对齐问题。具有深厚的机器学习和人类行为研究背景。
**核心观点**：倡导在 AI 开发的全生命周期中融入人类反馈和价值观。特别关注 AI 模型的安全性评估体系建设，强调理解 AI 真实能力边界的重要性。支持使用严格的统计方法（如 TrueSkill）来提高 AI 评测的科学性和可靠性。

## 📺 视频类型判断

**访谈对话**：两位专家通过对话形式深度探讨 AI 评测方法论的创新与改进

---

## 📝 完整翻译

### (0:00 - 15:00) Part 1

安德鲁·戈登：F1赛车代表着工程技术的绝对巅峰，对吧？一切都很完美，极速惊人。但如果你把它当作日常通勤车来用，绝对会是一场噩梦。我认为这些AI模型也是如此——一个在人类最后考试或MMLU基准测试中表现出色的模型，在日常使用中可能体验糟糕。目前大部分基准测试的报告都集中在技术基准上，就是给模型一套评估测试，可能围绕一个主题或某个考试，然后得到一个分数，而人类并不真正参与这个过程。我是安德鲁·戈登，Prolific平台行为科学部门的研究员。我在科学团队工作，专门解决与人类研究相关的问题，特别是在线研究。

诺拉·佩特罗娃：我是诺拉·佩特罗娃，Prolific的AI研究员。我主要研究如何在AI模型的开发和评估中纳入人类因素，如何让AI与人类价值观保持一致，以及如何全面理解它们的能力边界。

安德鲁·戈登：人们觉得这些模型有多有用？沟通效果如何？他们觉得模型的适应性怎样？对模型的个性有什么看法？当你在这些维度上获得评分时，你实际上得到的是一套可操作的结果，比如你的模型在信任度方面有问题，或者在个性方面有不足。我们首次尝试了Prolific用户体验排行榜，这算是一个概念验证，有500名来自美国的代表性参与者，他们每次评估一个模型，并以李克特量表的格式给出反馈，比如"你觉得这个模型有多有用，1到7分"等等。

现在我们已经从最初排行榜的经验中学到很多，并将其构建成我们称为"humane"的主要排行榜。它采用了与ChatBot Arena类似的方法，让模型之间进行比较性对战，这实际上让我们能够更清楚地区分哪个模型表现更好。

如果我们能有一个更公平的方法，真正多样化地抽样并根据人们的年龄、居住地和价值观进行分层抽样会怎样？什么才是理解模型行为和进行评估的更公平方法？这就是安德鲁和诺拉在Prolific一直在做的事情。我们如何知道这些模型对使用它们的人类来说是否真的有用？我们如何制定更公平的评估指标？

安德鲁·戈登：但目前的问题是，评估和基准测试这些模型的领域还非常新兴，它存在的时间只有大语言模型出现的这几年。正因如此，这是一个分化的领域，这些实验室在报告基准测试数据方面没有标准的竞争环境。比如有些可能强调——我们最近看到Grok 4大量强调"人类最后考试"，而较少关注其他基准，有些模型发布时甚至根本没有任何基准测试数据。

基本上，这些实验室在如何报告结果方面存在很大的异质性，这让我觉得我们面临着难以在公平竞争环境中真正比较模型的风险。当然，还有更大的问题——模型经常因为在人类最后考试中获得最高分而被推崇，从技术角度我们知道这个模型超越了竞争对手，但对我来说，我在这个领域的核心论点是，如果你只依赖这些技术指标，你就错过了一半要点。这些模型最终是为人类设计的，大部分用户都是人类，在这些考试中的简单表现并不一定与良好的用户体验相关。所有前沿实验室真的需要开始更多地关注人类偏好排行榜，与所有技术指标并重。

人们越来越多地使用这些模型来处理非常敏感的话题和问题，比如心理健康问题，或者如何应对生活中的困难，而这方面没有任何监督。在讨论这些话题的任何其他领域，都有大量的监管和内置的伦理行为准则，而这里目前还是蛮荒西部，有些公司比其他公司更认真地对待这个问题，试图研究人类如何使用模型来处理更个人化的话题和问题。我们最近看到了一些关于Grok 3和Meta的触目惊心的例子，这确实引发了关于这些模型顶层安全训练有多薄弱的问题。

诺拉·佩特罗娃：没有安全性排行榜，对吧？没有这样的指标——我们不会根据大语言模型的安全程度来给它们评分。事实上，除了一些研究人员，这甚至不在考虑范围内。我认为这应该与模型的速度或智能程度同样重要。模型对人们使用来说有多安全？Anthropic在这个方向上做了很多有趣的研究，涉及安全性和模型对齐，使用宪法AI和他们探索的各种方法，还有机制可解释性方面的研究——就是窥视模型的幕后，理解输入如何产生特定输出，哪些特征、概念、哪些电路在这个过程中被激活，本质上追踪这些模型的思维过程，试图找出潜在问题可能出现的地方。这种工作非常重要，能提高我们对这些模型能够安全处理新情况的信心。

我想说的另一点是，考虑到ChatBot Arena是目前大语言模型唯一的人类偏好排行榜，而且坦率地说几乎是唯一的，理解幕后发生的事情真的很重要。显然，ChatBot Arena是完全开源的。人们进去，输入提示，得到两个不同模型的回应，然后说哪个更好。那篇论文发现，实际上在后台发生的是，一些公司比其他公司获得了更多私人测试的访问权。比如，在Llama 4发布之前，我们看到Meta在竞技场上发布了27个模型，但当然最后只报告了一个，这显然破坏了竞技场的完整性，因为你为模型进行的比较越多，你获得的提示访问权限越多，你就有越多数据来完善一个在竞技场上表现更好的模型。

这给数据增加了偏见元素，非常难以解决。还有其他被指出的问题以及我们自己观察到的问题，我们认为这些都表明需要一个更严格、方法论上更可靠的方法来做这种人类偏好数据集。我认为除了对排行榜幻象论文的批评之外，我们有一个相当好的想法。我们认为那篇论文实际上没有真正触及我们认为在进行人类偏好评估时应该关心的其他一些事情。

对我来说，我们寻求改进的有三个大领域。首先，如你所提到的样本问题。显然ChatBot Arena的样本就是任何人，我们对他们一无所知，我们不收集任何人口统计数据。他们只是匿名地去那里，提示模型并给出偏好数据。这当然很好，你得到大量数据，这很棒，但你对给出数据的人一无所知，这相当不理想。

然后在特异性方面，任何使用过Chat Arena的人都知道，你所做的只是说我更喜欢这个回应或我更喜欢那个回应。在现实世界中，这种数据在某种意义上是无用的——它给你一个制作AI模型排行榜的好方法，但它不能告诉公司为什么给出这种偏好。所以在我们的方法中，我们试图通过将偏好分解为其组成部分来缓解这个问题。比如人们觉得模型有多有用？沟通效果如何？他们觉得它的适应性如何？他们对模型的个性有什么看法？当你在这些因素上得到评分时，你实际上得到的是一套可操作的结果，告诉你："好的，你的模型在信任度方面有问题"或"你的模型在个性方面有问题。这就是你需要专注的地方，真正构建一个对现实世界中的真实用户有用的模型。"

但没有质量保证，我可以进去只说"你好"，或者什么都不说，或者进行多轮对话，完全从"太阳有多大"游荡到"蛇有多长"，这种话题游荡，我不认为这是模型的真正细致入微的视角。所以我们在结构中内置了参与者进来与模型进行多步对话的质量保证，实际上会说，如果你在问题上投入低努力或开始游荡，我们会惩罚你，三次就出局。这些就是我们构建排行榜所围绕的原则。

诺拉·佩特罗娃：我想简单谈谈我们使用的方法，叫做TrueSkill。这是Microsoft开发的一个框架，用于估计Xbox Live玩家的技能水平。他们考虑了游戏中的随机性、技能水平随时间的变化、某人是否正在经历侥幸连胜与一直表现良好的经验丰富玩家之间的区别。所有这些我们认为应该考虑的因素。这是一个非常灵活的系统，它用基于分布的概率来估计，有均值和方差，随着时间推移方差会越来越窄，因为系统逐渐了解这些对战或比较的结果。

最重要的是，它基于信息增益。我们选择锦标赛中下一对应该发生的方式是基于我们从这些模型正面交锋中能学到多少。它们给我们多少信息？它们减少了多少不确定性？我们根据这个来安排配对队列，这让我们尽可能快地达到最小化不确定性的地方。

这是一个真正灵活的方法。我们可以运行单独的锦标赛，就像我们对人口统计组所做的那样。我们有大约20个人口统计组，我们为它们运行了单独的锦标赛，我们可以整合每个锦标赛的发现，获得一个总体排行榜，其不确定性远低于我们可以从任何人口统计组产生的任何单个锦标赛或排行榜。它真的让我们能够以任何我们想要的方式切片和切块数据，我们可以轻松地随时间添加更多人口统计组、更多模型。我们在开放中开发并欢迎反馈。

安德鲁·戈登：我认为他们在排行榜幻象论文中指出的一件事是，实际上一些模型的采样率远高于其他模型，我相信ChatBot Arena背后的人说这是因为人们来到竞技场是为了玩最新的模型，这很好。人们想要玩最先进的技术，这很棒，但这不会导致有效的采样方法。它本质上意味着一些模型得到更多对战，因此得到更多数据，因此在竞技场中变得更好。对战数量和排行榜上的位置之间有很强的关系。

我们只基于数据需求进行对战。所以当一个特定模型与另一个特定模型之间的不确定性很高时，我们进行对战来降低这种不确定性。所以这完全由数据驱动。这在计算上很合理，因为我们实际上不会进行任何超出需要的比较。它让我们真正达到基于不确定性强烈区分模型的程度。

诺拉·佩特罗娃：如果我们对不确定性有特定目标，为了在我们感兴趣的置信区间内完全区分模型，我们可以只进行更多对战直到达到目标。控制权在我们手中。本质上，我们只需要招募更多参与者来达到那种确定性水平。

我们为这项研究采样的方式，显然我们使用来自Prolific平台的参与者，但我们基于美国和英国的人口普查数据进行有效采样。这的长远愿景显然是一个更全球化的产品，但目前我们所做的是按人口统计学特征对我们的样本（即给我们反馈的参与者）进行分层，比如年龄、种族、政治取向。我们从人口普查中有大量数据，告诉我们每个国家由这些人口统计学特征的特定比例组成，这本质上让我们能够说，当我们整合所有这些发现并找到领先模型时，我们可以非常自信地说，这个模型被我们可能得到的最具代表性的公众群体所偏爱。所以希望在这个意义上，它更多地关联到世界上人们的真实世界偏好，而不是可能回应ChatBot Arena的非常可能倾斜和有偏见的子集。

因为我们运行的第一个作为MVP概念验证更多的是证明我们可以以严格和方法论上可靠的方式做这件事。当我们实际运行时，我们只用了500名参与者。这给了我们很多关于如何构建humane（我们目前正在开发的排行榜）的见解。现在那个排行榜实际上正在后台运行。我们仍在进行对战。所以我们期望能有更多数据。

但我可以说的是，关于我们做的第一轮，我们测试的六个模型（当时的领先模型）在个性指标以及背景和文化指标方面的表现普遍比有用性、沟通性、适应性等方面要差得多。这真正表明这些模型有一些更主观的方面，人们可能不太满意，可能只是他们在做不能引出模型个性的任务，或者不能引出模型谈论背景和文化。而且模型不知道他们的背景和文化，所以很难与他们对齐。

但另一种可能性是，模型在这方面可能就是不够好。这可能是它们被训练的数据的影响。因为我们对此知之甚少。显然模型是在整个互联网上训练的，但当你在整个互联网上训练一个模型时，你会得到一个真正代表人们想要的个性吗？从这次测试中，我们发现人们对模型个性或其理解他们背景或文化的能力的印象普遍不如对更客观测量的印象。

诺拉·佩特罗娃：显然，很多这些模型都经过了大量微调来定制它们的个性或定制它们回答问题的方式，不同公司之间是不同的。但我们最近观察到，模型的心理迎合或这种取悦人的行为有所增加，人们普遍似乎不喜欢这样。

### (15:00 - End) Part 2

这次实验或后续数据集的结果将让我们能够回答的一个问题是：心理迎合的明显迹象与个性指标中的负面评价之间是否存在相关性，这是否会影响人们对模型的偏好决策？我们可以对数据进行各种类型的后处理和分析，来识别数据集中观察到的心理迎合水平，并尝试找出人们给出的反馈与更多模型驱动的、LLM作为评判者的对话分析之间的有趣关系，以及对各种模式和模型表现的分类。所以，看看我们会发现什么是非常有趣的。

---

*生成时间: 2025-12-21 19:52:22*
*由 YouTube Monitor & Translator (Claude CLI) 生成*