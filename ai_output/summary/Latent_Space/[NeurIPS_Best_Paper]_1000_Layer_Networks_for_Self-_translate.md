# [NeurIPS Best Paper] 1000 Layer Networks for Self-Supervised RL — Kevin Wang et al, Princeton

## 📹 视频信息

- **频道**: Latent Space
- **发布日期**: 2025-12-31
- **时长**: 28:16
- **原始链接**: [https://www.youtube.com/watch?v=25FsKN0f8gQ](https://www.youtube.com/watch?v=25FsKN0f8gQ)

---

> 本文内容整理自普林斯顿大学研究生凯文·王（Kevin Wang）、伊安·布鲁斯（Ean Bruce）、妮可·米哈尔（Nicole Mihal）等在 Latent Space 播客的深度访谈。该论文获得了 NeurIPS 2024 最佳论文奖。

---

## TL;DR

突破性研究：通过自监督学习目标而非传统奖励函数，成功训练出1000层深度强化学习网络，挑战了"RL 无法扩展"的传统认知，为强化学习开辟了类似语言模型和计算机视觉的规模化发展道路。

---

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-02:00 | 获奖背景与团队介绍 | NeurIPS 最佳论文团队分享获奖感受和研究起源 |
| 02:00-05:33 | 研究动机与核心方法 | 对比 NLP/CV 的成功，探索 RL 的规模化可能性 |
| 05:33-08:38 | 技术突破与架构设计 | 结合残差连接等技术实现深度网络训练 |
| 08:38-11:44 | 自监督 RL 的本质洞察 | 从奖励最大化转向轨迹分类的目标重设计 |
| 11:44-16:20 | 应用前景与效率分析 | 机器人学应用潜力和计算资源权衡 |
| 16:20-22:27 | 技术细节与扩展维度 | Jax 环境加速和多维度扩展可能性 |
| 22:27-28:03 | 未来研究方向讨论 | 蒸馏部署、视觉语言行动模型等前沿探索 |

---

## 📊 核心论点

#### 强化学习的扩展悖论：为什么 RL 停留在浅层网络？

- **核心内容**：语言模型和计算机视觉已经成功扩展到数千亿参数规模，但强化学习仍然使用 2-4 层的浅层 MLP 网络。这种差异来源于传统 RL 的价值函数学习方法（如 Q-learning、TD error 回归）本质上是嘈杂和有偏的，难以支持深度网络训练。而语言和视觉领域的成功依赖于可扩展的分类目标和表征学习。
- **关键概念**：价值函数学习、TD 误差、网络深度扩展、参数规模、传统 RL 限制
- **实际意义**：揭示了强化学习发展瓶颈的根本原因，为突破现有框架提供了理论基础，指明了 RL 规模化的必要条件。

#### 自监督强化学习：从奖励优化到轨迹分类的范式转换

- **核心内容**：传统 RL 直接优化奖励信号，而自监督 RL 学习状态、动作和未来状态的表征，将同一轨迹上的表征拉近，不同轨迹的表征推远。这本质上是一个二元分类问题：判断某个未来状态是否属于当前轨迹。这种目标设计类似于语言模型的下一词预测，但应用于状态空间，避免了传统 RL 的噪声和偏差问题。
- **关键概念**：自监督学习、表征学习、轨迹分类、对比学习、目标条件 RL
- **实际意义**：为强化学习提供了全新的可扩展框架，消除了对人工奖励信号的依赖，使 RL 能够像其他深度学习领域一样受益于规模化。

#### 架构突破的关键组合：残差连接 + 深度网络的协同效应

- **核心内容**：单纯增加网络深度或仅添加残差连接都无法带来性能提升，只有将两者结合并配合特定的架构选择才能实现突破。在特定环境中，网络深度加倍不起作用，但再次加倍并结合残差连接后性能急剧提升。这种"临界深度"现象类似于相变，需要多个技术要素同时满足才能解锁。
- **关键概念**：残差连接、梯度消失、临界深度、架构设计、超参数优化
- **实际意义**：为深度 RL 网络设计提供了具体的工程指导，强调了系统性方法的重要性，避免了研究者的盲目尝试。

#### 深度 vs 宽度：参数效率的数学权衡

- **核心内容**：增加网络深度时参数数量线性增长，而增加宽度时参数数量近似平方增长。在相同参数预算下，深度扩展比宽度扩展带来更显著的性能提升。基准架构（256 隐藏单元，4 层）扩展时，深度曲线急剧上升，宽度曲线增长较缓慢。这为资源受限场景下的模型设计提供了明确指导。
- **关键概念**：参数效率、线性扩展、平方增长、计算复杂度、资源约束
- **实际意义**：帮助研究者和工程师在有限计算资源下做出最优的架构选择，指导实际部署中的成本效益分析。

#### GPU 并行环境：数据收集瓶颈的技术解决方案

- **核心内容**：使用 Jax-based GPU 加速环境同时收集上千条轨迹，在几小时内获得数亿时间步数据。只有跨越 5000 万转换步的数据规模门槛，深度网络的性能优势才会显现。这解决了传统 RL 的数据稀缺问题，使深度学习的数据驱动优势得以发挥，类似于语言模型利用互联网级数据规模。
- **关键概念**：并行环境模拟、GPU 加速、数据密集型学习、Jax 框架、规模化数据收集
- **实际意义**：为研究机构提供了可行的大规模实验平台，降低了深度 RL 研究的技术门槛，加速了算法验证和迭代。

#### 机器人学的无监督学习范式：从模仿到自主探索

- **核心内容**：当前机器人学主要依赖模仿学习，需要大量人工监督和演示数据，扩展性受限。目标条件强化学习提供了替代路径：机器人可以在完全无人工监督下学会解决有意义的任务。通过自监督目标和可扩展架构，机器人能够通过自主探索获得复杂技能，无需人工标注或演示。
- **关键概念**：模仿学习、目标条件 RL、无监督学习、机器人自主性、人工监督依赖
- **实际意义**：为机器人学开辟了新的技术路径，减少了对昂贵人工数据的依赖，提高了机器人系统的可扩展性和部署灵活性。

#### 多维扩展的协同效应：深度解锁批量大小的潜力

- **核心内容**：传统价值 RL 中扩展批量大小效果不佳，但深度网络成功训练后发现批量大小扩展变得有效。这验证了"只有足够大的网络容量才能充分利用大批量大小"的假设。深度、宽度、批量大小三个维度相互促进，类似于语言模型的多轴扩展策略，为 RL 开辟了全新的扩展空间。
- **关键概念**：多维扩展、批量大小扩展、网络容量、协同效应、扩展定律
- **实际意义**：为未来的大规模 RL 系统设计提供了框架，指导研究者如何系统性地推进算法性能边界。

#### 深度教师-浅层学生：部署效率的解决方案

- **核心内容**：训练阶段使用深度网络达到最佳性能，推理阶段通过知识蒸馏压缩到浅层网络，平衡能力和效率。这种范式分离了训练时的能力追求和部署时的效率需求，类似于其他深度学习领域的成功模式。研究团队将此列为重要的未来研究方向。
- **关键概念**：知识蒸馏、模型压缩、训练推理分离、部署优化、性能效率权衡
- **实际意义**：为深度 RL 模型的实际应用提供了可行路径，解决了深度模型在资源受限环境下的部署难题。

---

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Princeton University | 研究团队所在机构，项目发源地 | ⭐⭐⭐ |
| Google (Jax) | GPU 加速环境基础设施提供方 | ⭐⭐ |
| NVIDIA (H100) | 实验硬件平台，单 GPU 运行千层网络 | ⭐⭐ |
| PyTorch vs Jax | 深度学习框架选择，RL 领域趋势对比 | ⭐ |
| General Intuition | 视觉语言行动模型相关讨论 | ⭐ |

---

## 💬 经典金句

> "为什么我们停止让它们变得更深？强化学习是这个异常，我们继续使用这些非常浅的网络。"
> — Ben (指导教授)

> "我们从根本上将学习的负担从 Q 学习或回归到 TD 误差转移到了分类问题。"
> — Kevin Wang

> "这不仅仅是大网络，而是使用大网络需要架构技巧，也需要使用不同的目标。"
> — Ben (指导教授)

> "我们的代码中没有一行代码说'在这里最大化奖励'。"
> — Ben (指导教授)

> "也许构建智能系统的关键将是来自所有这些领域的杠杆洞察。"
> — Kevin Wang

---

## 👤 主要人物

#### 凯文·王（Kevin Wang）

**身份**：普林斯顿大学本科毕业生（项目负责人）
**背景**：机器学习研究新手，通过独立研究课程开始 ML 研究，现已进入全职工作
**核心观点**：强化学习应该像语言模型和计算机视觉一样实现规模化发展，通过自监督学习目标而非传统奖励函数训练深度网络。他强调从奖励回归转向轨迹分类的范式转换，并对机器人学中的无监督学习应用充满期待。

#### 本·艾森巴赫（Ben Eisenbach）

**身份**：普林斯顿大学教授，深度强化学习实验室负责人
**背景**：深度 RL 领域资深研究者，曾对深度网络在 RL 中的应用持怀疑态度
**核心观点**：本研究的关键不是简单的"大网络更好"，而是发现了架构技巧与新目标函数的必要结合。他强调这种方法模糊了强化学习与自监督学习的边界，可能为构建智能系统提供新的路径。

#### 伊安·布鲁斯（Ean Bruce）

**身份**：普林斯顿大学研究生
**背景**：参与独立研究课程，专注于 RL 中的拼接（stitching）研究方向
**核心观点**：强调了架构创新与目标函数创新的结合重要性，指出虽然使用了对比学习等已有技术，但关键在于架构与目标的融合使扩展成为可能。

#### 妮可·米哈尔（Nicole Mihal）

**身份**：普林斯顿大学研究生
**背景**：参与项目开发，专注于视觉语言行动模型研究
**核心观点**：积极探索表征学习在机器人学中的应用，关注从游戏剪辑到具身智能的技术迁移，研究视觉语言行动模型的实际部署可能性。

---

## 📺 视频类型判断

**访谈对话**：多人学术讨论、技术问答形式、会议播客

---

## 📝 完整翻译

### (0:00 - 2:00) 获奖背景与团队介绍

欢迎来到 Latent Space 播客。我们的目标是为不在现场的观众提供最佳的神经网络会议体验。首先恭喜你们的论文获奖，感觉如何？

非常激动。昨天我们有海报展示，今天还有口头报告。

现场人很多对吧？确实，连续三个小时不断有人过来，我们一直在努力应对。

我从来没有获得过最佳论文奖，你们是怎么发现的？是在网站上看到的吗？

我就是某天醒来查邮件，然后就看到了。

他们就直接告诉你们？

就是一封邮件说"恭喜你获得了最佳论文奖"。

不过你们从评审意见中应该也能感觉到表现不错对吧？是的，我们从评审中知道表现很好。但评审表现好和获得最佳论文奖还是不同的，所以获奖这部分我们确实不知道。

好的，我跳过了一些内容。也许我们可以逐一介绍一下，你们都是谁，在团队中做了什么？

我是 Kevin，刚从普林斯顿大学本科毕业。我主导了这个项目，很高兴能与 Isan、Nicole 和 Ben 合作。

你们是在同一个研究组吗？你们的合作背景是什么？

我们都来自普林斯顿大学。感谢 Ellen 为我们安排这次访谈。

这个项目实际上起源于一个独立工作研讨班，Ben 在教授的研究研讨班。这是我在机器学习研究方面的第一次经历，非常宝贵。Isan 也在那个研讨班，在做相关的工作，所以我们在研讨班期间进行了大量合作。项目取得了一些很酷的结果，后来 Nicole 也在做类似的工作，加入了项目，成为了很好的合作。

### (2:00 - 5:33) 研究动机与核心方法

Nicole 也在做类似的工作，加入了项目，成为了很好的合作。

我想问问你们其他人，还有什么其他因素促使你们决定做这个问题？Ben 的实验室主要做深度强化学习，但在历史上，"深度"意味着两层、三层或四层网络...

不是一万层！（笑声）

当 Kevin 和 Sean 提到他们想尝试真正的深度网络时，我对此能否成功是持怀疑态度的。我之前试过这个方法，没有成功，其他人也试过，也没有成功。所以我开始时非常非常怀疑。虽然我当时没有明说，但这确实是我的先验判断。

但你认为你的工作是筛选想法，比如"嘿，伙计们，这可能行不通，你们应该尝试不同的想法"，还是说即使看起来很愚蠢，你也应该鼓励他们？

这是在选择赌注。

没错。

这是我愿意下注的一个赌注。

什么让你愿意下这个赌注？看起来成本相对较低，特别是 Mihal 花了过去一年时间开发基础设施，让运行这些实验变得容易得多。而且先例表明，更深的网络可以做得更好——这正是过去十年深度学习革命的核心。

是的，我想知道为什么我们要停止让网络变得更深？（笑声）

强化学习就像这个异常情况，我们继续使用这些非常浅的网络，在我们研究的环境中尤其如此，因为你是从零开始，经常是从头开始。

你们还有其他想法想补充吗？

我想也许我应该先概述一下我们的项目。

好的，抱歉，请继续。

我对我们项目的理解是，如果你看深度学习的全景，你有 NLP（自然语言处理）、视觉，然后是强化学习。正如 Ben 提到的，在语言和视觉领域，我们已经收敛到这些大规模网络扩展的范式——数千亿参数，万亿参数。深度学习从中获得了很多收益。

但是在深度学习的第三个分支——深度强化学习中，情况似乎并非如此。当我参加 Ben 的课程和研讨班时，我很惊讶地发现，为什么你们只是在这些前沿的、最先进的强化学习算法中使用简单的两层 MLP？

所以我很好奇，我们能否设计强化学习算法？能否组合出一个强化学习的配方，让它能够以类似于语言和视觉可能扩展的方式进行扩展？我们所做的是，我们知道传统的强化学习，比如基于价值的强化学习，实际上并不能很好地扩展——从文献中可以清楚地看出这一点。所以我们尝试了一种不同的强化学习方法，叫做自监督强化学习。我们不是学习价值函数，而是学习状态、动作和未来状态的表示，使得同一轨迹上的表示被推到一起，长期策略的表示被推开。这只是一种不同的强化学习方法，允许我们以自监督的方式学习，因此我们可以在没有任何人工制作的奖励信号的情况下解决任务和达到目标。

我们知道自监督学习在深度学习的这些不同领域是可扩展的，那么自监督强化学习能否以类似的方式扩展呢？当我们第一次尝试时，实际上没有成功——我们让网络变得更深，但性能完全下降了。但后来我单独研究了一些其他工作，我们尝试了残差连接，还有一些其他架构组件必须加入到配方中。然后突然有一天，我运行了这个实验，在一个环境中，简单地加倍深度没有太大效果，但再次加倍深度并加上这些不同的组件，突然在这个环境中性能飞速提升。

让这个方法成功是非常不简单的，因为通常我们在做超参数优化时，会尝试改变 A，看看是否变好，尝试改变 B，看看是否变好。如果我们只是让深度更大，结果会变糟；如果我们只是添加残差连接，也不会变好。真正让这个方法成功的是 Kevin 和 Isan 找出的这种因素组合。

作为前奏，我们也尝试了沿不同维度扩展，比如扩展批大小，扩展网络的宽度即隐藏层。这些与单纯扩展深度的效果相似。然后当我们开始引入残差连接、层归一化等这些特定的架构选择时，我们看到了性能的显著跳跃，在某些关键深度处性能会成倍增长，这时我们真正注意到解锁了一些重大的性能提升，而不是仅仅沿宽度扩展只能带来一些性能改善。

### (5:33 - 8:38) 技术突破与架构设计

在一个环境中，仅仅将深度翻倍并没有什么效果，但是当我们再次将深度翻倍并加上这些不同的组件时，这个环境中的性能突然飞速提升。让这个方法成功是非常不简单的，因为通常我们在做超参数优化时，会尝试改变 A，看看是否变好，尝试改变 B，看看是否变好。如果我们只是让深度更大，结果会变糟；如果我们只是添加残差连接，也不会变好。真正让这个方法成功的是 Kevin 和 Ean 找出的这种因素组合。

作为前奏，我们也尝试了沿不同维度扩展，比如扩展批大小，扩展网络的宽度即隐藏层。这些与单纯扩展深度的效果相似。然后当我们开始引入残差连接、层归一化等这些特定的架构选择时，我们看到了性能的显著跳跃，在某些关键深度处性能会成倍增长，这时我们真正注意到解锁了一些重大的性能提升，而不是仅仅沿宽度扩展只能带来一些性能改善。

但是当你观察网络随着宽度增长时的参数数量，它大致是二次方的增长，而不像深度增长那样。所以在某种意义上它更加参数高效，根据我们进行的实验，样本效率也更高。

在某种程度上，你们是在复制野外看到的现象，但是在一个你们可以研究的非常小的模型上。你会这么说吗？

是的。正如 Kevin 之前提到的，我们在语言模型、图像生成模型中看到了巨大的性能改进，通过让它们变得更大、更深，这看起来非常直观。

我们的工作借鉴了基础研究，比如残差网络，它采用残差连接来避免梯度消失。这是我们在论文中的一些消融研究中展示的，可能在附录的更深处。我们做了没有这些残差连接的实验。所以我们借用了这些在其他领域已经存在的概念，并将它们应用到强化学习的环境中，证明它是有效的。

在 Ben 离开之前，我会让他说最后一句话。这项工作启发了什么额外的工作，是你们接下来想要推进的？

我想澄清论文的一点，然后直接回答这个问题。我想澄清的是，我认为很多读到标题的人会想"哇，大网络很棒，我要用大网络，问题解决了。现在我们可以直接应用了。"我们说大网络，把它们加到 PPO，加到 SAC，加到你最喜欢的强化学习算法中。但我认为这实际上不是主要结论。我认为主要结论是，使用大网络不仅需要这些架构技巧，而且正如 Kevin 之前提到的，它需要使用不同的目标函数。

这个目标函数实际上不使用奖励。所以标题中的另一个词"强化学习"也可能有点用词不当，因为我们并不直接试图最大化奖励。我们的代码中没有一行代码说"在这里最大化奖励"。那么最终这是一个强化学习方法吗？我不知道。它看起来更像机器学习其他领域的自监督方法。所以我认为这个方法、这项工作真正站在强化学习和自监督学习研究的某种有趣交汇点上。

我们在海报左下角有个小图，是 Yann LeCun 谈论如何构建智能系统的幻灯片截图，以及这是否会通过无监督学习、监督学习还是强化学习来完成。我认为我们的论文真正表明的是，这些东西之间的界限非常模糊，也许构建智能系统的关键将是利用所有这些领域的洞察。

### (8:38 - 11:44) 自监督 RL 的本质洞察

这种方法与机器学习其他领域的自监督方法更加相似。所以我认为这项工作真正站在强化学习和自监督学习研究的某种有趣交汇点上。我们在海报左下角放了一个小图，是 Yann LeCun 谈论如何构建智能系统的幻灯片截图，讨论这是否应该通过无监督学习、监督学习还是强化学习来完成。我认为我们的论文真正表明的是，这些东西之间的界限非常模糊，也许构建智能系统的关键将是利用所有这些领域的洞察。

主持人：正是这种分层踢法。感谢你的时间，我知道 Jon 你很快就要离开了。

Jon：谢谢你们来到这里。我认为这种模糊界限的洞察很有趣。你刚才谈到了表示学习的抽象层。我不知道这是否会在自监督学习和强化学习的混合方面触发什么思考。这是你们发现的一些根本性的东西，还是人们在阅读论文时不理解的地方？

我认为最好的解释方式是，我们知道标准强化学习不是超级可扩展的。那么为什么这种不同的方法或不同的目标函数强化学习能够可扩展呢？我认为这是因为我们从根本上转移了学习的负担，从像 Q-learning 或回归 TD 误差这样的方法（我们知道这是相当稀疏、嘈杂和有偏的）转移到了一个分类问题。我们试图分类未来状态是沿着相同轨迹还是沿着不同轨迹。我们通过表示学习来做到这一点，我们知道分类交叉熵损失和表示学习在深度学习文献中是可扩展的，如果我们想想语言模型和其中的一些目标函数。

从某种意义上说，我们正在模糊界限。我们在做强化学习，它仍然是一个 actor-critic 强化学习算法，是一个目标条件强化学习算法，但学习的目标和负担转移到了更类似于你在语言和视觉中可能看到的目标函数，我们知道这些已经扩展了很多。所以我认为这是我们看到的一个根本洞察，通过用这种不同的方法来处理强化学习，我们能够获得更多，我们能够将网络扩展到远超强化学习标准使用的规模。

Kevin：我来补充一下架构方面的背景。虽然我们使用了另一种目标函数，即对比损失，但架构与之前的工作如 BYOL、SimCLR、MoCo 以及 SimCLR v2 相当相似。我们也对这种架构进行了一些调整。但这并不是说我们第一次发明了轮子。是架构和目标函数的结合使得扩展真正得以实现，性能随着规模的增长而提升。

### (11:44 - 16:20) 应用前景与效率分析

我认为这是我们应该深入挖掘的内容。你认为在什么领域或行业中——如果你已经在多种不同类型的网络或数据集上应用了这种方法——有什么特定的亲和性是低挂的果实？

实际上，如果你看我们的许多任务，它们特别像机器人任务。所以我个人会很好奇像这样的工作如何能够影响机器人领域。据我对机器人学的理解，现在的机器人学有几种不同的方法。一种方法是我们想要使用模仿学习来训练机器人，所以我们尝试收集大量的数据，但我们需要大量的人工监督，我们尝试扩大这些数据的规模，我们通过模仿学习来学习。

但另一方面，可能还有另一种方法，比如目标条件强化学习，我们实际上可以训练机器人智能体和强化学习智能体来解决有意义的任务，完全没有人工监督，没有演示。

这更具可扩展性。

所以这可以作为一种替代方法，也许不是像扩展数据那样扩展人工监督（这并不是超级可扩展的），如果有方法使目标条件强化学习具有可扩展性，我们可以只是扩展架构或扩展——

因为你专注于你的目标函数。

对的，使用某些不同的目标函数？我认为这可能非常令人兴奋，看看这如何能够影响像机器人学这样的领域。

让我深入探讨一下你提到的效率问题。我预期网络越深，这应该在二次方面更糟糕。我不熟悉现有的文献，我只是在梳理直觉。但是你发现的权衡是什么，你认为可能想要警告人们注意的？

因为你是提到效率的人。

当然。我指的是我们海报上的一个图表，也在我们的论文中，我们比较了当我们沿着深度轴扩展和沿着宽度轴扩展时模型拥有的参数数量。

从我们的基线架构来看，最基本的是宽度为256，隐藏层有256个神经元，深度是四个隐藏层。我在那里要说的是，当你沿着深度扩展时，你模型拥有的参数数量将大致线性增长。而对于宽度，你让你的网络输出更宽，然后下一个网络的输入也在增长。所以你网络的参数数量将大约二次方增长。

我们做的一个实验是检查当我们通过沿着这两个不同选择扩展来增长模型中的参数数量时，对于相同的大致参数数量，哪一个产生更好的性能。深度曲线是这样的——它跳得相当快。这在我们的论文中始终存在。宽度增长更慢一些。

所以从中得出的结论是，如果你在资源方面更受限制，沿着深度扩展可能更好，因为参数更少，模型更小，可学习的参数数量更少。

宽度是昂贵的。

宽度是昂贵的，完全正确。一般来说，当然，更多的参数也会更昂贵。所以这只是在使用这些网络时需要考虑的另一个因素。

有没有其他类似的经验法则可以提取？这只是我能想到的最基本的一个。

我不知道是否还有其他的。

### (16:20 - 22:27) 技术细节与扩展维度

确保我们有足够的数据来充分挖掘学习潜力。

那就像是工作数据列。

好的，我不知道你是否想在这个方向上进一步探讨。

大多数人对 PyTorch 比较熟悉，对 JAX 不太熟悉。我认为 JAX 在强化学习领域正在获得越来越多的关注，因为对于在线强化学习，获得尽可能多的数据是最重要的。

**主持人：** 对于其他人探索这种部署方式，你有什么建议吗？

**发言人：** 我可以推荐用于目标条件强化学习的 JRL，但也有其他多智能体 JAX 实现。回到我们的论文，如果你看那些图表，只有当我们超过 5000 万次转换数据时，才能看到巨大的性能提升。所以我认为数据在这里是至关重要的。

我喜欢将这与深度学习其他领域的成功经验类比，比如大语言模型。我们之能够扩展到如此大规模的网络，是因为我们找到了一种范式，可以利用整个互联网规模的数据来学习。

传统上，强化学习中的数据一直很难获得，但现在有了这些 GPU 加速环境，我们可以在几小时内收集数亿个时间步的数据。我认为这为我们寻找扩大网络容量并获得类似收益的方法提供了一个很好的测试平台。

**主持人：** 你是说在大语言模型中你会以不同方式进行预训练？现在的目标有什么不同？

非常简单，你提到的范式是下一个词或下一个 token 预测，对吧？这非常稳健。

**主持人：** 你如何改变这一点？

我不是说我想改变它，而是想利用其中的洞察应用到强化学习中。

**主持人：** 我觉得你应该走相反的路。

也许吧，那也会是一个非常有趣的研究方向。实际上，关于这一点，我思考的一件事是我们的目标函数的工作方式。它不完全是下一个词预测，但有点像下一个状态预测。你想象自己处于某个当前状态和某个当前动作，我们想预测某个未来状态是否沿着相同轨迹或不同轨迹。从某种意义上说，我们实际上在做某种隐式世界模型。

**主持人：** 就像在语言中你使用交叉熵损失来分类下一个 token，这里我们只是对某个下一个状态是否为某种状态进行二元分类？

是的，这是分类。我确实看到这里有一些相似之处，也许我们应该深入挖掘，看看是什么核心要素使深度学习能够扩展，然后我们如何利用这些洞察，如何提炼这些见解并将其应用于所有不同的领域，无论是语言还是强化学习。

**主持人：** 你理解我关于世界模型的意思吗？

**发言人：** 理解。我想我昨天可能听到 Eisenbach 教授在海报前向几个人解释过这个。因为这就像在做表征学习，试图为给定状态和动作学习有意义的表征，但对于给定目标，从某种意义上说，你可以把它看作是学习环境模型、学习世界模型，但不必做任何下一帧预测或其他更高维和复杂的东西。

**主持人：** 我试图思考和推动的角度不是学习下一个世界，而是基本上生成一些候选可能世界并对它们进行分类。就像我在玩扑克时，试图分类你有什么牌。根据你的行为，有一系列可能的牌。我获得的信息越多，就越能确定你到底有什么牌，基于你展示的内容，或者你在虚张声势。我觉得这是表征的最终目标，也就是世界。但我不知道这是否过于模糊，相比于视频生成领域更具体的世界模型类型。

另外，我也在探索一个方向。你提到深度模型更慢或更昂贵，这确实是推理领域让模型更浅的趋势。我在想"深度教师，浅显学生"这个短语是否会是一个好的部署范式？

**发言人：** 你用深度模型推动前沿能力，然后蒸馏回来。这确实是个好观点。如果你访问我们的网站，这是我们在最底部列出的未来方向之一。我们很想看看是否能获得类似的性能。我们在 JAX 的目标条件强化学习上实现了显著的最先进性能，看到训练强化学习智能体的前沿能力被推进是非常令人兴奋的。如果我们能以与标准网络同样高效的方式做到这一点，那将非常酷。

因为训练不必与你部署的推理相同。如果有方法蒸馏到更小的模型或剪枝模型并仍然保持性能，这将是一个非常有趣的研究方向。

**主持人：** 你还有什么个人兴趣？

**发言人：** 目前我正在追求强化学习中的拼接方向。我们试图从较短的子行为泛化强化学习，使它们在测试时被拼接合并。我认为这是我在博士期间将要解决的最后一篇论文之一。

### (22:27 - End) 未来研究方向讨论

就个人而言，我非常好奇能否真正推进前沿技术的极限。如果你看我们的论文，我们专注于深度缩放，但我们注意到宽度缩放实际上也能提高性能，而且通过深度缩放，我们实际上也解锁了批大小缩放的能力。

对，在传统强化学习中，基于价值的强化学习中批大小缩放并不是特别有效，但我们在深度学习的其他领域也看到其他工作表明，批大小缩放只有在网络容量足够大、能够利用扩展的批大小时才最有效。我们发现，一个假设可能是，传统强化学习中批大小缩放不那么有效的原因是，我们一直使用这些无法捕捉信息的小网络。

我们的一个实验发现，正因为我们成功训练了深层网络，这实际上为测试这个假设提供了很好的测试平台。我们发现，随着网络容量的扩展，我们也解锁了批大小缩放这一不同维度。所以我非常好奇，对于拥有足够算力的人来说，能否在这些环境中将深度扩展到最大能力，同时也扩展宽度和批大小。就像在语言模型中我们在很多不同维度上进行缩放一样，我们能否也解锁不同的缩放维度，以及能从中获得什么能力，我们能将训练这些强化学习智能体的前沿推进到什么程度。

**主持人：** 在进入下一个话题之前，当你说到足够的算力时，你们的算力预算是什么，我想了解一下你们得到了什么资源？

好问题。我们希望确保这是相当可及的，所以实际上好的一点是，我们所有的实验，甚至是千层网络，都可以在一个80GB的H100 GPU上运行。

**主持人：** 所以是一台GPU。

对，所有东西都可以在一个GPU上运行。但理论上，如果我们有分布式训练设置，能够向这个方向投入大量算力并真正想要推进前沿技术，看看会发生什么将会非常有趣。

**主持人：** 很酷。

我一直在积极学习尽可能多的视觉语言动作模型知识，在机器人领域的角色模型。

**主持人：** 视觉语言动作模型。

对，视觉语言。我对这些模型在机器人技术中的应用很感兴趣，正在积极探索这个领域，阅读大量文献，与尽可能多的人交谈。

**主持人：** 我们刚刚发布了与General Intuition的访谈。如果你了解他们的历史，他们最初是一家游戏剪辑公司，基本上拥有一个视觉语言动作模型。我看到了预览，非常印象深刻。我不确定它对具身用例的可转移性如何，但它不必局限于屏幕，你知道的。你有什么看法吗？

这确实是一个令人兴奋的研究方向。

**主持人：** 我认为将动作作为输出的概念在工业界实际上并不那么流行，对吧？仅仅因为文本在过去三年完全占主导地位，还有工具调用，这只是结构化文本的另一种形式。我感觉动作研究有点像...我不知道需要发生什么才能解锁下一阶段。你有什么有趣的见解吗？

有很多关于利用预训练视觉语言模型的酷炫工作，你冻结它然后在其基础上应用某种专家来输出动作。还有用于分层规划的系统，可能输出一些更高级别的计划，这是一个需要更长时间推理的大型网络，所以它以较低频率输出计划，然后有某种第二个系统运行得更快一些。我认为这个方向有相当多有趣的研究，这也是我期待的方向。

**主持人：** 很好。最后一个问题。海报展示会上被问到的最难问题，或者你遇到的任何名人的有趣经历？

**(28:55 - End)**

实际上我还没有机会多参加会议。我现在在全职工作。所以我实际上就在会议开始前几分钟才拿到徽章。所以我可能不是回答这个问题的最佳人选。

**主持人：** 不，不，就是人们问你的东西，对吧？或者遇到你的人，给我们一些人们在说什么的感觉。

我认为人们觉得这是一篇非常开眼界的论文，因为目标相当简单，相当优雅，而我们能够挑战强化学习不太可扩展这一传统智慧，将其推进到如此极限，比如千层深度，并看到性能持续改进。我得到的总体印象是，如果我们能沿着这个方向构建，我们真的可以在所有这些不同维度上进行缩放，推进强化学习能力的前沿。我非常好奇这会如何发展。

**主持人：** 好的，非常感谢你的到来。再次祝贺这篇论文。

谢谢你邀请我们。

**主持人：** 祝你未来工作顺利。

---

*生成时间: 2026-01-02 04:58:05*
*由 YouTube Monitor & Translator (Claude CLI) 生成*