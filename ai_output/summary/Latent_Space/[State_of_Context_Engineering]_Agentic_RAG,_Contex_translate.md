# [State of Context Engineering] Agentic RAG, Context Rot, MCP, Subagents — Nina Lopatina, Contextual

## 📹 视频信息

- **频道**: Latent Space
- **发布日期**: 2025-12-31
- **时长**: 26:49
- **原始链接**: [https://www.youtube.com/watch?v=tSRqTerZrH8](https://www.youtube.com/watch?v=tSRqTerZrH8)

---

> 本文内容整理自 Contextual 公司 Nina Lopatina 在 Latent Space 频道的技术访谈。

## TL;DR

Context Engineering（上下文工程）正从原型阶段向规模化发展，Agentic RAG 已成为新基准，Sub Agent 模式兴起，但系统级设计模式仍待统一。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-05:07 | NeurIPS 会议见闻与小模型趋势 | 讨论 NeurIPS 活动和小模型在实际应用中的趋势变化 |
| 05:07-09:10 | Agentic RAG 兴起与黑客松实战 | 从传统 RAG 到 Agentic RAG 的演进及实际比赛验证 |
| 09:10-14:21 | Context Engineering 现状分析 | 上下文工程的发展阶段和主要挑战如 Context Rot |
| 14:21-17:54 | MCP 协议的机遇与挑战 | Model Context Protocol 的实际应用场景和局限性 |
| 17:54-22:25 | 自动化上下文优化技术 | JEPA、ACE 等自动提示优化方法和 KV Cache 应用 |
| 22:25-26:29 | 代码域应用与未来预测 | Context Engineering 在代码生成的应用和系统化趋势预测 |

## 📊 核心论点

#### Agentic RAG 已成为新的基准标准

- **核心内容**：传统 RAG 正在被 Agentic RAG 替代，即使是最基础的查询重构步骤也能显著提升性能。通过将初始查询分解为子查询，更好地匹配相关文档，然后重新组合检索结果，这一方法已成为新的性能基准。Sweet Bench 的工作展示了通过训练模型进行大规模并行工具调用（6-8 个并行搜索）和限制代理运行时间来防止无限循环的创新。
- **关键概念**：查询重构、子查询分解、并行工具调用、有限代理、性能基准
- **实际意义**：改变了信息检索系统的架构设计，推动了从静态检索向动态推理的转变，为复杂查询处理提供了更可靠的解决方案。

#### Sub Agent 模式正在成为主流架构

- **核心内容**：2024年是"Sub Agent 年"，开发者越来越多地使用受约束的代理执行特定任务。过于通用的代理往往表现不佳、不可靠或缺乏必要工具。在大规模数据集（如10万文档的零售数据）测试中发现，Sub Agent 需要明确的轮次限制和工作验证机制，否则会陷入过度检查循环。
- **关键概念**：受约束代理、任务特化、轮次限制、工作验证、反过度优化
- **实际意义**：为AI工程提供了更可靠的架构模式，使得复杂任务可以分解为可管理的组件，同时支持针对特定任务的模型微调和性能优化。

#### Context Rot 成为最重要的性能瓶颈

- **核心内容**：Context Rot（上下文腐化）已成为广泛认知的关键问题。研究表明，在百万token上下文中，当到达70万token位置时，检索准确率降至30%。这不仅是直觉问题，而是有具体量化指标的性能瓶颈，影响整个系统的可靠性。
- **关键概念**：上下文腐化、检索准确率、长文本处理、性能瓶颈、量化评估
- **实际意义**：推动了新的架构设计，包括意图性上下文压缩、分层检索策略和更智能的内容管理，直接影响大规模AI应用的实用性。

#### MCP 协议面临上下文膨胀挑战

- **核心内容**：Model Context Protocol（MCP）虽然加速了原型开发，但存在上下文膨胀问题。工具描述以大型JSON格式放在前端，当工具数量达到10个时就会导致严重的Context Rot。实际应用中，开发者在原型验证后往往转向直接API调用以减少复杂性和依赖。
- **关键概念**：工具描述膨胀、原型到生产转换、API调用优化、依赖管理
- **实际意义**：影响了工具集成的最佳实践，推动了工具选择Sub Agent的发展，并促进了更高效的工具发现和管理机制的需求。

#### 自动化Prompt优化技术正在成熟

- **核心内容**：JEPA等自动化prompt优化技术通过让LLM分析输出结果并推理如何改进prompt来实现持续优化，类似PyTorch模型训练但仅作用于prompt而非权重。ACE方法通过小幅调整而非完全重写来避免信息丢失，在金融等复杂文档集上显示出更好的基准性能。
- **关键概念**：自动prompt优化、evolutionary算法、增量改进、信息保留、基准提升
- **实际意义**：降低了高质量prompt工程的门槛，为非专家用户提供了自动化优化路径，推动了更可靠和高效的AI系统开发。

#### KV Cache 正在重塑多轮对话架构

- **核心内容**：Key-Value Cache不仅提供效率提升，还为Agent提供更稳定的环境来执行连续操作。最佳实践是将不变内容（如系统prompt）放在前端，变化内容放在后端。随着对话轮次增加，context膨胀问题凸显，用户往往主动开启新窗口而非信任模型的自动压缩能力。
- **关键概念**：键值缓存、多轮对话、上下文膨胀、意图性压缩、稳定环境
- **实际意义**：影响了对话系统的设计模式，推动了主动上下文管理策略的发展，为长期交互应用提供了更可靠的基础架构。

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Contextual | Nina所在公司，Context Engineering平台提供商 | ⭐⭐⭐ |
| Anthropic | MCP协议创建者、Context Engineering研究贡献者 | ⭐⭐⭐ |
| OpenAI | 模型内部压缩技术提供商 | ⭐⭐ |
| Chroma | Context Rot概念推广者、向量数据库厂商 | ⭐⭐ |
| NVIDIA | Turing party主办方、AI硬件厂商 | ⭐⭐ |
| Mixbread | 黑客松获胜团队使用的开源模型平台 | ⭐ |
| Cursor | 代码AI助手，多轮对话体验参考 | ⭐ |

## 💬 经典金句

> "Agentic RAG is just generally better than RAG, even that initial incremental step of making that doing query reformulation improves performance so dramatically that became the new baseline."
> — Nina Lopatina

> "This year is kind of the year of the sub agent for me in terms of people using constrained agents to do very specific things."
> — 主持人

> "I think wherever you can use the KV cache for both efficiency but also for some sort of more stable environment for the agent to be able to take more and more actions in."
> — Nina Lopatina

## 👤 主要人物

#### Nina Lopatina

**身份**：Contextual AI 公司代表（具体职位未明确说明）
**背景**：神经科学研究背景，2016年首次参加NeurIPS（在巴塞罗那），曾在伯克利做博士后研究，专注于奖励学习和决策制定领域的神经科学研究
**核心观点**：Context Engineering正从原型阶段向规模化发展，Sub Agent和系统级设计模式将成为下一年的重点。强调实际应用中需要明确的轮次限制和性能优化，而非无限制的Agent行为。

#### 主持人（Latent Space）

**身份**：Latent Space播客主持人
**背景**：AI领域资深观察者，熟悉Sweet Bench等技术项目，对AI工程实践有深入理解
**核心观点**：推动Sub Agent概念的普及，强调限制性设计和并行化处理的重要性。认为小模型在实际应用中面临挑战，苹果Intelligence等产品未达预期。

## 📺 视频类型判断

**访谈对话**：技术专家与播客主持人的深度访谈，涵盖Context Engineering领域的现状、挑战和未来趋势。

---

## 📝 完整翻译

### (0:00 - 5:07) NeurIPS 会议见闻与小模型趋势

我们又回到了 Nina Latutina，欢迎来自 Contextual 的嘉宾。

Nina Latutina：谢谢。

我们今天要聊聊上下文工程的整体状况。不过我想先给大家分享一下我们在 NeurIPS 会议上的见闻。我刚才问她哪些是最佳的 NeurIPS 会后聚会，你说是 McCourt 和另一个什么？

Nina Latutina：Turing 和 NVIDIA。

对，Turing 和 NVIDIA，那里有一场非常精彩的炉边谈话，这很难得，因为我通常都会避开炉边谈话。但显然那次的嘉宾是 Yejin Choi，她在这些领域是非常知名的人物。那次谈话的主题是什么？

Nina Latutina：主要是关于 AI 的整体发展状况。由于她在 NVIDIA 工作，他们谈论了很多关于缩放定律和 AI 未来的话题。我觉得 Turing 的 CEO Jonathan Sidaris 在采访她时问的问题很有趣，观众的问题也很棒。最后一个问题特别完美。她专注于小语言模型研究，有人问道："你的研究不会导致 NVIDIA 市值大幅下跌吗？"

事实上，她在加入 NVIDIA 之前就把自己的研究发邮件给了黄仁勋，他回复说："很棒"，我觉得这很合理。

这就是现在世界的现状。

不过实际上，这里的含义是，如果你有更小的语言模型，你能做更多事情，可以让它们在手机上运行，所以你实际上可能最终会使用更多计算资源来支持越来越多的小语言模型去完成更多任务。

Nina Latutina：我想说的是，在 NeurIPS 上，OpenRouter 团队发布了他们基于 OpenRouter 数据的 AI 状况调研，其中有一个关于小型、中型、大型语言模型采用情况的有趣图表。分界线是：小型模型少于 150 亿参数，中型模型 150 亿到 700 亿参数，大型模型超过 700 亿参数。可以看到小型模型的市场份额在实践中呈下降趋势。

真的吗？

Nina Latutina：中型模型的市场份额在上升，而大型模型的份额保持不变。

Nina Latutina：当然这只是开源模型的数据，显然闭源模型的规模一直在不断增长。这只是一个数据点。

我觉得这就像苹果智能今年被广泛认为是失败的产品，而他们曾是设备端小模型运动的倡导者，这本来需要发挥作用但实际上没有成功。Gemini 今年在 Chrome 中推出了 Gemini Nano，我没有使用过，不知道效果如何。基本上，小模型的希望和梦想我觉得仍然局限在非常小的使用场景中，还没有大规模推广，我不知道需要什么条件才能实现大规模部署。

Nina Latutina：对于通用目的模型来说，这很有道理。但我认为对于其他组件模型，比如重排器，由于延迟限制，我从其他开发者那里听说更小的模型更好，我能看到这会成为一个更普遍的趋势。

### (5:07 - 9:10) Agentic RAG 兴起与黑客松实战

像 Agentic RAG 通常比传统 RAG 效果更好，即使是最初的增量步骤——进行查询重构。当你接收到初始查询时，能够将其分解为子查询，这样你就能更好地将这些查询与你想要检索的文档进行匹配，然后将结果组合起来进行检索。即使是这一步骤也能如此显著地提升性能，这基本上已经成为了新的基准线。

确实，这显然非常有帮助，你可以将其分散处理并行化，然后重新整合。今年我在 SweetGraph 上做的工作就是这样做的，我们有一个快速上下文模型，经过了训练。我认为有几个创新非常有趣。

一个是针对搜索进行了大规模或比正常情况更多的并行工具调用训练。正常的并行度通常是一到两个，最多四个。我们将基准线训练为一次六个并行搜索，最高可达八个。

另一个是有限的智能体能力，你不希望你的 Agentic 搜索永远运行下去。你确实希望它在某个时候终止并返回答案。因此通过强化学习来激励这一点，我认为是有帮助的、简单的，实际上扩展得非常好。

是的，我们确实发现对子智能体设置轮次限制和限制，让它们检查和验证自己的工作是非常重要的。

最近我和我团队的一名成员参加了一个上下文工程基金的活动。

Brian Bishoff 和 Hamill Hussein 上个月11月中旬在旧金山主办了这个活动，他们有大约不到10万份文档，都是零售领域的PDF文件。这个项目叫做零售宇宙，他们有日志文件、表格、巨大的CSV文件，我们使用我们的动态智能体来回答这个数据集中非常具有挑战性的查询并生成结构化输出。

在早期阶段我们注意到，对于如此大的数据集，智能体会想要让子智能体——比如说非结构化检索或结构化检索子智能体——会想要进行很多轮次，以确保他们检查了数据集中的每一个角落，但我们实际上不希望这样，因为数据集太大了。而且子智能体还会想要一遍又一遍地检查自己的工作。

这就是我们在大规模应用中注意到的问题。与你发现的情况类似，你不希望无限制的运行，无论是通过强化学习奖励结构还是系统提示中的明确指令来强制执行。我认为今年确实是子智能体之年，人们使用受约束的智能体来做非常特定的事情，有时过于通用实际上是一种反模式，因为它要么无法独立走得很远，要么不够可靠，或者没有所需的工具等等。

### (9:10 - 14:21) Context Engineering 现状分析

你的公司名叫 Contextual，这个名字简直太贴切了。你如何描述今年在 Context Engineering 方面的发展？

这是非常快速的一年，因为 Context Engineering 实际上在六个月前才真正兴起，但感觉像是已经过了一年。我认为最突出的一点是，有很多设计模式正在涌现，但人们还没有使用统一的设计作为架构标准。

我认为还有很多优化和效率提升的空间。对于很多新的开发，你通常会从让智能体使用尽可能多的 Token 开始，然后再想办法约束和优化它，比如使用键值缓存或其他能够帮助真正扩展技术的方法。所以我认为目前更多是在原型阶段，我预期明年我们会真正看到 Context Engineering 的规模化。

**主持人：** 规模化会是什么样子？到明年年底，我们能做什么现在做不到的事情，或者现在没有在做的事情？

我认为我们能够解决的任务类型会增加。比如说，我们已经看到了这种趋势的开始。在 HOW 基准测试中有一个测试项目——

**主持人：** 这是 Chroma 的那个吗？

不，这是来自普林斯顿的。

**主持人：** 好的，我不熟悉。

HOW 我现在忘了它代表什么，但它是一套用来评估长期运行的智能体任务的基准测试。其中有一个项目是评估重现研究论文，该基准测试在十月份发布，但在这周早些时候就被攻破了。

**主持人：** 天哪，被 Claude Code 攻破了？

**主持人：** 天哪。

是的，实际上他们需要让人类来运行评估，因为解决方案有点像人类会采取的不同方法，某种程度上是超人的。你知道，这是现在常见的模式，实际上黄金数据集本身有一些错误，所以模型会纠正它们。

**主持人：** 是的。

因为如果它得到100分，你会想："哦，有什么地方不对。" 这是一个警示信号，表明确实有问题。

**主持人：** 是的。

**主持人：** 所以我们应该故意这样做。

是的。我认为我们会继续看到这些基准测试，你知道，这些新的、经过深思熟虑且极具挑战性的基准测试问世，然后很快就被攻破。我认为对于越来越具有挑战性的任务，我们会继续看到这种情况。

**主持人：** 在一般的开发工作、营销以及领导一个类别方面，你觉得维护一个基准测试或者说拥有一个"这是 Contextual 基准测试，每个人都应该采用"的东西有用吗？我在这方面有些纠结，因为显然很多基准测试来自研究界而不是工业界，但我觉得工业界应该发挥作用。

实际上，我们一直在将我之前描述的黑客马拉松的数据集作为基准测试使用。我认为这是一个非常有趣的数据集，因为大多数基准测试使用非常小的数据集进行训练或推理，而这个实际上需要对大量数据进行推理。

**主持人：** 是的，这永远不会适合上下文窗口，对吧？

是的，所以我认为——

**主持人：** 你知道它有多少 Token 吗？还是你说有10万个文档？

我不知道那转换成多少 Token。

**主持人：** 哦。

我估计每个大概4000个 Token。

**主持人：** 可能更多。

可能更多，好的，那就是数十亿 Token。

**主持人：** 是的。好的，很酷。

是的。所以我认为我很希望看到更多基准测试。我很希望看到更多工业界的基准测试，因为这些会帮助我们在真实规模上进行评估，而不是像许多基准测试那样使用玩具示例。

### (14:21 - 17:54) MCP 协议的机遇与挑战

没错，因为 MCP 是前端这个巨大的 JSON 结构。你需要塞进所有工具的描述，所以当你有 10 个工具时，很快就会遇到直接的上下文污染问题，特别是当这些工具很复杂的时候。

是的。所以在工具使用方面有一些非常有趣的工作，一些非常有趣的博客文章。Manis 写了一篇比较通用的文章，但包含了一些工具使用的最佳实践，Anthropic 也写了更多关于工具使用模式的内容。

**主持人：** Cloudflare。

是的，那是另一篇。是的。

实际上很有趣，回到我们的重排序器，我实际上建立了一个选择使用哪些 MCP 服务器的原型。能够选择这些服务器本身就是一个上下文挑战，因为有太多了。

**主持人：** 这是一个子代理。

是的。

**主持人：** 有趣。人们对此很兴奋吗？他们在大规模部署。你看到很多吸引力了吗？

我认为是这样的，所以我认为是的，我们确实看到了很多很好的使用。我们的动态代理也使用 MCP 服务器，我认为今年早些时候我做了一些非常有趣的演示，能够非常快速地组合工具来制作原型。是的。所以我认为它真的帮助人们更快地制作原型，并在早期版本中显示价值，然后再大规模构建。

对我个人来说，在我的动态代理配置中，我更倾向于 API 调用和一些更轻量的方案。一旦我能够用 MCP 服务器制作原型并弄清楚我要如何使用它，我认为你可以减少复杂性并减少依赖性。

**主持人：** 是的。提到 MCP，我不知道你想深入到什么程度，但 Anthropic 的 MCP 网关终于推出了。有很多 MCP 服务在做各种发现和注册表的工作。人们应该知道什么？人们在押注什么？实际上什么在起作用？

在 MCP 服务器或目录、网关方面，基本上就是 SCP 初始推出后发生的一切。是的，已经有很多工作。有 MCP UI，但我不知道这是否真的严格属于上下文工程。

是的，我们将我们的 MCP 服务器添加到了注册表中，感觉那个注册表真的是为代理阅读而不是人类阅读而设计的。

**主持人：** 官方的 Anthropic 注册表。

是的。这就像一个 GitHub 仓库。是的。这很有趣，因为我认为是的，拥有强大的代理体验绝对有价值，我认为这将是明年肯定会增长的领域，能够让代理在没有人工干预的情况下完成任务。

但我确实认为对于 MCP 服务器，我认为你仍然希望人类来选择要包含在工具列表中的内容。你想要检查安全性和其他事项，然后才会让它运行。

### (17:54 - 22:25) 自动化上下文优化技术

**主持人：** 请纠正我，这有点像原始 DSPI 理念的演进，你设置目标并让 LLM 通过查看输出和推理来优化自己的 prompt，思考应该在 prompt 中继续添加什么来改善评估。这就像一种类似 PyTorch 的训练循环模型，但只是在 prompt 中，而不是在权重中。

**嘉宾：** 你显然也可以将其扩展到权重。我认为为什么叫它 Jeepa 的另一个原因是，它有一个进化元素或遗传进化元素，你推出多个样本，然后从中选择最佳的幸存者。

**主持人：** 还有什么我遗漏的吗？

**嘉宾：** 没有，你概括得很好，实际上让我想起了 ACE。

**主持人：** 是的。所以实际上，自动上下文工程的那种方法实际上已经在金融和其他复杂文档集上显示出更好的基准性能，他们采用的方法非常有趣。基本上，如果你采用像 Jeepa 这样的方法，基本上可能丢弃整个 prompt 并重新开始，或者你做很多很多步骤，你不断压缩和扩展、压缩和扩展，你会失去一些信息，并且可以看到性能的显著下降。因此，他们使用这种代理方法只是对当前 prompt 进行较小的调整，而不是从头重写。

**嘉宾：** 这是他们该方法的创新之一，我认为这实际上与 Seth 关于 KV 缓存说的内容相符，真的，我认为代理可能会很快变得相当困惑，这真的会降低性能并导致幻觉。

因此，我认为无论你在哪里都可以使用 KV 缓存，不仅是为了效率，也是为了为代理提供某种更稳定的环境，使其能够采取越来越多的行动。

**主持人：** KV 缓存在上下文工程中有多少决策制定？我觉得显然答案是不变的内容放在前面，变化很多的内容放在后面。我认为这主要是因为我关心的大多数代理都是多轮的，所以缓存是整个轮次，之前发生的五轮，我不会经常改变系统 prompt。如果你向一千个客户提供相同的 prompt，KV 缓存会节省费用。我想这就是这样。但我们不这样做。

**嘉宾：** 我认为它也可以提高性能。但我认为随着对话变得越来越多轮，我还没有真正看到一个能很好处理这个问题的系统。比如对于 Cursor，我到了对话中的某一点，我就会打开一个新窗口，即使我还没有完成那个对话，因为上下文变得臃肿。

Dexter 会称这为有意的上下文压缩，有意的频繁上下文压缩，因为你还不相信模型能做好压缩。我要说的是，Anthropic 和我认为 OpenAI 以及可能 Gemini，他们都在模型内部做压缩，每一次前沿版本发布都是如此。

### (22:25 - End) 代码域应用与未来预测

**主持人：** 自动化上下文工程真的很棒。有没有专门针对代码的应用？我想我不太了解你们在代码领域遇到多少情况。听起来是有的。代码领域和法律、零售、客服这些其他领域相比，有很大区别吗？

**嘉宾：** 我们的目标是创建一个真正端到端的平台，可以轻松启动并运行于任何领域和任何用例。我想我们在刚才提到的电商黑客马拉松测试中亲身体验了这一点，虽然我们确实有那个领域的客户，但我还没有真正接触过那些工作。代码一直是我们合作的领域之一。我们实际上刚刚使用我们的平台为设备进行测试代码生成。我们发现采用的方法是相同的，我们有多模态摄取能力，能够获取文档内容的层次结构，然后是包括过滤器、重排器、混合搜索的检索管道。所有这些结合起来是一个很好的起点，然后我们能够快速爬坡，实际上在基于人类的评估中获得了最先进的结果，或者说与编程平台相比，该客户获得了最高分。所以我认为虽然需要一些定制化，但上下文工程适用于代码的方式与适用于其他领域的方式是相同的。

**主持人：** 很棒。来到预测环节。你认为在上下文工程中，现在什么被低估了，但明年会成为热门话题？基本上什么是被低估的？人们应该更多讨论但现在没有讨论的是什么？

**嘉宾：** 我认为真正被低估的是完整系统。我觉得现在人们讨论的是系统某个部分或另一个部分的创新，比如说不同的组件，比如记忆系统或重排器，或者围绕上下文压缩的设计模式等等。我认为明年我们会有完整的系统，可以成为一种设计模式，而不是组件级别的讨论，我们将在那个级别进行讨论。

**主持人：** 很棒。这是你第五次参加NeurIPS，我想说对于老参与者来说，我觉得Nina上次在圣地亚哥是佛罗里达的NeurIPS。我不知道你当时是否在那里。

**嘉宾：** 那是哪一年？

**主持人：** 大概2017年吧。你再次回来时，如何反思这个场景的变化？

**嘉宾：** 很有趣。我第一次参加NeurIPS是2016年在巴塞罗那。那很棒。当时我刚完成神经科学的研究生研究，专注于奖励学习和决策制定，我在伯克利做博士后，但我有一张研究生工作的海报。所以我飞去了几天，这真的不是我期待的研究会议，因为我主要参加神经科学研究会议。

**主持人：** 有什么不同？

**嘉宾：** 这是神经信息处理系统会议。我知道。我在这里也遇到了很多神经科学家和早期职业阶段的人。我没有想到会有那么多行业派对。这在神经科学领域根本不存在。

而且那时会议要小得多，你知道，它看起来像一个大型会议，那里的每个人都说："哦，是的。以前要安静得多，小得多。"我觉得现在有太多人了。很多人是第一年参加，我已经看不到早年遇到的一些相同面孔了。

**主持人：** 他们还在。你只需要在派对和休息区之类的地方找到他们。很好。有什么行动号召吗？人们如何帮助你，找到你，诸如此类？

**(26:05 - End)**

**嘉宾：** 我们在上下文工程领域有一些非常令人兴奋的更新即将到来。所以，请在Twitter或LinkedIn上关注我Nina Ladina，或者关注Contextual AI，请保持关注。

**主持人：** 请保持关注。太棒了。谢谢你。

**嘉宾：** 谢谢。

---

*生成时间: 2026-01-02 05:03:50*
*由 YouTube Monitor & Translator (Claude CLI) 生成*