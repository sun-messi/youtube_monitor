# [State of Code Evals] After SWE-bench, Code Clash & SOTA Coding Benchmarks recap — John Yang

## 📹 视频信息

- **频道**: Latent Space
- **发布日期**: 2025-12-31
- **时长**: 17:42
- **原始链接**: [https://www.youtube.com/watch?v=MxB-xRGXxkk](https://www.youtube.com/watch?v=MxB-xRGXxkk)

---

# 代码评估前沿：从 SWE-bench 到 Code Clash 与最新编码基准测试综述 — John Yang

> 本文内容整理自斯坦福大学博士生 John Yang（约翰·杨）在 Latent Space 频道的技术访谈。

---

## TL;DR（一句话核心洞察）

从单一任务评估到长周期开发竞赛，代码评估基准正在从静态测试转向动态对抗与人机协作场景，SWE-bench 引发的军备竞赛推动了 Code Clash 等新型评估框架的诞生，揭示了当前 AI 编程能力评估的局限性与未来方向。

---

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-02:00 | SWE-bench 项目回顾与发展 | 从 Devon 的成功看 SWE-bench 如何引爆 AI 编程军备竞赛 |
| 02:00-04:00 | SWE-bench 的多语言扩展 | JavaScript、Rust、Java 等 9 种语言覆盖 40 个代码库 |
| 04:00-07:00 | Code Clash：编程竞技场 | AI 模型通过编程游戏对抗，评估长期开发能力 |
| 07:00-09:00 | 2025 年编码评估新趋势 | 从效率优化到安全评估，多维度评估框架兴起 |
| 09:00-11:00 | 用户模拟器与 WebArena | 探索 AI 在真实用户交互场景中的表现 |
| 11:00-14:00 | 长时自主开发 vs 交互式协作 | 5 小时自主编程与实时人机协作的权衡 |
| 14:00-17:00 | 未来研究方向与挑战 | 用户交互数据获取、代码库理解与自动化上下文工程 |

---

## 📊 核心论点

### 1. SWE-bench 的意外成功与局限性

- **核心内容**：SWE-bench 发布于 2023 年 10 月，最初反响平淡，直到 Cognition 的 Devon 在该基准上取得突破性成绩（约两周前通知作者），才引发整个行业的"军备竞赛"。然而，SWE-bench 存在根本局限：所有任务相互独立，一旦模型提交就结束评估，无法测试长期开发能力或代码库的持续演化。
- **关键概念**：独立任务评估、单元测试验证、Django 偏向性、任务分布局限
- **实际意义**：推动了行业从"一次性任务"向"持续开发能力"评估的转变，促使研究者开发更贴近真实软件开发流程的评估框架。

### 2. Code Clash：从静态测试到动态对抗

- **核心内容**：Code Clash 引入编程竞技场概念，让两个或多个语言模型维护各自代码库，通过多轮对抗评估长期开发能力。每轮包括"改进阶段"（模型自主优化代码）和"竞争阶段"（代码库对抗）。初版使用编程游戏如 Halite（由 Cursor 的 Michael Trolls 开发），类似"可编程的星际争霸"。
- **关键概念**：长期开发评估、代码库演化、多智能体对抗、编程游戏竞技场
- **实际意义**：更接近真实软件开发中的迭代优化过程，能评估模型的策略规划、代码维护、错误修复等综合能力。

### 3. 多维度评估框架的兴起

- **核心内容**：2025 年编码评估呈现多元化趋势：Efficiency 专注代码性能优化（保持功能不变但运行更快），SecBench 聚焦网络安全，SciCode 2 面向科学计算，Critical Point 涉及物理模拟。这些基准针对不同垂直领域，反映了 AI 编程能力评估从通用到专业的演进。
- **关键概念**：垂直领域评估、性能优化、安全评估、科学计算、领域专业化
- **实际意义**：企业可根据具体应用场景选择合适的评估基准，推动 AI 编程工具在特定领域的深度优化。

### 4. 长时自主 vs 实时交互的权衡

- **核心内容**：作者设想的"5 小时自主编程"愿景与 Cursor/Windsurf 强调的"快速交互"形成对比。长时自主适合批量数据处理、JSON 解析等低交互任务，而复杂功能开发需要频繁的人机交互来处理歧义和需求变更。这不是二选一，而是针对不同任务类型的互补方案。
- **关键概念**：任务抽象层级、人机协作模式、需求歧义处理、交互频率优化
- **实际意义**：未来 AI 编程工具需要支持多种协作模式，让开发者根据任务特性选择最优的人机交互方式。

### 5. 用户交互数据的稀缺与解决方案

- **核心内容**：学术界缺乏 Cognition、Cursor 等公司拥有的真实用户交互数据，这限制了人机协作研究。两种解决路径：构建吸引用户的产品（如 El Marina）收集数据，或开发高质量用户模拟器。后者不能简单依赖"ChatGPT 扮演人类"，需要更精细的行为建模。
- **关键概念**：交互数据采集、用户行为模拟、产品驱动研究、学术-工业差距
- **实际意义**：推动学术界与工业界合作，共享脱敏数据或开发开源用户模拟器，加速人机协作研究。

---

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Cognition (Devon) | SWE-bench 突破者，引发 AI 编程军备竞赛 | ⭐⭐⭐ |
| Cursor | 编程游戏 Halite 开发者，交互式 AI 编程先驱 | ⭐⭐⭐ |
| Anthropic | Impossible Bench 开发者，测试模型拒绝能力 | ⭐⭐ |
| Jane Street/Two Sigma | 编程竞赛文化推广者，Halite 游戏使用方 | ⭐⭐ |
| Menlo | WebArena 相关工作，环境构建探索 | ⭐ |

---

## 💬 经典金句

> "I don't like unit tests as a form of verification."
> — John Yang（关于传统评估方法的局限性）

> "It's like playing Starcraft but you can code."
> — John Yang（描述 Code Clash 的编程竞技理念）

> "I tell it a goal... I walk away for 5 hours. I'm hanging out with you, talking to my friends. I come back and it gives me like literally a SOTA codebase."
> — John Yang（对未来 AI 编程的愿景）

---

## 👤 主要人物

### John Yang（约翰·杨）

**身份**：斯坦福大学博士生，师从 Diyi Yang 教授；SWE-bench 主要作者之一
**背景**：在普林斯顿大学与 Shunyu Yao、Karthik Narasimhan 密切合作；专注于代码生成评估和人机协作研究
**核心观点**：认为现有基于单元测试的独立任务评估无法反映真实软件开发的复杂性，提出通过长期开发竞赛（Code Clash）和多维度专业评估来推进 AI 编程能力的全面提升

### Carlos Jimenez & Ofir Press

**身份**：SWE-bench 共同作者
**背景**：与 John Yang 合作开发了 SWE-bench 及其多个扩展版本
**核心观点**：推动评估基准的多样化，包括多语言支持和更广泛的代码库覆盖

---

## 📺 视频类型判断

**访谈对话**：主持人与 John Yang 的一对一深度技术访谈，探讨代码评估基准的现状与未来

---

## 📝 完整翻译

### (0:00 - 15:00) Part 1

我们现在在 NeurIPS 现场，与 SWE-bench 和其他多个项目的创造者 John Yang 在一起。欢迎你。

John Yang：非常感谢邀请我。很高兴来到这里。

去年我和 Ofir 聊过，我想还有 Carlos，他是你的合作者之一。SWE-bench 现在怎么样？这个项目已经一年半了吧。

John Yang：是的，一年半是指它真正有用并发布的时间。我们在 2023 年 10 月发布，当时没有太多人关注。然后 Cognition 公司出现了，Devon 的发布非常惊艳，我认为从那时起就真正启动了这场竞赛。

他们事先告诉你了吗，还是直接就发布了？

John Yang：大约两周前我收到了一封邮件，我记得是 Walden 发的。他说："嘿，我们在上面取得了不错的成绩。"我说："哇，恭喜。感谢使用我们的工具。"然后正式发布时真是让人震惊。我当时想："哇，这些人做得太出色了。"

太棒了。然后 SWE-bench Verified 是去年推出的吧。

John Yang：没错。

给我们介绍一下今年的进展。你们有其他语言版本，现在有各种各样的 SWE-bench 变体。大家应该了解些什么？

John Yang：当然。我认为有几个扩展正在进行。一个是更多的 SWE-bench 版本，像 SWE-bench Pro、SWE-bench Live。

SWE-bench Pro 是你们做的吗？因为看起来是独立的，作者不同。

John Yang：完全独立。他们只是在没有我们授权的情况下叫它 SWE-bench Pro。我觉得我们对此没什么意见。当它出来时，我们觉得："哦，很酷，有意思。"如果能参与其中会很有趣，但我要恭喜他们，这是个很棒的基准测试。

还有多模态。

John Yang：是的，我们做了多模态和多语言版本。多语言似乎是...

是像 JavaScript 吗？还有什么？

John Yang：多语言版本涵盖了大约 40 个仓库的 9 种语言。包括 JavaScript、Rust、Java、C、Ruby 等等。

关于 SWE-bench 本身，很多人都在谈论它对 Django 的侧重。我们该如何超越 Django？

John Yang：当然，很高兴看到许多新的基准测试真正努力使仓库多样化。在我们做的多模态和多语言的后续工作中，我们特意这样做了。

但你也可以直接发布 SWE-bench 2025，做一个新的分布。

John Yang：确实如此。很高兴看到这些后续工作。我很兴奋地想看看人们如何策划下一批测试集。看到他们在文献或博客文章中如何证明为什么要创建独立的分支很有意思。更简单的是增加更多语言、更多仓库，现在人们会说我们的更难是因为这种策划技术。我很期待看到这能持续多久，以及我们将如何引导评估的方向。

最近你在做 CodeClash。

John Yang：没错。我已经在其他播客上谈过这个，比如和 Andy 的对话，但我很乐意给大家一个简短的介绍。

基本思想是我不喜欢用单元测试作为验证形式。我也认为 SWE-bench 存在一个问题，就是所有任务实例都是相互独立的。一旦模型提交了，就结束了，故事就此终结。

而在 CodeClash 中，我们想要真正评估长期开发，以及在一个有影响力的代码库上的开发，这个代码库会受到模型之前所做操作的影响。基本理念是让两个或更多语言模型进行编程锦标赛。每个模型维护自己的代码库，在锦标赛的每一轮中，首先它们可以按照自己的想法编辑和改进代码库，非常自主。然后在竞赛阶段，两个代码库会相互对抗。代码库会运行，通常有一个竞技场，我们有很多不同的竞技场，竞技场会判定代码库 A 优于代码库 B，然后在多轮中重复这个过程。

由 LLM 评判来判定吗？

John Yang：是的，LLM 评判确实是其中一种机制。我们从一些相当简单的编程游戏开始。其中比较酷的是 Halite...

哦是的，我为 Jane Street 玩过。

John Yang：没错。你知道那很棒。Halite 一、二、三。Cursor 的 Michael Truell 写了这个游戏。

是 Two Sigma 还是 Jane Street？

我在 Two Sigma 工作过。

John Yang：哦，Two Sigma。2016 年的事了，但我们要让它回归。Halite 很有趣。如果你从未参加过需要控制舰队、攻击、防御和收集资源的编程竞赛...

就像玩星际争霸，但你可以编程。

John Yang：完全正确。

有很多游戏。除了游戏还有非游戏类的吗？

John Yang：这是个很好的观点。对于初始发布的科学目的，我们使用了现有的编程游戏。当前正在进行的工作是构建具有经济价值的竞技场。

是的，SWE-ter 今年很重要。

John Yang：GDP，太棒了。我认为 Terminal bench 和 SWE-bench 这些评估的主要卖点是它们非常接近真实世界的实用性，所以我认为这对 CodeClash 来说是可以解决的，这就是我们正在做的。

你是小组的一部分。其他学生也发布了很多其他东西。你会重点介绍什么？

John Yang：Ofir 在基准测试方面是一位非常高产的导师。我真的很喜欢效率方面的工作...

EfficientCode 是由一位名叫 Jeffrey Ma 的博士生撰写的，他碰巧是我的高中同学。其理念是你拿到一个代码库，只想做一些修改让代码运行得更快。比如并行化、SIMD 操作之类的。

所以不改变行为，只是更快？

John Yang：正是如此。保持单元测试通过，但我要更好的运行时间。

然后还有 AlgoTune 也是这个方向的。还有在科学编程领域的推进。SciCode 2 很棒，他们做了一个快速的...

对人们来说，我解释 SciCode 的方式是它是 HumanEval 但更好。

John Yang：完全正确。我认为现在有很多好的东西，这就是方向。

就是说 SWE-bench 运行起来很昂贵。任何代理技术基准测试都很昂贵。实际上你确实需要一些完成型基准测试...

John Yang：正是如此。你可以先在这些上面表现良好，然后再升级到多轮昂贵的测试。

除此之外，2025 年在编码评估领域的其他工作，我们提到了 Metr。他们使用 VBench，有一个非常有趣的人工时数指标。他们把运行时间作为 x 轴，完成度作为 y 轴，我们可以做更多长时间运行的速度和任务。我认为这些预测相当有趣，我很欣赏他们使用 SWE-bench Verified 来代理很多这些东西。

还有其他引起你注意的工作吗？

John Yang：在 Terminal bench 方面，Critical Point 很酷。这是 Ofir 做的一个非常新的基准测试，我认为它与物理学有关。还有一个叫 SecBench 的与网络安全相关。SBench，我认为它与 Llama 有关联。看到人们真正深入不同的编码领域很酷。

稍微跳出编码领域，我个人认为用户模拟器的东西相当有趣。像 TAU-bench、WebArena...

我对此有复杂的感受。

John Yang：我很想听听。

就像你在采样一条路径。说实话，我不知道它有多现实。LLM 做这个很酷，但...

John Yang：当然，我同意。我认为这是一个很好的初步努力。对我来说，看到像 Metr 这样的公司专注于构建代码之外的环境非常酷。我认为有像 WorkGym 风格的东西可能会很有趣。这是我在斯坦福的导师 Dorsa Sadigh 经常思考的东西。

### (15:00 - End) Part 2

或者你建立真正好的用户模拟器来尝试模拟这些设置。但这也并非易事。我认为这不像说"嘿，ChatGPT，表现得像个人类"那么简单。

John Yang：是的。所以如果能从中获得灵感，了解这些数据到底是什么样子，或者在这两者之间，什么是扩展评估人机交互的最佳方式，那就太酷了。然后我想从我自己的角度来看，我们正在推动更多的竞技场。对于 Code Clash，让我兴奋的是，当前的框架是真正的长时间运行的 suite agents，但你知道，你可以有多智能体，比如两个智能体在代码库上协作，会发生什么？你让一个人类和一个智能体在代码库上工作，而不是只有 AI，会发生什么？当模型改进，希望它们能够爬坡并变得更擅长消化日志和迭代分析时，人机交互会如何随着模型能力而改变？

所以我希望能够激励和说服人们，这是一个非常酷的测试平台，你可以在不同的竞技场上进行许多不同的人机组合，一次玩一个竞技场，或者同时玩多个竞技场。

主持人：是的，我非常有兴趣与你在交互方面进行合作。

John Yang：哦，那太棒了。

主持人：然后我想再补充一点，关于 Cognition，我们将大力推动代码库理解，这有点像代码库检索的升级版。主要是帮助人类更好地理解他们自己的代码库，使人类能够与机器进行思维融合，来完成 LLM 单独无法完成、人类单独也无法完成的最高级任务。

另一件事是基本上为 LLM 进行自动化的上下文工程。所以这就像是我们正在开发的一种研究子智能体。

John Yang：太棒了。

主持人：所以我不知道基准测试会是什么，因为你如何对理解进行基准测试？

John Yang：这倒是真的。[笑声]

主持人：除了我认为你可以冻结一个仓库，有一些手动整理的答案，然后你提出琐碎的问题，但这很容易达到饱和。所以，我不知道还有什么其他方法。

John Yang：是的，我想 Silas 不久前发推说过那种代码维基的东西，那真是太不可思议了。我的意思是我用...

主持人：Google 实际上刚刚推出了他们自己的版本。

John Yang：哦是的，那个反重力团队的。

主持人：不不不，这是一个独立的团队。

John Yang：明白了，明白了。

主持人：但很酷。这就是代码的现状。

John Yang：是的。

[音乐]

---

*生成时间: 2026-01-02 04:35:05*
*由 YouTube Monitor & Translator (Claude CLI) 生成*