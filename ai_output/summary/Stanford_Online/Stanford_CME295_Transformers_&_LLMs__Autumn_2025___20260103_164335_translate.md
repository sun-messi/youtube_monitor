# Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 3 - Tranformers & Large Language Models

## 📹 视频信息

- **频道**: Stanford Online
- **发布日期**: 2025-10-17
- **时长**: 1:48:41
- **原始链接**: [https://www.youtube.com/watch?v=Q5baLehv5So](https://www.youtube.com/watch?v=Q5baLehv5So)

---

本文内容整理自斯坦福大学教授克里斯托弗·雷（Christopher Ré）等在 Stanford Online 频道的《Transformers 与大语言模型》课程第 3 讲。

## TL;DR

深度解析大语言模型的核心架构和生成机制：从混合专家系统（MoE）的稀疏计算优化，到温度控制的采样策略，再到 KV 缓存等推理加速技术，系统阐述了现代 LLM 从训练到部署的完整技术栈。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-09:25 | 大语言模型基础与混合专家系统 | 定义 LLM 并引入 MoE 概念，解决计算效率问题 |
| 09:25-36:00 | 混合专家系统深入：路由机制与训练 | 详解稀疏 MoE 的路由选择、训练挑战和实际应用 |
| 36:00-54:00 | 文本生成机制：从贪心到采样 | 对比贪心解码、束搜索和采样策略的优劣 |
| 54:00-67:00 | 温度控制与引导解码技术 | 分析温度参数对概率分布的影响和格式化输出 |
| 67:00-82:00 | 提示工程：上下文学习策略 | 零样本、少样本学习和链式思维推理技术 |
| 82:00-108:00 | 推理优化技术：精确方法 | KV 缓存、分组查询注意力和页面注意力机制 |
| 108:00-124:00 | 推理加速：近似与投机解码 | 多潜在注意力、投机解码和多令牌预测技术 |

## 📊 核心论点

#### 大语言模型的定义与特征

- **核心内容**：LLM 是基于 Transformer 的解码器模型，具有三大特征：参数规模达到十亿级别、训练数据量达到千亿甚至万亿令牌、需要大量计算资源。与 BERT 等编码器模型不同，LLM 专门用于文本生成任务，采用因果自注意力机制进行下一个令牌预测。现代 LLM 超过 90% 采用纯解码器架构，移除了交叉注意力层。
- **关键概念**：解码器架构、因果自注意力、下一令牌预测、参数规模、训练数据量
- **实际意义**：为理解现代 AI 系统奠定基础；统一了 GPT、LLaMA、Gemma 等主流模型的架构范式；明确了 LLM 与传统语言模型的区别。

#### 混合专家系统（MoE）的核心机制

- **核心内容**：MoE 通过稀疏激活解决大模型计算效率问题，核心思想是根据输入选择性激活专家子网络。公式为 $\hat{y} = \sum_{i} G(x)_i \cdot E_i(x)$，其中 G 是门控网络，E_i 是专家网络。稀疏 MoE 只激活 Top-K 个专家（通常 K=1 或 2），大幅降低 FLOPs（浮点运算次数）。专家通常替换 FFN 层，因为 FFN 参数量占比最大（$d_{model} \times d_{ff} \times 2$）。
- **关键概念**：稀疏激活、门控网络、专家选择、FLOPs 优化、路由崩塌
- **实际意义**：实现模型容量与计算成本的解耦；Switch Transformer 达到万亿参数规模；成为 GPT-4、PaLM 等超大模型的核心技术。

#### 路由崩塌问题与解决方案

- **核心内容**：训练 MoE 面临路由崩塌（routing collapse）问题，即门控网络倾向于总是选择少数几个专家，导致其他专家闲置。解决方案是在损失函数中添加辅助项：$L_{aux} = \alpha \times n \times \sum_i f_i \times P_i$，其中 f_i 是路由到专家 i 的令牌比例，P_i 是专家 i 的平均路由概率。该项促使专家使用更加均匀。
- **关键概念**：路由崩塌、负载均衡、辅助损失、均匀分布
- **实际意义**：确保所有专家得到有效训练；提高模型容量利用率；为大规模 MoE 训练提供稳定性保证。

#### 温度参数对生成多样性的影响

- **核心内容**：温度参数 T 控制 softmax 输出的尖锐度。数学上，当 T→0 时，概率分布变为尖峰分布，仅激活概率最高的令牌；当 T→∞ 时，分布趋向均匀。通过因式分解 $P(w_i) = \frac{e^{x_k/T}(e^{(x_i-x_k)/T})}{\sum_j e^{x_k/T}(e^{(x_j-x_k)/T})}$，可证明低温度产生确定性输出，高温度增加创造性。
- **关键概念**：温度参数、softmax 尖锐度、生成多样性、确定性vs创造性
- **实际意义**：为不同应用场景提供可控的生成策略；平衡输出质量与创新性；成为所有 LLM API 的标准参数。

#### 上下文腐烂（Context Rot）现象

- **核心内容**：虽然现代 LLM 支持百万级别的上下文长度，但存在"大海捞针"问题——随着上下文长度增加，模型检索特定信息的能力显著下降。研究表明，干扰信息（distractors）会进一步降低检索能力。因此，检索任务中应优先提供最相关的上下文，而非盲目增加上下文长度。
- **关键概念**：上下文长度、信息检索、大海捞针测试、干扰因素
- **实际意义**：指导 RAG 系统设计；优化长文档处理策略；平衡上下文容量与检索精度。

#### 链式思维（Chain of Thought）推理

- **核心内容**：CoT 要求模型在给出最终答案前输出推理过程，显著提升复杂推理任务的性能。核心机制是强制模型展示"思考路径"，避免直接跳跃到结论。结合自一致性（self-consistency）技术，通过多次采样并投票选择最频繁答案，进一步提升鲁棒性。现代模型的推理能力提升使得良好的指令设计有时能超越少样本学习。
- **关键概念**：推理路径、中间步骤、自一致性、多数投票、指令优化
- **实际意义**：提升数学推理和逻辑任务性能；增强可解释性和可调试性；推动"慢思考"范式发展。

#### KV 缓存优化机制

- **核心内容**：在自回归生成中，避免重复计算历史令牌的键值表示，将其缓存以供后续使用。结合分组查询注意力（GQA），多个查询头共享键值投影，减少缓存存储需求。页面注意力（PagedAttention）通过固定大小块（如16个令牌）管理内存，减少内部碎片化，显著提升内存利用效率。
- **关键概念**：键值缓存、分组查询注意力、页面注意力、内存碎片化
- **实际意义**：大幅降低推理延迟；提高服务器并发处理能力；成为所有商用 LLM 推理引擎的标准技术。

#### 多潜在注意力（Multi-Latent Attention）压缩

- **核心内容**：通过矩阵分解减少 KV 缓存存储需求。将键值投影矩阵分解为低维压缩和解压缩两步：先降维到潜在空间，再恢复到目标维度。关键创新是跨键值和多头共享压缩表示，使每个令牌在每个 Transformer 层只需存储一个压缩向量，而非多个完整的键值向量。
- **关键概念**：矩阵分解、潜在空间压缩、跨头共享、存储效率
- **实际意义**：在长上下文场景下显著减少内存占用；意外带来正则化效果提升性能；为资源受限环境部署大模型提供可能。

#### 投机解码（Speculative Decoding）技术

- **核心内容**：利用小型草稿模型预测多个候选令牌，然后用大型目标模型并行验证。接受机制基于概率比较：若草稿模型预测令牌的概率 p_draft ≥ p_target，则接受；否则按 (p_target - p_draft)/p_draft 概率接受。数学上可证明该方法产生的分布与直接使用大模型相同。
- **关键概念**：草稿模型、并行验证、概率接受机制、分布等价性
- **实际意义**：在保持输出质量前提下显著加速生成；充分利用推理时的内存带宽；为实时对话系统提供关键性能提升。

#### 多令牌预测架构创新

- **核心内容**：在同一模型中嵌入草稿和目标机制，训练时预测多个未来令牌而非单个下一令牌。架构在最后一层添加多个预测头，推理时第一个头作为主模型，其余头作为草稿模型。虽然失去了投机解码的分布等价性保证，但避免了维护两个独立模型的复杂性。
- **关键概念**：多头预测、训练目标修改、嵌入式草稿模型、贪心验证
- **实际意义**：简化部署流程；减少模型管理复杂度；为未来的统一训练推理架构提供思路。

## 🔬 提及的技术/方法/论文

| 技术/论文 | 讨论语境 | 重要性 |
|----------|----------|--------|
| Switch Transformer (2021) | 首个万亿参数 MoE 模型 | ⭐⭐⭐ |
| GPT, LLaMA, Gemini | 主流解码器模型代表 | ⭐⭐⭐ |
| PagedAttention (vLLM) | 内存高效的 KV 缓存管理 | ⭐⭐ |
| Multi-Latent Attention | KV 缓存压缩技术 | ⭐⭐ |
| Speculative Decoding | 推理加速的并行验证方法 | ⭐⭐ |
| Chain of Thought | 推理能力提升的提示技术 | ⭐⭐ |
| Group Query Attention (GQA) | 减少 KV 缓存的注意力变体 | ⭐⭐ |
| Plan and Solve | 基于推理的零样本学习 | ⭐ |
| Context Rot 研究 | 长上下文检索能力分析 | ⭐ |

## 💬 经典金句（3-5 句）

> "现代 LLM 超过 90% 都是解码器架构。"
> — 讲师

> "如果你有一个数学问题，你会问数学家、化学家、历史学家所有人吗？现在的模型就是这样做的。"
> — 讲师（关于 MoE 的形象比喻）

> "Transformer 架构中没有任何随机性，唯一的非确定性来自下一令牌的采样方式。"
> — 讲师

> "温度为零时，你知道输出一定是一样的。温度越高，分布越接近均匀分布。"
> — 讲师

> "在推理时我们受内存限制，而非计算限制。"
> — Shervin（关于投机解码的核心洞察）

## 👤 主要人物

#### 主讲教师（Stanford CME295）

**身份**：斯坦福大学计算数学工程系教授
**背景**：负责 CME295 Transformers 与大语言模型课程，专注于深度学习和大规模模型的数学基础与工程实现
**核心观点**：强调从数学原理到工程实践的完整理解路径，注重学生对 Transformer 架构演进的系统性掌握，特别关注效率优化和实际部署中的技术挑战

#### Shervin（共同讲师）

**身份**：斯坦福大学计算数学工程系讲师/博士后研究员
**背景**：专长于深度学习推理优化和分布式系统，具有丰富的大模型工程化经验
**核心观点**：重点关注 LLM 的推理效率和实际部署，深入分析从提示工程到硬件优化的全栈技术，强调理论与实践的结合

## 📺 视频类型判断

**教程示范**：学术课程形式的技术深度讲解，结合理论推导与实际案例，包含大量数学公式和架构图示

---

## 📝 完整翻译

### (0:00 - 9:25) 大语言模型基础与混合专家系统
> 定义 LLM 并引入 MoE 概念，解决计算效率问题

大家好，欢迎来到 CME 295 的第三讲。

今天是非常激动人心的一天，因为我们终于要介绍大语言模型了。不过在开始之前，我先按惯例说一些通知事项。有些同学希望能在课前拿到课件，所以提醒一下，如果你们想在课件上做标注，现在网站上已经有了。请随时下载。我和 Shervin 会努力定期在每周四晚上把课件发布到网站上，方便大家下载和标注。

好的，那我们开始吧。和往常一样，我们先回顾一下上周的内容。

如果你们还记得的话，第一讲和第二讲都是在介绍自注意力（Self Attention）的概念，并将其与 Transformer 的结构联系起来。上次课我们看了所有类型的模型，以及它们是如何都基于 Transformer 构建的。

主要有三类模型。第一类是编码器-解码器（Encoder-Decoder）模型，它基本上依赖于 Transformer。它包含 Transformer 的编码器和解码器，通常用于文本输入、文本输出的任务。

我们看到的一个例子是 T5 及其所有变体。

第二类模型是从 Transformer 中移除解码器，得到仅编码器（Encoder-Only）模型。

我们深入研究了 BERT，这是典型的仅编码器模型。我们也看到 BERT 有一个很好的特性，就是它的编码嵌入（Encoded Embeddings）对输入非常有意义且具有表达力。我们看到了分类和情感提取的例子。我们具体考虑的是 CLS token 的编码嵌入。

在实际应用中，BERT 用于编码文档和句子，我们稍后会看到这些模型的用处。

最后但同样重要的是第三类模型——仅解码器（Decoder-Only）。我们只保留 Transformer 的解码器部分。在这里我们也做了一点修改，移除了交叉注意力（Cross Attention），因为我们不再需要它了，我们没有编码器。这类模型也是文本输入、文本输出。

GPT 是这类模型的很好例子，实际上现在大多数模型都是仅解码器的。

这就是你们能看到的三种主要模型类型。

到目前为止大家都跟上了吗？

好的，现在我要介绍 LLM 这个术语。LLM 代表大语言模型（Large Language Model）。什么是大语言模型呢？

首先，大语言模型是一个语言模型。语言模型是一个为 token 序列分配概率的模型。

在这种情况下，我们的模型总是预测下一个 token 的概率。在这个意义上，它是一个语言模型。

但大语言模型之所以"大"，是因为这些模型在规模上被大幅放大了。首先是模型大小。现在看到数千亿参数的模型并不少见。

不过，通常当我们说 LLM 时，至少要达到十亿级别的参数。

这些模型也在大量数据上进行训练。这里的数据量是通过它们预训练时使用的 token 数量来量化的。这个数量级是数千亿甚至数万亿 token。

我认为最大的那些模型使用了数十万亿的 token。所以是巨大的训练集。

它们之所以"大"，还因为需要大量计算。通常你需要一堆 GPU 来让它们工作。尽管现在已经有很多优化，让它们能在消费级 GPU 上运行。我们稍后会看到这个。

但 LLM 在这些方面都是"大"的。

我想指出的另一点是，所有这些术语都相对较新。我记得在 2018、2019 年，没有真正的 LLM 定义。我觉得最初没人真正谈论 LLM。也许有人谈论 LLM 时包括了 BERT，但 BERT 是仅编码器模型，不产生文本。

根据当前已经确立的 LLM 定义，BERT 不会被认为是 LLM，因为它不产生文本。所以这里我们只考虑那些文本到文本、在大小、训练数据量和计算方面都非常大的语言模型。

正如我们之前看到的，这些模型是仅解码器的。我们移除编码器，只保留掩码自注意力、前馈神经网络，以及加法和归一化。

我们只保留这些，这就是 LLM 的骨干。

我提到 GPT 是一个很好的例子，但不仅如此，你们还有很多其他模型。你们可能听说过 Meta 的 Llama、Google 的 Gemma、Deepseek、Mistral、Qwen 等等，名单很长。

我想说，粗略估计，超过 90% 的现代 LLM 都是仅解码器的。这是需要记住的一点。

### (9:25 - 36:00) 混合专家系统深入：路由机制与训练
> 详解稀疏 MoE 的路由选择、训练挑战和实际应用

好的，现在你们知道 LLM 是如何构成的了，但现在人们还为这些模型引入了其他东西，我们马上会看到。

我提到这些模型规模巨大，通常有数千亿参数，所以仅仅计算一次推理或训练这些模型就需要大量计算。

但你们可能会想，你真的需要在前向传播中激活所有这些参数来做一个简单的预测吗？

我要用一个小比喻。假设你进入一个房间，房间里有一个数学家、一个物理学家、一个化学家和一个历史学家。你来到这个房间，有一群在各自领域都是专家的人。

你有一个问题，一个数学问题。我的问题是你会问谁？你会问数学家吗？你会问化学家吗？你会问所有人吗？

现在我们问所有人。我们让模型的所有参数都参与生成的计算。

所以这里的想法是，给定一个输入，也许没有必要让所有人都参与计算。所以想法是让模型的一个子集参与下一个 token 的计算。

我刚刚引入了专家（Experts）的概念。假设我们引入以下符号。假设我们有 n 个专家。把它们想象成你的数学家、化学家、历史学家，等等。这些是你的专家，想法是给定输入 X，你要问自己谁应该参与输出的生成。

你要有另一个网络，我们称之为 G，像门控（Gate），但有时也叫路由器（Router）。

假设我们有一些门控告诉我们哪个专家应该参与推理。

如果我们有这个，假设门控告诉我们，专家二号很适合回答你的问题。这里的想法是输入只会流向那个专家，而不是其他专家。

这有一个名字，叫做专家混合（Mixture of Experts）。大家都在谈论这些专家混合。你们会经常看到的公式是这个。输出 y，记作 ŷ，是专家输出的加权和，权重是门控的输出，它告诉你每个专家的输出有多重要。

有个好问题。问题是如何训练 G？如何训练 E？通常你联合训练它们。我们稍后可能会看到这个，但你可以把它想象成正常的训练。你做前向传播，计算损失并反向传播。

这确实是个有趣的问题，因为训练 MoE 会带来一些挑战，我们马上就会看到。

问题是这些 E 是什么，它们的架构是什么？现在假设它们就是一些网络。我们现在不具体说明，但我们马上会看到。现在假设它就像某种网络。

好的，我告诉你们，如果我们不激活所有人怎么办？如果我们激活一个子集怎么办？但这个公式实际上假设我们考虑了所有专家输出。我想区分两种类型。

有一种叫做稠密（Dense）MoE。稠密 MoE 实际上对参与的专家数量没有任何约束。这些权重可以在零和一之间的任何地方。把它们想象成概率分布，但它会对某些专家给予比其他专家更多的权重。

回到我之前的例子。假设我有一个数学问题。我会问数学家、化学家、历史学家。相比历史学家，我可能会对数学家的话给予更高的权重。这就是想法。

### (15:00 - 30:00) Part 2

不，没见过。那么它代表浮点运算（floating point operations）。

它量化了前向传播中涉及多少运算操作，比如加法、乘法等，基本上量化了你的任务需要多少计算量。

通常我们说，当我们使用稀疏 MoE 而不是稠密 MoE 时，我们的 flops 数量会更低。

这就是你会看到的度量单位。

但回到你的问题。那么这些专家是什么？

如果你还记得，我是说 10 分钟前我们说过 LLM 是仅解码器的模型。

那么我有个问题要问你。假设我们想在我们的 LLM 中放入一些 MoE，我们会把它放在哪里？

这里我想我们有三个选择。我们有掩码自注意力层，我们有前馈神经网络，然后我们有这种归一化。那么问题是，你觉得我们应该把它放在哪里？

我想你认为网络中最复杂的部分在哪里？哪里有很多操作？

前馈网络？是的。很好的答案。确实是前馈神经网络。

原因是我想 Shervin 在第一节课中提到过。如果你还记得，前馈神经网络是这样一个网络：你有输入，也就是你的 d 维输入向量，然后它被投影到 d_ff 维空间，然后它又回到 d 维空间。

所以 d_ff 通常比你的输入维度更大。当我说输入时，就是这里。所以它通常更大。

所以你在前馈神经网络中拥有的参数数量大约是 d_model × d_ff × 2 加上一些偏置的数量级。这基本上就是你的数量级。

而如果你想想注意力层，记住它基本上由投影矩阵组成。投影矩阵的维度是什么？所以是 d_model 乘以键的维度、查询的维度、值的维度。

这个维度通常要低得多。想想几百的数量级。

所以你的 d_model 通常是 O(100)，O(1,000) 的数量级，然后这里的投影 d_ff 是 O(1,000) 或者 O(10,000) 的数量级。

很好。那么现在大家都相信这是放置混合专家的好地方了吗？

是的。

很好。这实际上就是实际做法。在现代 LLM 中，这种不让所有人都参与下一个 token 预测计算的想法是这样的：你会把混合专家放在 FFN 的位置，基本上就是这里。

通常你会有一个稀疏的混合专家，意思是回到你的问题，这些专家是前馈神经网络。所以你会有几个可以训练的网络，但你只会激活其中一个。

所以通常 K 等于 1。它也可以等于 2，但只会是一个子集。这就是我的观点。这种路由会在 token 级别完成。

如果你还记得，解码器基本上接收一些输入。你知道，一堆 token，这里我要说的是每个 token 将由一个专家处理，这个专家可能与其他 token 的专家不同。

所以这里的路由器会将 token 的表示作为输入，并找出哪个专家最适合这个 token 流向。

这个想法有意义吗？

我稍后有个小插图，希望能帮助理解。

现在回到你关于如何训练这个模型的问题，比如你是分别训练路由器还是分别训练专家。训练这些模型时人们面临的一个挑战是，要确保所有专家都被使用，都有权重，因为很可能你训练你的模型时，不知怎么只有一两个专家总是被激活，而其他的总是不活跃，从不参与计算。

这个问题叫做路由坍塌（routing collapse）。为什么叫路由坍塌？因为路由器总是选择某些专家而不选择其他专家。

这是一个挑战，人们试图缓解这个挑战的方法是改变损失函数，并添加一些额外项，写在这里。

基本上是某个超参数 alpha 乘以专家数量，再乘以依赖于 token 是否流向某个专家 i 的数量之和，然后对所有专家求和。

你不需要完全理解这个公式是如何工作的，这并不重要。我认为你应该从这张幻灯片中得到的唯一要点是，这个额外的损失允许这些量更趋向于均匀分布的收敛。

提醒一下，这些量是什么？f(i) 是路由到专家 i 的 token 比例，P(i) 是专家 i 的平均路由概率。

当我说所有专家都应该被同样使用时，我的意思是我希望这个概率在各个专家之间是均匀的。

是的。

所以问题是你什么时候计算这些量？你可以把它想象成常规的训练过程，你做一些小批次，通过模型，然后计算所有这些量，然后你基于此进行反向传播。我希望你记住的是，这激励路由器的选择在各个专家之间更加均匀，这缓解了路由坍塌现象。

是的。

是的，所以问题是我们能使用 dropout 吗？当然你总是可以将它与其他技术结合。所以人们发现这非常有帮助。说到其他技术，有一个我没有讲过的东西，它与 dropout 想法非常相似。叫做噪声门控（noisy gating）。

噪声门控基本上是你有来自门控的预测，然后你给它添加一些噪声。

所以基本上纯粹通过偶然性，它让其他专家参与计算。所以这也是其他技术。有很多技术，但是 dropout 对于过拟合之类的事情确实很有用，这个想法可以在不同设置中重复使用。

是的。

是的。

所以问题是你怎么能，你是指可微分的吗？

这里我想你怎么求导数是你的问题吗？

那是怎么回事？你能再解释一下你的担忧是什么吗？

嗯。对于这个问题，平均路由概率，那个是门控输出的函数，对吧，那是 pi，对吧。我想你的问题是对于 fi...

好的，我对 fi 没有很好的现成答案，但我认为人们有一些技术，现在你甚至不需要手工做这个。你有内置的东西。也许我可以在 fi 方面跟进你，但对于 P(i)，你看到这个很清楚吗？它只是来自门控的概率平均值。

是的。所以基本上来自门控的输出概率，你可以把它想象成向量被投影到 n 的空间上，其中 n 对应你的专家数量，然后经过 softmax。所以你的输出基本上加起来等于 1。

这些维度中的每一个都代表与专家 i 被使用相对应的值，例如第一维对应专家 1，第二维对应专家 2，依此类推。你只需取这个的平均值，你可以从所有参数中表达它。所以我认为你对这个应该没问题。

是的。

所以问题是如果我们增加 MoE 的数量，它会增加模型参数的数量吗？这是个很好的问题。这实际上是基于 MoE 模型背后的想法之一，即你可以扩展模型而不必承担在推理时间有显著更多计算的成本。

所以你可以增加，人们说容量，可以增加你模型的容量，但你仍然会保持对活跃参数数量的控制，活跃参数是用于前向传播的参数。所以人们只是使用那个。所以是的，它只是会增加参数数量。

这就是为什么你看到一些基于 MoE 的模型甚至比我们之前拥有的那些更大，达到数千亿的数量级。我们甚至有万亿参数数量级的。例如这里，我推荐的一篇阅读是 Switch Transformer，它扩展到了 1 万多亿参数。

所以是的，肯定更多，但话虽如此，如果你读这篇论文，你也会看到这些模型更加样本高效。

所以它们花费更少的时间来达到与参数数量较少的模型相同的效果。如果你画出训练曲线作为训练时间的函数，你会看到这些模型通常更加样本高效。

抱歉。

是的。是的。确切地。这里的一切都是权衡。这里的一切都是权衡。

是的。很好。是的。

所以问题是每个注意力头都会有一定数量的专家。实际上与注意力头无关。注意力头你可以把它们想象成独立的其他东西，专家的数量与此无关。

有意义吗？

（此部分字幕结束）

### (30:00 - 45:00) Part 3

每个block都会有一定数量的专家。答案是肯定的。而且通常这些权重是不共享的。

实际上我们会看到一个例子。很可能第一层选择的是第三号专家，而第二层选择的是第一号专家。这完全是自由的，是可训练的。

问题是我们如何决定专家的去向。这些都是由gates（门控机制）决定的。

所有的选择都是由具有可训练权重的gates决定的。你可以把它想象成从输入x到n维空间的投影，其中n是专家的数量。

那么问题是在推理的哪个阶段做出决定？假设我们处于推理时间，我来详细解释一下工作原理。

你有输入x，有attention（注意力）机制。它与过去的所有tokens交互，因为这是仅解码器架构。它使用masked attention，在前馈神经网络块开始时，token已经是上下文化的，包含了其他tokens的信息。

X首先进入G。

G计算所有专家的概率分布，在稀疏MoE设置中，我们只选择top K个专家。假设选择top-1，即概率最高的专家。

你会知道选择了哪个专家，然后只计算被选中专家的输出值。

主持人： 你能详细说明一下吗？具体是在什么时候做决定的？

这是在自注意力层之后。

问题是对于不同的头是否有不同的分类？不是的。只有一个路由器。

我理解你的问题。你问的是当多个attention头并行计算时如何处理。记住attention层有不同的头，但最终它会将所有头的结果连接起来，然后再次投影到D模型空间中。

问题是我们是否有不同的G？我只能告诉你G是层特定的。它是层特定且可训练的，会学习如何处理所有这些输入。每一层都有一个G，第一层一个，第二层一个，依此类推。

现在我想给大家展示一个很酷的东西，是Mistral团队在他们论文中展示的。

他们展示了对于给定的文本片段，每个token被路由到哪个专家。

正如我们之前注意到的，专家在不同层之间是不同的。这里是第零层的情况，我们可以看到这些tokens大致均匀地利用了不同的专家。

你不希望看到的是每个token都是相同的颜色，幸运的是并非如此。

### (36:00 - 54:00) 文本生成机制：从贪心到采样
> 对比贪心解码、束搜索和采样策略的优劣

这是表示路由工作方式的一种很酷的方法：输入文本，然后表示每个token去了哪个专家。

我们刚才看到的是现代LLM改变架构的一种方式，它们采用这种方式来处理我们可能想要扩大模型规模但不增加单次前向传播计算复杂度的需求。我们看到了MoE的实现。

你会看到很多基于MoE的LLM，现在我们将专注于——别担心——我们将专注于响应是如何生成的。

记住我告诉过你们，现代LLM的工作方式是接收一些文本输入，然后输出一些文本。这通常是下一个token预测的任务。

你输入一个token，比如句子开始符号，通过LLM，它生成下一个词或下一个token，比如"a"，然后你输入"a"，它输出"teddy"，然后是"bear"等等。

但到目前为止，我们从未真正深入研究如何选择下一个token。

现在我们要了解具体如何生成下一个token。

如你所知，我们的LM只是一个仅解码器架构。这里你有一个解码器，输入在这里，输出在那里。

假设我们知道中间发生的一切，我们得到了输出概率，看起来像这样。

给定一个token或某个token序列作为输入，你得到一个输出概率分布，表示模型认为下一个token等于"a"、"airplane"、"fluffy"等的可能性。

现在我的问题是：如果我告诉你我们有某个序列作为输入，我们想选择下一个token，而我们的模型给出了一个概率分布，你会如何基于此选择下一个token？

学生： 选择概率最大的token。

很好。第一个想法是选择概率最高的token。

这是一个很自然的方法，但我不确定你们是否使用过ChatGPT或Gemini。每次你问问题时，它总是回应略有不同的内容，对吧？

如果你总是选择概率最高的token，而这里的计算是完全确定性的，那意味着无论给定相同的输入，你总是生成相同的东西。这是一个限制——不太多样化。

第二个问题是：如果你在迭代基础上选择最高概率的token，你是局部最优的，但不一定是全局最优的。

这意味着什么？我们的目标是生成一个高概率的输出token序列。但问题是，如果你总是选择最高概率的token，你不一定能获得最高概率的序列。

你相信这个说法吗？让我给你一个例子。

假设下一个token中，一个token概率是0.8，另一个是0.2。你选择0.8的路径。

假设如果你走这条0.8开始的序列路径，假设所有其他token的概率都很低。基本上你会得到一个输出序列，其概率会低于另一条路径，后者在后续步骤中可能有更高的概率预测。

这就是原因。如果你选择最高预测概率，这是一个好的第一想法，但它是局部最优的，不一定是全局最优的。

这就是我们有第二种方法的原因——跟踪K个最可能路径。

我不确定你们是否听说过beam search（束搜索）。这就是beam search的作用。

这里K有时称为beam size或beam width。这些术语指的是我们跟踪的路径数量。

工作原理如下：假设我们从句子开始token开始生成。我们想找出下一个token是什么。假设在这个简单例子中我们有三个tokens，假设两个最高概率的tokens是"a"和"z"。

如果K等于2，我们将跟踪这两个分支。这是第一次迭代。

第二次迭代时，我们查看这两个tokens的下一个token预测的所有概率，我们总是保存两条最可能的路径。这里假设是"the"和"fluffy"，还有"a"和"cute"。

这就是我之前说的。如果你选择沿着最高概率token路径的路径，比如"the"，很可能"the"之后的最高概率token比"a"之后的概率要低得多。

这就是beam search试图做的——寻求更全局最优的解决方案。

### (45:00 - 1:00:00) Part 4

因此，如果你添加更多概率低于1的项，这个量我认为会趋向于零。

所以我想这个方法本身会倾向于优先选择较短的序列。因此，beam search有一些额外的项来基本抵消这种效应，比如1除以token数量的某次幂之类的。在实践中有一些技术来确保这些东西能相对正常工作。

但是，假设我们解决了所有这些问题，问题是我们需要跟踪这个最可能的路径。我们需要做所有这些保存等工作，这需要大量的计算。

另一件事是我们仍然只关注最可能的路径，这基本上会导致一个模型认为非常可能的序列，但有时你想要的是让输出更多样化或更有创造性。

这就是为什么beam search实际上不是人们通常使用的方法。人们在机器翻译等任务中使用beam search，在这些任务中你确实需要有一个非常接近高可能性的结果，但实际上人们使用第三种方法，这种方法也叫做采样方法。

我告诉过你我们有一个关于下一个token应该是什么的概率分布。所以人们所做的就是使用这个概率分布来采样下一个token。这有道理吗？

在这个例子中，fluffy、gentle、kind，我们假设smart会有更高的概率被抽取，而不是airplane和wear这样具有较低出现概率的词。它们有非零概率，但概率较低。

到目前为止有任何问题吗？

学生：这不是用于训练，而是用于推理，对吗？

正确。你可以把它想象成响应生成。假设你有一个已经训练好的模型，你想要生成一个输出。那你会怎么做？

好的，我来完成我的回答。在训练期间，你会关心输出概率，然后将它们与实际标签进行比较，大多数时候这是一个硬标签。这就是你要比较的内容。所以这个是假设你有一个已经训练好的LLM，你如何生成响应？

每个人都明白了吗？

学生：在这种情况下你如何进行采样？

实际上这是我下一张幻灯片要讲的内容，但我只是想确保大家对直觉都在同一个页面上。

所以最高概率称为贪心解码，这可能不是我们想要的。Beam search稍微好一点，它更接近全局最优。它不是全局最优，但更趋向于那个方向。所以它更好，但缺乏多样性，缺乏创造性，这就是为什么我们想要做的实际上是对每个token进行采样。

我们有一些方法也会限制我们想要采样的token类型，因为我提到了那些非常低概率的token，理论上它们仍然可以被采样，但这不一定是我们想要的。

所以人们通常做的是限制最高概率的token，只从它们中采样。你可能听说过top-K采样这个术语。有谁听说过这个术语？好的，有一点。

你所做的是选择概率最高的K个token，然后从中采样。假设如果K等于4，你取概率最高的4个token，然后从中采样。还有一个在概念上非常相似的方法叫做top-P。

Top-P是你将自己限制在概率最高的token中，使得它们的累积概率大于阈值P。

它会做同样的事情，选择概率最高的token。

但是我还有一部分没有讲到，就是你如何首先获得这些概率。

如果你记得，这里我提到我们只是假设我们有概率，我们想选择下一个token是什么。

但现在我要问的问题是，既然我们知道要用这些概率做什么，一个问题是你如何首先获得这些概率？

如果你记得，这些基于transformer的架构基本上计算输入的编码表示。你有一些最终层在图的最顶部，旨在将向量投影到词汇表的空间中，因为你想要做的是为采样给定token的概率有一个概率数。

在架构的最顶部，你的输入是token的编码嵌入，通过一个线性层，基本上将你的d_model向量投影到维度为词汇表大小V的空间中。

当然，你想要的是概率。所以你会有一个softmax层，基本上将所有东西转换为概率，使得所有东西加起来等于1。

为了计算概率，这是你会使用的公式，即softmax层。有一个非常重要的超参数我想和你们谈论，就是温度T。

这就是它出现的地方。下一个token是给定词的概率等于该给定词输入的指数除以温度，然后你通过所有其他指数量除以t的和来归一化。

现在我们要看到的是这个t在实践中用于什么，或者这个t实际上做什么。

在我们深入之前，有一个问题：有人在响应生成方面听说过温度吗？

很好。希望这能让你更好地理解温度如何影响你的输出预测。

我想问你的问题是，低温度与高温度相比会有什么影响？

低温度会对应多样化。为什么？

回到你关于降低温度会产生什么影响的解释。假设你有一个较低的温度，会发生什么？

### (54:00 - 1:07:00) 温度控制与引导解码技术
> 分析温度参数对概率分布的影响和格式化输出

简而言之，低温度会创建一个尖峰分布，高温度会创建一个均匀分布。我想你有正确的直觉。

让我用更数学的术语来写这个。

假设你有概率，它是$e^{x_i/T}$除以所有$e^{x_j/T}$的和。

从数学上你实际上可以证明这里发生了什么。假设你有最高$x_i$的索引，我们称之为K。

假设我要用这个量来因式分解。它将是$e^{x_k/T}$，然后这里你有所有j的$e^{(x_j-x_k)/T}$的和。

我所做的是用$e^{x_k/T}$乘以分子和分母。当然它们会约掉。但是我这里有$(x_i-x_k)/T$，这里有$(x_j-x_k)/T$。

当i等于k时，这一项是零，对吧？无论温度如何，它都恰好是零。

这里$x_j - x_k$总是负的或零。

如果i等于k，你有$0/T$，所以这里是1。然后你有$1 +$某个量，$(x_j-x_k)/T$将是负的。

当t趋近于零时，那个会趋向于负无穷，$e^{-\infty}$是零。

所以你最终得到的是，对于i等于k，你会有一个非零概率，但对于i不等于k，你会有零除以不等于零的东西，结果是零。

从数学上，如果你用$e^{x_k/T}$因式分解，其中k是最高值的索引，最高logit或最高激活向量，你实际上可以证明，对于小温度，只有最高值的索引会是概率最高的，在那里有一个尖峰。

当你有高温度时，它基本上看起来像均匀分布，因为当t趋向于正无穷时，这趋向于零，所以$e^0$等于1。它是1除以J的数量，所以就是词汇表中所有token的均匀分布。

学生：我如何解释一个小的...

[内容继续，但由于已达到章节时间范围上限，翻译结束]

### (1:00:00 - 1:15:00) Part 5

学生：我如何将一个小的数值解释为...

教授：这个问题是能否将其解释为高斯分布？我认为可以，但有些困难，因为高斯分布需要连续的量作为 x 轴。这里我们处理的是 token，是离散的，无法排序。所以可能应该这样理解：低 temperature 会产生非常尖锐的分布。你会有一组概率最高的 token 更频繁地出现。

如果你设置高 temperature，那么即使是原本概率不高的 token 也会获得调整后的更高概率。可以将其视为某种缩放效应。简而言之，我想说的是，如果你使用低 temperature，会鼓励下一个 token 趋向于最高概率的 token。

如果你使用高 temperature，概率分布会更接近均匀分布（如果你大幅增加 temperature 的话）。所以你的输出会更有创意，会抽取到一些原本不会被选中的 token。

实际应用中，假设你在使用你最喜欢的 LLM，想要写一些非常有创意的内容。你会使用低 temperature 还是高 temperature？

学生：高 temperature。

教授：没错。如果你想要更确定性的、更高质量的输出，你会使用更低的 temperature。这里有个重要点：如果你使用严格正数的 temperature，每次对同一个输入运行模型，你会得到不同的输出。

我想指出的是，Transformer 架构中没有任何部分是概率性的，一切都是确定性的。唯一不确定的是如何采样下一个 token，这是唯一非确定性的部分。

那么如何获得确定性输出呢？

学生：设置 t 等于零。

教授：t 等于零时，你的输出肯定只有一种结果。这是理论上的性质。

但实际上，计算过程中会发生一些引入非确定性操作的情况，这远超出了本课程的范围。我只是想指出，实际中 t 等于零可能因为这些实际因素导致不同结果。所以我推荐有个可选阅读材料，是关于"defeating nondeterminism in LM inference"的最新文章。

### (1:07:00 - 1:22:00) 提示工程：上下文学习策略
> 零样本、少样本学习和链式思维推理技术

高层思路是：我们的 GPU 和硬件在执行这些操作时，有时会对完全不同尺度的数字进行归约。操作发生的顺序实际上非常重要。如果这些操作以不同顺序发生，可能导致不同结果。即使理论上应该完全相同，实际中可能不同。这篇文章很好地解释了直觉原理。

好的，我知道有点晚了。最后我想说的是，假设你想生成非常特定格式的输出，比如 JSON 格式。

一个简单方法是告诉 LLM "请以 JSON 格式生成"，然后它产生某些内容，你检查是否为有效 JSON，如果不是就重复，直到生成正确格式。这是第一种简单方法。

第二种方法叫做 guided decoding（引导解码）。在生成过程中，它会过滤掉所谓的无效下一个 token。

假设我想生成这个 JSON，我确定第一个 token 必须是打开括号的符号。然后只能是属性名，依此类推。有时你可能有多个允许的下一个 token，这时会回到我们的下一 token 策略。

学生：如何限制其他 token？

教授：有很多论文涉及这个问题。我们不会在这里详述，但我可以给你一些指导。搜索"finite state machine FSM context grammar"，有很多相关论文，但我们这里不会覆盖。

好的，我想我们现在可以转到第二部分，由 Shervin 来讲解。

Shervin：谢谢。

现在我们一起来看看不同的 prompting 策略。既然我们知道了如何生成响应，我们来看看如何获得响应，如何获得优秀的响应。

让我们回到我们最喜欢的可爱泰迪熊例子。我想介绍一个词汇概念。当你有某种输入时，输入的长度用 token 数量来衡量，在文献中你会看到不同的名称。

可以叫做 context length（上下文长度）、context size（上下文大小）、window size（窗口大小），我认为最新的工具如 Cursor 或其他代码辅助工具更多称之为 context length，但这些术语都是等价的，指的是同一个概念。

现在我想花些时间讨论这些数量级的概念。现代 LLM 通常在数万、数十万或数百万输入 token 的量级。这是它们能处理的输入类型。有时你会听到新 LLM 宣传某个数字，那个 context length 数字就是指这个，即在单次处理中能容纳多少 token。

### (1:15:00 - 1:30:00) Part 6

基本上除了输入查询之外，你不提供任何其他内容，你只是要求 LLM 做你想让它做的事情。然后还有另一种思路叫做 few-shot learning（少样本学习），在这种方法中，你在询问感兴趣的输入之前，先给 LLM 提供输入和输出的示例。

以泰迪熊和睡前故事为例，你可以给泰迪熊命名。比如你有一只叫 Teddy 的泰迪熊，然后你生成了一个故事。你放入查询，放入你想生成的故事，并给出多个这样的示例。假设你有另一只叫 Bob 的泰迪熊，你想为 Bob 生成故事。你放入所有这些示例，然后要求 LLM 生成你感兴趣的故事，这基本上就是我们所说的 few-shot 设置。

好的，很棒。通常当你查看给定任务的性能时，给出示例往往能很好地引导 LLM 完成你感兴趣的任务，因为你给了模型一个关于你在寻找什么的概念，然后它可以使用在训练过程中学到的知识来连接这些点，基本上进行复制。

但是当然，你需要收集这样的示例，这是有成本的，这会在 context window（上下文窗口）中放入更多 token，所以你需要更多计算资源。但这里有一个有趣的权衡，我们在最近的模型中看到了这一点。它们获得了越来越多的推理能力，这就是为什么我在这里加了"通常"这个细微差别。

它们通常在 few-shot 方面更好，但并不总是如此，因为现在随着这些更好的模型，人们发现通过基本上改进指令，你可以使上下文学习的性能与提供示例的性能相当甚至更好。因为基本上当你想到它时，当你提供示例时，你将模型限制在给定的有限示例集中。

所以当你在推理时想要在一个未见过的数据分布上执行任务时，模型更难泛化，因为它会试图与在上下文中看到的内容保持一致。而如果你将指令转换为更基于推理的东西，用自然语言解释如何完成任务，它可以使用推理能力来实际完成任务。

这是你现在越来越多看到的事情。关于这方面的文献我认为仍在进展中，但你有一些论文，比如最近出现的"plan and solve"，它显示如果你要求 LM 先规划自己然后解决问题，它可以有很好的性能。所以这是一个有趣的事实。

好的，很棒。现在我们已经看到了基本上我们拥有的学习类型，接下来我们要看看我们能做什么来提高响应质量。有一个叫做 chain of thought（思维链）的概念，研究人员发现，如果你强制模型在实际给出答案之前先想出一些理由，它会给出更高的性能。

这就是人们所说的 chain of thought，基本上是引导你得出答案的过程。例如，如果你问泰迪熊多大了，如果你强制模型直接用数字回答，它可能无法准确连接为什么泰迪熊是给定的年龄。而当你给出完整的推理链时，就会清楚地知道是什么导致了那个响应。我认为这是这种技术背后的主要思维模式。

基本上这是可以在 in-context learning 中使用的东西。如果你给出 few-shot 示例，比如这只泰迪熊多大了，你给出一些数字，然后你稍微改变查询，基于推理格式和响应，LLM 可以相应地调整推理和响应。

它会强制模型与响应一起输出一些推理，这显示了基本指标的改进。

好的，很棒。然后我想说的另一件事是，假设你想做一项任务并且想做好它，通常你总是会有一些不起作用的样本，对于这些样本你需要调试能力，对吧？通常当你用 LLM 调试时，你不像以前那样调试，你不查看权重或 LLM 的内在特征。你想要的是从 token 角度出来的更可解释的东西。


---

*生成时间: 2026-01-03 16:43:21*
*由 YouTube Monitor & Translator (Claude CLI) 生成*