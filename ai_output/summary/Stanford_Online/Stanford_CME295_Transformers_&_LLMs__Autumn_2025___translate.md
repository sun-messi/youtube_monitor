# Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 9 - Recap & Current Trends

## 📹 视频信息

- **频道**: Stanford Online
- **发布日期**: 2025-12-09
- **时长**: 1:51:25
- **原始链接**: [https://www.youtube.com/watch?v=Q86qzJ1K1Ss](https://www.youtube.com/watch?v=Q86qzJ1K1Ss)

---

本文内容整理自斯坦福大学 CME295 课程讲师艾芬（Afin）和谢尔文（Shervin）在 Stanford Online 频道的课程总结讲座。

## TL;DR

斯坦福 CME295 期末课程：从 Transformer 基础架构到大语言模型的完整技术回顾，涵盖预训练-微调-偏好调整三阶段流程、推理能力训练、RAG 与工具调用、模型评估，以及 2025 年的前沿趋势包括视觉 Transformer、扩散模型在文本生成中的应用等新兴技术方向。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-10:32 | Transformer 架构基础回顾 | 从文本标记化、嵌入表示到自注意力机制的完整技术演进 |
| 10:32-15:20 | LLM 扩展技术与解码策略 | 专家混合模型、温度采样等大规模模型优化技术 |
| 15:20-29:53 | 训练流程：预训练到偏好调整 | 三阶段训练的完整技术栈，重点解析 RLHF 和奖励建模 |
| 29:53-42:54 | 推理能力与 GRPO 算法 | 从思维链到 GRPO 的推理训练技术突破 |
| 42:54-59:17 | RAG 与智能体架构 | 检索增强生成、工具调用和现代智能体工作流程 |
| 59:17-1:10:13 | 多模态 Transformer 扩展 | Vision Transformer 和视觉语言模型的技术实现 |
| 1:10:13-1:23:38 | 扩散模型在文本生成的应用 | 从图像扩散到掩码扩散模型的并行文本生成范式 |
| 1:23:38-1:51:25 | 2025 技术趋势与未来展望 | 架构优化、数据策略、硬件创新与应用前景分析 |

## 📊 核心论点

#### Transformer 自注意力机制的核心创新

- **核心内容**：自注意力机制通过 Query-Key-Value 框架，让序列中的任意两个 token 都能建立直接连接，彻底解决了 RNN 的长距离依赖问题。数学公式 softmax(QK^T/√dk)V 实现了高效的并行计算，成为现代硬件的最佳适配架构。相比 Word2Vec 的静态嵌入，Transformer 提供了真正的上下文感知表示。
- **关键概念**：自注意力、查询-键-值机制、位置编码、上下文感知嵌入、并行计算
- **实际意义**：奠定了从 BERT 到 GPT 系列所有现代 LLM 的技术基础；使得大规模并行训练成为可能；彻底改变了 NLP 领域的技术路线，从循序处理转向全局注意力模式。

#### 三阶段训练流程：从语言建模到人类偏好对齐

- **核心内容**：现代 LLM 训练分为预训练（数万亿 token 的下一词预测）、监督微调（教授具体任务格式）、偏好调整（通过 RLHF 注入人类价值观）三个阶段。预训练学习语言结构，SFT 学习"做什么"，偏好调整学习"不做什么"。每个阶段解决不同层面的问题，缺一不可。
- **关键概念**：预训练、监督微调、偏好调整、RLHF、奖励建模、Bradley-Terry 公式
- **实际意义**：确保 LLM 不仅具备语言能力，还能遵循人类指令并体现正确价值观；成为所有生产级 LLM 的标准训练范式；解决了早期模型"能力强但难控制"的核心问题。

#### GRPO：无价值函数的策略优化新范式

- **核心内容**：GRPO 通过生成多个候选答案并比较奖励，取代了 PPO 中复杂的价值函数训练。特别适用于数学推理等具有可验证奖励的任务，大幅简化了训练复杂度。通过相对优势计算避免了价值函数的偏差问题，同时降低了计算成本。但原始 GRPO 存在长度偏差，催生了 GRPO-DT 等改进版本。
- **关键概念**：GRPO、价值函数、可验证奖励、相对优势、长度偏差、策略优化
- **实际意义**：降低了推理模型训练的技术门槛和计算成本；推动了 OpenAI o1、DeepSeek 等推理模型的发展；为数学、代码、科学推理等垂直领域提供了专门的训练范式。

#### RAG：外部知识与内部推理的深度融合

- **核心内容**：RAG 通过双编码器检索+交叉编码器重排序的两阶段流程，让 LLM 能够访问实时和专业知识。检索过程包括候选召回（语义相似度计算）和精确排序（上下文相关性评分），最终将 Top-K 文档注入提示词。解决了知识截止期限制和幻觉问题，特别适合需要准确性的应用场景。
- **关键概念**：双编码器、交叉编码器、语义检索、重排序、知识截止、检索增强
- **实际意义**：成为企业 AI 应用的核心架构模式；解决了 LLM 知识过时和领域知识不足的问题；催生了向量数据库、检索优化等新兴技术生态。

#### Vision Transformer：跨模态架构迁移的成功典范

- **核心内容**：ViT 将图像切分为固定大小 patch，通过线性投影转换为 token 序列，直接使用 Transformer 编码器进行分类。在大规模数据上，ViT 超越了传统卷积网络，证明了自注意力机制的通用性。关键创新是降低了归纳偏置，让模型从数据中学习最优的视觉模式，而非依赖人工设计的卷积操作。
- **关键概念**：图像 patch、归纳偏置、跨模态迁移、自注意力通用性、大规模预训练
- **实际意义**：开启了多模态 Transformer 时代；推动了 DALL-E、CLIP 等视觉-语言模型发展；证明了 Transformer 架构的领域无关性，为统一架构奠定基础。

#### 扩散模型在文本生成中的并行化突破

- **核心内容**：文本扩散模型将图像生成的噪声-去噪过程转换为掩码-去掩码过程，从全掩码序列开始并行预测所有 token，而非逐个生成。推理步数固定且通常远小于序列长度，理论上可实现 10 倍速度提升。特别适合代码补全等"填空"任务，提供了自回归之外的全新生成范式。
- **关键概念**：掩码扩散、并行生成、固定步数推理、填空任务、非自回归生成
- **实际意义**：为高实时性应用（如代码助手）提供低延迟方案；挑战了自回归生成的垄断地位；推动了 Inception、Google 等公司在快速推理方向的技术竞赛。

#### 小语言模型：从规模竞赛到效率优先

- **核心内容**：随着基准测试趋于饱和，行业重心从"更大更强"转向"更小更快"。小语言模型（SLM）通过精心设计的训练策略和推理优化，在保持高质量的同时大幅降低部署成本。现实中许多 LLM 提供商在最高价格档位仍处于亏损状态，推动了对成本效益的关注。
- **关键概念**：小语言模型、成本效益、推理优化、模型压缩、边缘部署
- **实际意义**：降低了 AI 应用的门槛和成本；推动了端侧部署和隐私保护；为中小企业和个人开发者提供了可行的 AI 解决方案。

#### 专用硬件：从通用 GPU 到 Transformer 原生架构

- **核心内容**：现有 GPU 主要为矩阵乘法优化，但 Transformer 的 QK^T 计算、注意力权重归一化等操作并不高效。新兴的模拟信号硬件直接将注意力计算嵌入物理电路，利用基尔霍夫定律等物理原理实现"零额外计算"的注意力操作。在模拟环境中展现了显著的延迟和能耗优势。
- **关键概念**：专用硬件、模拟计算、物理嵌入、注意力加速、能耗优化
- **实际意义**：为大规模 AI 部署提供了能耗可持续的解决方案；推动了 AI 芯片设计的范式变革；为实时推理应用提供了硬件加速基础。

#### 数据策略：从互联网爬取到精细化策展

- **核心内容**：早期 LLM 受益于大规模人类生成的互联网数据，但如今搜索结果 80% 都是 LLM 生成内容，导致"模型坍塌"风险。业界转向数据策展策略：预训练→中间训练→微调的三阶段流程，其中中间训练使用高质量精选数据集。同时出现了专业的数据策展公司和更精细的质量控制流程。
- **关键概念**：数据策展、模型坍塌、三阶段训练、中间训练、数据质量、人机生成内容比例
- **实际意义**：保证了未来 LLM 的训练质量和多样性；催生了数据标注和策展的新产业；推动了对数据来源和质量的更高标准要求。

#### 持续学习：从静态权重到动态适应

- **核心内容**：当前 LLM 采用"训练-冻结"模式，通过 RAG 和工具调用获取新信息，但无法真正学习和记忆新知识。持续学习要求模型能在保持已学知识的同时不断吸收新信息，避免灾难性遗忘。这需要在架构、优化算法和训练范式层面的根本创新，是下一代 AI 系统的关键挑战。
- **关键概念**：持续学习、灾难性遗忘、动态权重、增量学习、知识保持
- **实际意义**：将实现真正"终身学习"的 AI 系统；减少重新训练的巨大成本；为个性化和适应性 AI 应用奠定基础。

## 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Stanford Online | 课程主办方，CME295 课程平台 | ⭐⭐⭐ |
| OpenAI | GPT 系列模型、ChatGPT、o1 推理模型的核心开发者 | ⭐⭐⭐ |
| Google/DeepMind | Transformer 原始论文、Attention is All You Need、Vision Transformer、文本扩散模型研究 | ⭐⭐⭐ |
| Meta | LLaMA 系列模型、Yann LeCun 的雇主、视觉语言模型研究 | ⭐⭐⭐ |
| Anthropic | Claude 模型系列、RLHF 技术贡献 | ⭐⭐ |
| Inception | 文本扩散模型的商业化先锋公司 | ⭐⭐ |
| Hugging Face | 模型开源平台、Transformers 库、Trending Papers | ⭐⭐ |
| NVIDIA | GPU 硬件提供商、AI 训练基础设施 | ⭐⭐ |
| DeepSeek | 推理模型和 OCR 创新、小语言模型研究 | ⭐⭐ |
| Papers with Code | 学术资源平台（已被 Hugging Face 替代） | ⭐ |

## 💬 经典金句（3-5 句）

> "现在的问题是，推理时生成实际上不能并行化，因为你总是需要之前的内容来预测下一个token。"
> — 艾芬

> "扩散模型的关键优势是解码现在只需要更少的前向传播次数，因为以前你必须做和要预测的token一样多次的前向传播。"
> — 艾芬

> "我觉得知道如何使用LLM来加速你日常生活中所做的一切已经变得非常重要。"
> — 谢尔文

> "也许明天你会有某种证书来保证网站对AI助手浏览是安全的。"
> — 谢尔文

> "按照设计，LLM会'幻觉'，因为我们训练LLM预测下一个token，而不是将陈述映射到事实。"
> — 谢尔文

## 👤 主要人物

#### 艾芬（Afin）

**身份**：斯坦福大学 CME295 课程主讲师
**背景**：深度学习和大语言模型专家，专注于 Transformer 架构、训练算法和推理优化研究。在斯坦福教授变换器和大语言模型的前沿课程。
**核心观点**：强调 Transformer 架构的通用性，认为自注意力机制不仅适用于文本，还能成功迁移到图像等其他模态。看好扩散模型在文本生成中的潜力，特别是其并行化优势能显著提升推理速度。

#### 谢尔文（Shervin）

**身份**：斯坦福大学 CME295 课程共同讲师
**背景**：AI 系统和应用专家，专注于模型评估、智能体架构和 AI 工程实践。曾经是在线学习者，深刻理解远程教育的价值。
**核心观点**：重视 AI 技术的实际应用价值，强调从规模竞赛转向效率优先的行业趋势。关注 AI 助手的用户体验和实用性，认为未来突破将来自成本效益的优化而非纯粹的性能提升。看好 AI 在教育和学习辅助方面的巨大潜力。

## 📺 视频类型判断

**教程示范**：技术教学、学术总结

---

## 📝 完整翻译

### (0:00 - 10:32) Transformer 架构基础回顾
> 从文本标记化、嵌入表示到自注意力机制的完整技术演进

大家好，欢迎来到 CM295 的第 9 讲。如你们所知，今天是特殊的一天，因为这是整个课程的最后一讲。今天的内容安排与平时会有所不同。

我们将把今天的讲座分为三个部分。第一部分，我们会回顾整个课程中学到的内容，看看不同知识点是如何结合在一起的。第二部分，我们会探讨 2025 年的一些热门话题以及我们认为在不久的将来会成为趋势的内容。第三部分将更多地用于总结，并为大家提供后续学习的方向。

听起来怎么样？

好的。那么我们从第一部分开始，也就是我刚才提到的回顾整个学期所学内容的部分。这里没有新知识，只是把所有内容串联起来。

如果你们还记得的话，大概 10 周前的第一讲重点讲解了什么是 Transformer。在课程开始时，我们甚至不知道如何处理文本。第一步是分词（tokenization），即将输入分解为原子单位。我们将文本分解的方式在某种意义上是任意的。我们有不同的算法来实现这一点，并且看到最常见的分词算法是子词级分词器（subword level tokenizer）。

我们发现其中一个优势是词根可以被重复使用和利用，特别是在表示这些 token 时。

说到表示，一旦我们能够将输入文本分解为原子单位（即 token），下一步就是学习如何表示这些嵌入。如果你们还记得的话，我们看到了一些当时非常流行的方法。其中一种叫做 Word2Vec，表示是从代理任务中学习的，比如预测中心词或预测上下文词。

但我们发现这种学习表示的方法有一些局限性。其中一个是这些表示不具备上下文感知能力。意思是如果一个词出现在某个句子中，或者在另一个句子中，它们在两个句子中都会有相同的表示。

因此我们看到了一些在 2010 年代流行的其他方法。其中一种是 RNN（递归神经网络）。RNN 具有这种递归结构，逐个处理 token 并保持序列的内部表示。

但我们发现了一个重大局限性，就是长距离依赖问题，特别是在过去很远位置编码的 token 无法随着序列变长而保持下去。

这就是为什么我们看到了整个课程的核心思想——自注意力（self attention），其中 token 可以相互关注，无论它们在序列中的位置如何。你可以把这看作是一个直接连接。

这就是我们所看到的。我们发现人们使用三个主要术语：查询（query）、键（key）和值（value）。通常你想知道查询与序列中的键有多相似，通过计算一些缩放和 softmax 的点积来量化，然后取相应的值。最终我们得到序列中所有 token 的某种加权平均。

你们现在可能也很熟悉这个公式了：softmax(QK^T/√d_k) × V。这是我刚才提到内容的矩阵表述，能够以非常高效的方式处理这些计算，也是今天的硬件能够很好地支持的。

然后我们通过现代 LLM 基础架构——Transformer 完成了第一讲，我们看到 Transformer 有两个值得注意的部分。左边部分是编码器（encoder），右边部分是解码器（decoder），我们看到了这在翻译情况下是如何应用的。

第一讲结束时，我们了解了促使我们最终采用 Transformer 的动机，并看到 Transformer 在翻译方面表现得相当好。

在下一讲中，我们看到了自 2017 年发布以来，人们对这个架构所做的一些小改进。

人们做出的一个特别改进是我们考虑位置的方式，因为在原始 Transformer 论文中，位置是以绝对方式编码的，即每个位置都有自己的嵌入，这个嵌入被加到 token 嵌入上。

但如果我们想想，位置方面我们实际上并不关心绝对位置，我们关心的是 token 之间的相对位置，特别是在自注意力计算中 token 的距离有多远。

这就是为什么我们看到了现在相当流行的方法叫做旋转位置嵌入（Rotary Position Embeddings，简称 RoPE），现在被广泛使用。这是一种旋转查询和键的方法，两者都在自注意力计算中发生。

这里量化的纯粹是两个 token 之间相对距离的函数，不仅如此，它是在我们关心的自注意力层中处理的。

这是一个重大改进，然后我们看到了一些其他改进，特别是在多头注意力层的组成方面，我们发现可以对我们学习的矩阵进行一些分组。所以我们不需要为每个头都有一个矩阵，比如为键和值各有一个投影矩阵，我们实际上可以将它们分组。这就是这里提到的分组查询注意力（Group Query Attention）。

我们还看到了一些其他技术，比如 Transformer 中的归一化层，这里在每个子层之后进行，但现在人们尝试将归一化部分移到子层之前。这里是后归一化（post-norm）版本，而在子层之前的部分称为前归一化（pre-norm）版本。

我们看到的最后一点是，从这个 Transformer 架构中，有很多基于它的衍生模型。我们看到如果我们只保留编码器部分，我们可以计算非常有意义的嵌入。

如果你们还记得的话，有一篇关于仅编码器模型的里程碑式论文，就是 BERT，它在分类任务中被大量使用，因为它依赖于 CLS token 的编码嵌入。这是其中一种。

但我们也看到还有许多其他类型的模型，都或多或少衍生自 Transformer。你可以只保留编码器（如 BERT），只保留解码器（如 GPT），或者两者都有（如 T5）。

### (10:32 - 15:20) LLM 扩展技术与解码策略
> 专家混合模型、温度采样等大规模模型优化技术

这些模型的一个特殊方面是，仅编码器模型按我们看到的方式无法生成文本，但能够生成可用于下游任务的嵌入。但编码器-解码器模型（如 T5）或仅解码器模型（如 GPT）可以进行自回归并生成文本。范式可以是文本输入文本输出。

然后我们专注于现在大家都称为大语言模型（Large Language Models）的内容，这些是基于 Transformer 的模型，特别是文本到文本模型。所以是仅解码器的基于 Transformer 的模型，我们看到人们现在想出了很多新技巧，因为正如名称所示，人们已经将这些模型扩展了。

但接下来抛出了一个问题：你真的需要所有这些参数来只做一次前向传播吗？

所以我们看到了一种基于专家混合（Mixture of Experts）的变体。专家混合是什么呢？不是通过整个完整模型运行所有内容，而是你将拥有许多专家，你将以稀疏的方式激活它们。

例如，对于一个输入，你将只激活一个子集，然后对于另一个输入，你将激活另一个子集，这样你就不需要一直进行所有计算。

我们看到这些专家混合在 LLM 中使用，特别是在前馈神经网络层中。这里你会有专家作为不同的前馈神经网络，你会有一个门控机制将输入路由到正确的前馈神经网络。

我们还看到一些论文能够产生一些很好的可视化，显示哪个 token 被路由到哪个专家，因为这种重新路由是在 token 级别完成的。

在 token 级别进行的一个原因是能够智能地将专家放在不同的硬件上，不同的 GPU 上，然后更好地并行化计算。

我们还看到这些 LLM 总是被赋予预测下一个 token 的任务。为了预测下一个 token，我们感兴趣的是我们如何做到这一点。人们使用的一种特殊方法就是从输出分布中采样。给定输入，你有一个模型输出的下一个 token 的概率分布。你所做的不是采取最高概率（称为贪婪解码），而是实际采样。

这引入了一些随机性，允许模型产生更大的输出变化。我们看到你可以通过调整称为温度（temperature）的超参数来调整你想要的输出变化程度。非常低的温度导致非常尖锐的分布，更具确定性的输出，而较高的温度更随机，更有创造性。

### (15:00 - 30:00) Part 2

好的。所以到目前为止我们了解了什么是 LLM，它们是如何基于 Transformer 架构的，以及它们如何连接到我们在第一讲中看到的架构。然后在第四讲中，我们看到了人们实际上是如何训练这些 LLM 的，因为正如我提到的，这些 LLM 很大，所以你不能天真地将它们放在你的硬件中，你需要对此聪明一点。

特别是，人们在 2020 年代早期注意到的是，你的模型越大，你的性能就越好。所以人们就开始构建越来越大的模型。在这个图示中我们看到，y 轴是测试损失，越低越好。我们看到你使用的计算越多，你的测试性能就越好，增加数据集大小也是如此，增加参数数量也是如此。

### (15:20 - 29:53) 训练流程：预训练到偏好调整
> 三阶段训练的完整技术栈，重点解析 RLHF 和奖励建模

但是计算并不是无限的。所以社区中自然出现了一个问题：如果给你一个给定的预算，一个给定的计算预算，你能否选择某种最优的参数数量和数据集大小来训练你的模型。

所以我们看到有一篇论文在 2020 年代早期发表，实际上研究了数据集大小和模型大小以及在测试集上性能之间的关系。然后我们看到实际上当时大多数模型都是我们所说的训练不足的，因为相对于它们训练的数据集来说，它们太大了。它们训练的数据集没有应该的那么大。

特别是，从中产生了一个经验法则：如果你的模型有给定数量的参数，你应该至少在 20 倍参数数量的 token 上训练它。

例如，如果你有一个 1000 亿参数的模型，你应该至少在 2 万亿 token 上训练它，因为 2 万亿等于 1000 亿乘以 20。这是人们使用的经验法则。

正如我之前提到的，这些模型是巨大的。所以人们也试图让计算更高效，因此有一种实际上非常重要的方法叫做 Flash Attention。

Flash Attention 是一种利用底层硬件优势的方法，特别是它关注 GPU 具有的内存类型。它有大但慢的内存和小但快的内存，分别是 HBM 和 SRAM。

我们看到这种方法试图最小化对大而慢的内存（HBM）的读写次数。它的做法是将计算分成小块，发送到小但快的内存（SRAM），这样它就可以进行端到端的计算，然后发送回原来的位置以完成完整的端到端计算。

这种方法是精确的，意味着我们没有对结果做任何近似，但它带来了显著的加速。论文中还有第二个重要想法：有时候你可以不存储结果，可以将它们丢弃，然后在再次需要时重新计算。这就是重新计算的想法，使用我描述的方法，即使我们做了更多的计算，也能获得更快的运行时间。

这就是 Flash Attention。我们还看到了许多其他旨在并行化计算的方法。我们看到了数据并行，这是一种不让所有数据在单个 GPU 上处理，而是将它们分配到多个地方的想法。

然后我们有第二种方法叫做模型并行，即使对于给定的前向传播，你实际上也会涉及多个 GPU。

无论如何，有很多非常有趣的技术，很多关于如何以有效方式训练这个模型的不同想法。特别是，我在这里描述的主要对 LLM 训练过程的第一步很重要，称为预训练，旨在教模型语言结构和代码结构。特别是这个模型用大量数据进行训练，想想万亿 token 甚至数十万亿 token。

这第一步从一个初始化的模型到一个能够自动完成的模型，因为它是用预测下一个 token 的目标进行训练的。

在这第一阶段结束时，你有一个知道如何自动完成的模型，但你有一个不是很有用的模型，因为它只知道如何完成事情。

为了让模型对我们的用例有用，我们有第二步称为微调步骤，我们在其中教模型我们希望它表现良好的输入输出对的类型。这也被称为 SFT 阶段，即监督微调阶段。在这第二步结束时，我们有一个不仅知道文本和代码结构，而且能够按照你想要的方式行为的模型。

但到目前为止，直到第二步，我们只教会了我们的模型该做什么。我们还没有教它不该做什么。这就是为什么我们有第三步，偏好调优步骤，我们取经过预训练阶段、经过 SFT 阶段的模型，现在我们想要注入一些负面信号，比如我希望你偏好这个输出而不是那个输出。

第三步使用偏好数据。正如名称所示，偏好调优使用偏好数据，通常是成对数据，人类说我偏好这个输出而不是那个输出。

通常这里的模型能够将它产生的输出与人类偏好对齐，这些偏好可能在有用性、安全性、友好性、语气等维度上。有很多不同的维度，但这就是在第三步中发生的事情。

在第三步中，实际上在第五讲中我们深入研究了第三步的内容。如果你记得，我们在 LLM 产生 token 的方式与强化学习领域的人们认为给定策略如何与某些环境交互、执行某些动作并处于某些状态之间建立了平行关系。我们建立这种平行关系的原因是为了能够利用一些基于强化学习的技术来训练我们的模型。在这种情况下，我们说我们的 LLM 有点像一个策略。

给定某种状态（即它迄今为止收到的输入），它可以执行下一个动作，在这种情况下是预测下一个 token，这种预测是在 token 环境中进行的。当我们预测一个完成时，最终我们有一些信号、一些奖励部分，可以是人类偏好。

这是我们与强化学习世界建立的平行关系，考虑到这一点，我们谈到了奖励，但问题是奖励只对有限的数据集可用，这就是为什么我们看到如何建模奖励。

我们看到了这个公式，如果你记得，它被称为 Bradley Terry 公式，它建模了一个输出比另一个更好的概率，作为两个分数的函数，比如输出 I 的分数和输出 J 的分数。我们看到奖励模型通常通过考虑这个公式以成对方式进行训练。

这意味着对于奖励模型，你给它两个输出。你说这个是好的，这个是坏的，然后我希望你说这个是好的。你以成对方式训练它。但是你的模型实际上总是预测两个分数，它总是预测输出 I 的分数 RA I 和输出 J 的分数 RJ。所以在推理时你只给它一个输出。

我认为这是一个微妙之处，我们以成对方式训练它，但在推理时我们以个体方式使用它。

一旦我们使用这个公式训练了我们的奖励模型，然后我们就能够使用它来引导我们的 LLM 朝着我们关心的方向发展。

如果你记得，我们引导 LLM 朝着人类偏好方向发展的方式是给它一个提示，这样它就可以产生一个完成，也就是一个推演或者更简单地说，一个答案。

然后我们取这个提示，取这个答案，将它们都放入奖励模型中，它告诉我们模型响应有多好。根据奖励模型说的，我们可以以最大化在人类偏好上训练的奖励的方式调整 LLM 的权重。

这个强化学习设置的损失函数通常试图最大化奖励，但也保持模型接近基础模型。这里的基础模型是指 SFT 模型。我们想要这样做的原因是因为这个奖励是不完美的。

我们看到了这种奖励黑客现象，你的奖励可能是不完美的，LLM 可以利用其不完美的性质以实际上不符合你想要的方式进行调优。

所以你希望 LLM 不要偏离基础模型太远，基础模型实际上已经是一个好模型。这是一种正则化的方式，你也希望迭代更新不要太大。

所以你通常有这两个约束：你不希望它偏离基础模型太多，但你也不希望它偏离之前的强化学习迭代太多。

作为提醒，我认为这是第五讲，这是整个课程中技术上最具挑战性的一讲。所以如果你第一次听的时候不知道发生了什么，这完全没关系。但希望现在应该更清楚一些了。

很酷。在第五讲之后，我们想，好的，我们做了很多艰苦的工作。好消息是我们现在在 2025 年，在过去的 12 个月或现在 14 个月中，我们看到了很多具有推理能力的模型被发布，它们被训练展示这些高级推理能力的方式实际上利用了我们在第五讲中看到的很多技术，就像基于口头的技术一样。

### (30:00 - 45:00) Part 3

### (29:53 - 42:54) 推理能力与 GRPO 算法
> 从思维链到 GRPO 的推理训练技术突破

我们希望我们的 LLM 在产生最终答案之前输出一个推理链。我们希望这样做的原因是人们已经看到它能提高模型的性能。所以它实际上依赖于思维链这个概念，我相信我们在第三讲中看到过，这是一种提示技术，让你的模型在输出响应之前先输出推理过程。

长话短说，直到第六讲，我们的 LLM 都是直接将提示作为输入然后直接输出结果。但在第七讲中，抱歉，在第六讲中，我们说，好吧，让我们让 LLM 实际上先输出一个推理链，用户可能有也可能没有访问权限，然后再输出最终答案。所以你想教会 LM 这样做。那么你怎么做呢？

首先，在做这件事之前，我只想给你们展示这个图表，我们看到的是模型在我们教它产生这些推理链时的性能。人们通常通过与某些基准测试进行比较来衡量性能的改进，这是一个流行的基准——AIM 基准测试，这是数学基准测试。我们看到随着训练的进展，LLM 输出的准确性数字在增加。

但回到我刚才说的，我们用来教模型如何输出这些推理链的关键技术是利用我们在第五讲中看到的强化学习技术。特别是，到目前为止我们看到了 PPO，这是人们直到去年为止使用的主要强化学习算法，现在人们开始优先使用 GRPO 作为强化学习算法来教模型更好地完成推理任务。

这样做有几个原因，我现在就明确说明。我们看到了这个对比 GRPO 与 PPO 如何不同的插图，如果你能看到图表中，有几个不同之处。第一个是 GRPO 不依赖价值模型。那么，谁记得什么是价值模型？

学生：是的。

完全正确。所以，价值函数试图预测如果你要遵循 LLM 的策略会得到什么奖励。这是一种为一些预测的好坏提供基准的方式。你想让它变得更相对。所以价值函数是我们让这些奖励彼此更相对的一种方式。这就是 PPO 的做法。所以它有一个价值模型在做这些预测，然后我们有这个广义优势估计方法，它将奖励预测与价值函数预测结合起来，以获得我们称为优势的东西。优势是你的输出与某个基准相比有多好。

但与此相反，GRPO 说："好吧，树，我们不需要价值函数，因为它训练和维护成本太高。我们要做的是生成几个完整输出，然后有一些公式来比较这些完整输出的奖励。所以它在某种意义上会有一些相对效应，它会让事情变得更相对，这样做你实际上就不需要维护和训练价值函数了，这是与 PPO 相比的一个重大差异。

第二个重大差异，这在这个插图中没有体现，是 GRPO 通常是人们在教你的模型更好地完成推理任务的背景下使用的算法。我们看到这类问题有一个可验证的奖励，因为当你完成一个数学问题时，你实际上知道你需要得到的答案。所以你不需要训练奖励模型来告诉你最终答案有多好，因为你已经知道答案了。

我们看到 GRPO 特别用于当你实际上甚至不需要奖励模型、当你实际上有可验证奖励的情况下。所以最终，你需要保留的只有两个模型：策略模型和参考模型，能够比较你距离参考模型有多远。

很好。我知道这也是一堂有挑战性的课。到目前为止一切都好。这也会在期末考试中出现。这就是为什么我在这个回顾的第二部分进展得更慢。到目前为止一切都好吗？好的。

完美。我们还看到了 GRPO 的一些扩展。如果你记得，GRPO 的损失函数有一个归一化项，它惩罚较短输出中的标记，这导致了某种偏差。我们看到如果你在原始情况下、原始形式下使用 GRPO，在某个点之后算法会激励你的模型产生越来越长的答案，越来越长的错误答案。

它这样做的原因是相对于短的错误答案，它对长的错误答案惩罚较少。这就是今年人们致力于一些扩展的原因。其中之一是 GRPO done right。我们看到他们基本上移除了归一化项，还有另一种方法叫做 DAPO，它也有一些变化，那是针对推理模型的。

### (42:54 - 59:17) RAG 与智能体架构
> 检索增强生成、工具调用和现代智能体工作流程

然后在第七讲中，我们有了一个模型，我们知道如何训练它，我们知道如何将它用于推理任务，如何训练它变得更好，但现在我们希望模型在与外部系统交互时有用。我们看到了一种技术，这是一种基本技术，叫做 RAG，是检索增强生成的缩写，旨在让你能够从某个知识库中获取相关文档来回答问题或回答提示。

### (45:00 - 1:00:00) Part 4

这里有一个关键概念，叫做 LLM 作为评判者，你会接收提示作为输入，以及模型响应，还有你希望用来评估响应的标准。然后你希望你的语言模型输出两个东西。第一个是为什么给出某个分数的理由，以及那个分数。

如今，作为评判者的语言模型，它们通常输出二元响应，要么通过或失败，要么真或假，只是因为这更容易。我们也让理由在分数之前输出，因为在实践中，这也能略微提高作为评判者的语言模型的性能，就像推理模型所做的那样，在输出答案之前先输出推理链。

但我们也看到这种方法存在一些偏见。我们看到了位置偏见，也就是你呈现要比较元素的方式很重要。所以如果你先呈现某个东西，那么可能语言模型就会优先考虑第一个。还有冗长偏见，也就是你的语言模型偏好更长的输出。自我增强偏见是另一个，它偏好自己的输出。

然后我们也看到了许多基准测试，人们现在用它们来说明他们的语言模型有多优秀。所以如果你看到发布的版本，通常会有许多人们知道的不同基准测试的一系列指标。这涵盖了知识、推理能力、编程（这非常重要，因为很多应用都与编程相关）、安全性，这不是一个详尽的列表，实际上还有更多维度。

我想这就是我们上次停下的地方，这就是上节课的内容，这是你们期末考试需要知道的全部内容。之后的所有内容都不会成为期末考试的一部分。

到目前为止有什么问题吗？

很好。好的，我期待每个人期末考试都能拿到一百分。我想说我刚才讲的内容对期末考试来说是基础性的。所以我想如果你理解了我说的一切，你就为期末考试做好了准备。

学生：期末考试的范围是第5讲到第8讲吗？

是的。所以期中考试是第1、2、3、4讲，这次是第5、6、7、8讲。我想这是同等规模。

很好。好的，很棒。说到这里，我们刚刚完成了回顾本季度所有讲座的内容，现在我们要进入今天菜单的第二项，那就是看一些热门话题。

我要从第一个开始。我将这样介绍它。如果你记得，我们看到变换器（Transformer）是一个概念和架构，最初是在机器翻译的背景下引入的。它表现很棒。人们说好吧，它在机器翻译上表现很好，为什么不在其他文本任务上试试呢？所以他们尝试了，表现很棒，但现在问题是，你能不能将它用于文本以外的东西？

这是一个自然的问题，对吧。为了回答这个问题，我只想让我们提醒自己，这个架构依赖于自注意力（self attention）这个概念，这就是使变换器工作得如此好的原因。

如果我们回顾一下自注意力是什么，这个图解很好地完成了这个工作。你有一个查询，然后你有一堆其他元素，它们由你的键和值表示，你想知道哪些其他元素实际上是相关的，以便计算该查询的嵌入。

到目前为止，我们只使用了标记，你知道，文本标记，但文本标记实际上是向量。所以如果你取那些向量，你实际上表示除了文本之外的其他东西，比如图像的部分。问题是，基于这种输入的变换器是否也会表现良好？

所以这里我想问的关键问题是，我们如何调整我们的变换器来处理非文本输入，比如这里我们可以考虑图像理解输入。所以你有一些图像，这是一个传统的计算机视觉任务，你想知道这个图像属于哪个类别。

所以你想知道拥有一些基于变换器的架构在那种情况下是否会工作得很好。好吧，答案是，首先为了让它适应这个任务，你会取变换器的编码器部分，因为为了理解图像中有什么，你需要在某种意义上对那个图像进行分类。

如果你记得，在我们看到的模型中，有一个在分类方面工作得很好的模型是BERT，因为BERT是仅编码器的。它计算有意义的嵌入，然后可以用于投影目的或分类目的。所以这是一个非常自然的选择。这里我们只保留变换器的编码器部分，然后让自注意力机制发挥作用，计算有意义的嵌入，然后我们可以为我们相关的任务进行投影。

这正是一组研究人员在2020年所做的。你们听说过ViT视觉变换器吗？我这里描述的正是他们所做的。他们拿了一张图像，将那张图像分成补丁（patches）。这些补丁由一些向量表示。当然你有一些位置信息，让你知道你的补丁在图像中的位置。

然后你只是把它通过变换器编码器，也就是变换器的编码器部分，你计算对应于CLS类的表示，非常类似于BERT，你会将那个表示投影到一些感兴趣的类别上，然后你会执行你的计算。

那篇论文发现，如果你在大量图像数据上训练这样一个模型，你就会超越这些传统的卷积神经网络方法。这是相当显著的，为什么显著呢？因为在视觉情况下，有这个归纳偏置（inductive bias）的概念，你想让你的模型朝着查看某些东西的方向来推导结果。

卷积神经网络是一种以某种方式设计的模型，让你以某种滑动的方式查看图像。你知道，你查看图像的方式有点像你作为人类在实践中查看它的方式。人们假设这样的偏置，这样的归纳偏置对于视觉任务来说实际上是有意义的。

与此形成对比的是视觉变换器，它实际上让图像的所有部分互相关注，这在另一方面具有非常低的归纳偏置。所以这篇论文显示，如果你给你的模型足够的数据，那么它实际上会学习如何对图像进行分类。这是一个相当显著的结果。

我想这是非常显著的，也是我们所看到一切的一个很好的扩展。考虑到这一点，我想让我们通过一个端到端的例子，说明你如何处理图像并通过ViT（视觉变换器）进行预测。

这里你会拿你最喜欢的图像，将它分成补丁。这里你可以考虑预定义一些固定大小的补丁。这里我会说比如3x3，然后每个补丁有一些固定数量的像素，然后你要做的是为每个补丁尝试有一些向量表示。

所以你可以把每个补丁看作是由像素组成的。如果每个像素有三个值，对应红、绿、蓝，那么你可以找到一种方法将它们投影到某个低维扁平空间上，你可以学习如何通过某种线性层进行投影。长话短说，你只是找到一种方法将向量与每个补丁关联起来，然后表示你的每一个输入。

然后你当然有CLS标记的特殊嵌入，你也可以学习。你添加位置嵌入。所以你对所有输入都做同样的事情，然后非常类似于BERT，只是把它通过你的编码器。让每个人与每个人互动，然后最后你关心的是输入的有意义表示。

通常人们取CLS标记的编码嵌入。他们取这个的原因，一是因为这是一个惯例，二是这个CLS标记，编码嵌入实际上是一个通过这个自注意力机制与所有其他标记互动过的嵌入。所以它已经看到了一切，然后你会通过前馈神经网络将那个CLS标记编码嵌入投影到某个类上，以便预测你的最终类别。

在这种情况下，我们知道这是一张泰迪熊的图片。所以这里我们希望模型将其分类为泰迪熊。到目前为止还不错，这有意义吗？很好。

现在，我们知道如何处理图像输入，对吧？现在，另一个问题是你如何让你的语言模型回答关于你图像的问题，这实际上是你现在可以做的事情。比如如果你打开ChatGPT，你可以输入一张图像并问它问题。

所以你会有两种输入。你会有一张图像，我们看到我们可以找到一种方法来表示，然后是文本，你现在非常清楚如何用标记来表示。让模型处理所有这些的方式通常如下。有几种方法，第一种是最常见的，你只是将所有东西作为输入提供。

### (1:00:00 - 1:15:00) Part 5

### (59:17 - 1:10:13) 多模态 Transformer 扩展
> Vision Transformer 和视觉语言模型的技术实现

所以图像token作为输入，文本token作为输入，你有一些表示方式让模型知道这些是图像token，这些是文本token，然后让它以仅解码器的方式生成答案，自回归的方式，就像你处理第一种方法一样，很多这样的模型都是这样设计的。

比如有一个非常流行的开源权重视觉语言模型（VLM，这是它们的称呼），叫做LLaVA，它就是这样做的。它们在图像部分有一些编码器，产生一些token，然后与输入到LLM的文本token连接起来。这是一种方法。

第二种方法不太常见，是在交叉注意力层输入图像。在这种情况下，你有文本输入和图像输入。你不把它放在输入中，而是让它在交叉注意力层内与文本token交互。Llama 3在他们的论文中展示了这种技术。这种技术通常不太常见，第一种更常见。

我想说的是，CME 295专注于变换器在文本到文本问题中的具体应用。所以文本生成就是你们这门课所了解的全部内容，但我们也有变换器用于非文本应用。这里我们看到了图像理解，即使用ViT的视觉理解，然后我们现在没有时间说这个，但对于图像生成任务，我们也可以在该架构中使用变换器的一部分。

所以你可能会听说扩散变换器或多模态扩散变换器，它们实际上依赖于自注意力机制，这实际上并不是一个详尽的列表。这也被用于其他领域，如推荐系统、语音等。我希望你们记住的是，变换器是一个在机器翻译任务中表现非常好的架构，但后来证明在其他文本相关任务中也表现很好。

然后它被重新用于其他一堆领域，也被证明是相当成功的。所以我鼓励你们在这门课之后也对非文本相关的变换器应用保持开放的心态，我在这里提到的这些可能只是你可以查看的论文类型的几个初步指引。

很好。所以在这里我们说变换器是来自文本世界的东西，在其他世界也很有用并被使用。现在我想告诉你们一些其他的东西，我想它在非文本世界中被使用并且有用，可能在文本世界中也很有用，我想告诉你们关于基于扩散的LLM。

有谁听说过扩散这个术语吗？谁了解扩散？很好。我们将看到如何将其应用于LLM。这是一个非常热门的话题。我相信第一篇论文始于2020年代初，但我想只是现在人们才开始真正让它发挥作用。

我想从一个动机开始，到目前为止我们已经理所当然地认为我们的LLM是一个自回归LLM，我说的自回归是什么意思？它接受一些输入，LLM试图做的是预测下一个token。所以给定到目前为止的一切，我们预测下一个token。

### (1:10:13 - 1:23:38) 扩散模型在文本生成的应用
> 从图像扩散到掩码扩散模型的并行文本生成范式

我们这样做，然后我们取刚刚预测的token以及到目前为止预测的一切，然后我们再次预测下一个token。我们预测它，然后一次又一次地进行，直到用序列结束token完成序列，这使得生成停止。

所以这是真正的自回归生成，就是我们取到目前为止的输入来预测下一个token，然后重复这个过程直到结束。这是人们现在试图给它一个名称的东西，比如自回归模型类型的模型。所以，ARM，如果你看到这种符号，那就是它的意思。

这种范式的问题是推理时生成实际上不能并行化，因为你总是需要之前的内容来预测下一个。但我想说推理时生成不能并行化，但训练可以并行化。如果你记得我们进行训练的方式，我们输入所有我们想要模型预测的token，然后让模型从中生成token。

### (1:15:00 - 1:30:00) Part 6

所以你要学习某种模型，使其能够以重构原始句子的方式来去掩码这些掩码token。这涉及到一些数学原理。显然我们几分钟内没有时间深入讨论。但我只想强调关键思想，即你想要做扩散，但要以对文本输入有意义的方式进行。

而有意义的方式就是将图像的噪声视为文本输入的掩码token。很酷。基于这个想法，现在有一系列模型被发布，称为掩码扩散模型（MDM）。所以每当你现在看到MDM，你就知道我们在讨论什么类型的模型了。

符号表示仍然不是很明确定义，所以将来可能会改变。但你会看到的另一个术语是DLLM，即基于扩散的LLM。这就是它在做的事情。它不是像自回归那样一次预测一个token，而是在推理时从完全掩码的输入序列开始，试图预测这些掩码token背后是什么token。

当然，在实际应用中你会有一些提示。你会有一些提示。所以当然为了预测这个答案，你需要一些条件。所以你会告诉模型，给定这些提示，你将从所有这些掩码token开始，试图预测答案是什么。

如果你对这个直觉有困惑 - 我一开始也有困惑，不知道为什么用扩散方式解决文本问题会有意义，因为通常你写作时是一个词一个词地写。一个有帮助的思考方式是，假设你想写一篇演讲。你不会直接以线性方式写演讲。你首先会有一个大致的计划。

你说好的，我要首先谈论这个，第二、第三，你有某种草稿，然后你试图细化每个部分的内容。所以你可以将扩散视为以这种方式工作，它试图从粗糙到精细地改进输出，所以它可以预测在某个尚未预测的token之后的内容。你可以将其视为从非常草稿的版本到非常精细版本的过程。

这就是我对这个过程的理解，希望有帮助。这里的关键优势是解码现在用更少的前向传递完成，因为之前你必须做与要预测的token数量一样多的前向传递。但在这里对于扩散，你只需要做与扩散过程中的步骤数一样多的传递。步骤数是你可以固定的。

### (1:23:38 - 1:51:25) 2025 技术趋势与未来展望
> 架构优化、数据策略、硬件创新与应用前景分析

步骤越多，输出质量越高，但通常比输出长度要低得多。所以这是为什么这种模型比自回归模型快得多的核心原因。当然我们没有太多时间，但如果你好奇，如果你在期末完全解脱后有兴趣思考这些东西，我列了一些可能有帮助的参考文献。

今年早些时候有一篇论文叫做LADA，大语言扩散模型与掩码。实际上我不记得完整的缩写，但它实际上讲解了我刚才提到的数学原理和为什么有效。然后还有很多其他论文也会有帮助。如果你有兴趣，链接在幻灯片底部。

在交给Shervin之前，我想讲最后两件事。首先是这种新范式的优势。第一个是速度。正如我们提到的，它会比传统自回归模型快得多，特别是对于较长的输出。一些基准测试甚至说它大约快10倍。对于编码等用例，它可能非常强大，因为你可能需要进行多次模型调用，作为用户你只是在等待代码生成，更低的延迟会带来很大差异。

另一件事是这种方法的本质实际上是将文本作为整体来进行预测。所以有一类编码任务叫做"填充中间"，即试图找出你有一堆代码，想知道中间缺少什么。填充中间和扩散模型通常更适合这类任务，因为它们可以考虑来自多个方向的输入，这就是为什么这种方法对某些应用可能有用的原因。

就目前的工作而言，这些模型看起来很棒，我提到的内容看起来很棒，但性能至少在一段时间内没有达到当前前沿模型的水平，但这可能会改变。我提到的论文实际上显示的性能正在赶上自回归模型。所以确实有希望。然后另一个研究方向就是适应人们想出的所有技术，比如推理链。你如何将其适应扩散？还有很多技术本质上更适合自回归类型的模型，可以为此进行适应，这就是人们正在努力的方向。

### (1:30:00 - 1:45:00) Part 7

过去你有后归一化（postnorm），但现在有预归一化（pre-norm），它将归一化操作提前到了层的更早位置。但除了归一化位置的设计选择之外，甚至归一化的类型也在改变。Transformer 论文使用了层归一化（layer norm），但现在你可能会看到其他类型的归一化技术，比如 RMS norm，它使用更少的参数等等。所以背后的理论还没有确定。

你还有其他类型的参数。Ashin 提到了分组查询注意力（grouped query attention）论文，你会看到现在的 LLM 论文不是固定的设计，而是每篇论文都采用自己的技术。有时你会在给定层看到一种注意力机制，然后它会切换，不同的论文采取不同的设计决策，所以这并不是一成不变的。然后你还有激活函数。传统深度学习中，人们非常重视 ReLU，它非常简单并且效果很好。

但在 LLM 的世界中，转向了类似 ReLU 但不完全是 ReLU 的激活函数。你有高斯误差线性单元（GELU），还有其他类型，这方面的研究仍在进行中。你仍然会看到新的激活函数不断出现。然后你还需要考虑是否将 LLM 设计为一体化（monolithic）架构，以及 LLM 的层数和其他超参数，比如注意力头的数量、FFN 中单元的数量，所有这些在设计决策方面仍然存在争议。所以它并不固定。

然后我想提到的另一个研究领域是数据部分，这很关键。早期的 LLM 享受了相对干净的状态，因为你可以抓取互联网，希望得到很多肯定是人类生成的数据。所以你可以从所谓的高质量来源学习这些模式，即使当你看典型的互联网数据时，它的格式不是高质量的，但它仍然是人类生成的。现在情况已经改变了。你在喜欢的搜索浏览器中输入任何内容，第一批结果很可能有 80% 是 LLM 生成的。

那我们注定失败了吗？也许不是，因为实际上你会看到数据策展（data curation）工作的越来越多的发展。过去你只是抓取整个互联网，然后在上面训练下一个 token 预测。但现在你有越来越多的工作在策展感兴趣的数据集，有公司在做这个工作，出现了更新的微调模式。过去你有预训练然后微调，现在你有预训练、中期训练和微调。中期训练部分仍然在大量数据语料库上训练，但质量更高。

所以人们正在找到解决方法。我会说情况并不全是悲观的，只是我们需要做更多工作才能掌握有意义的数据。幻灯片底部链接的论文处理的现象是如果你在 LLM 生成的数据上进行训练会怎样，它谈到了一个叫做模型坍塌（model collapse）的概念。它说 LLM 生成的文本通常多样性较差。所以你在训练时看到的数据分布会发生变化，导致训练时的学习不那么有意义，这就是为什么它通常是不好的，并且激发了对数据部分更多工作的需求。

好的。然后你知道，即使退一步看我们一直在使用的架构本身，它是最好的吗？这并不清楚。所以这本身就是一个研究领域，未来的突破可能来自重新设计这个架构。在过去几年中，我们看到的很多研究都是在每次改进基准测试。你有一组基准测试，每个人都试图获得最好的结果。这是一个自然的趋势，因为你想要有越来越强大的模型来满足所有用例。

但假设我们达到了一个点，我们关心的所有用例都解决了。然后呢？我认为我们将看到帕累托前沿第二边界的出现，我们最关心的是让 LLM 的预测具有成本效益，同时仍然保持非常高的质量。所以我们看到越来越小的 LLM 的出现，比如我认为在文献中被称为小语言模型（SLM）。你有时会听到 LLM 提供商说，即使在最高级别的套餐上，他们也在亏钱。

所以我认为这反映了这样一个事实：你需要在测试时为服务 LLM 查询所花费的计算上更加聪明，我认为这将在未来几年激发越来越多的这一研究方向。然后还有另一个我们在这门课中根本没有涉及的领域，那就是硬件部分。通常用来训练这些 LLM 的设备类型是 GPU，它们擅长一件事：矩阵乘法。但问题是，我们一直保持这些架构来训练我们的模型，即使该架构不仅仅需要矩阵乘法作为基础的计算原子单元。

自注意力世界和 Transformer 世界有所有这些特殊需求。你知道我们看到的 Q K 转置部分实际上非常昂贵，这激发了像 Flash Attention 这样的论文，正如 Afin 提到的，实际上为了不在内存方面做太多移动而放弃一些数据，即使这意味着之后要重新计算相同的东西。然后很多工作都在优化 GPU 内存流的位置。这表明也许你需要更优化的硬件架构来解决这些用例。

最近有一篇论文出来，实际上将所有这些操作编码为硬件的一部分。过去 GPU 擅长的核心操作是矩阵乘法，基于此你试图构建你想要的所有输入输出。但这里出来的这篇论文，我想是在 9 月份，展示了一个概念证明，你可以将所有这些计算作为用模拟信号实现输入输出的副作用来完成。所以你有所有这些计算嵌入为硬件的一部分，基于模拟你的数组值的脉冲作为输入。

这些硬件架构有一些物理属性，比如你可以想到基尔霍夫定律在强度场中，你可以将它们相加，它使用这样的属性来得到你需要的输入，然后只是读出结果作为输出。所以当论文模拟这种架构时，他们观察到在延迟和节能部分都有相当大的改进，这并不太令人意外。两者都可以解释，因为你不需要自己进行计算，你只是作为硬件的副作用得到它们。

### (1:45:00 - End) Part 8

也许明天你会有某种证书来保证网站对 AI 助手浏览是安全的。从更高层面来看，也许在你的桌面或移动手机上，在 LLM 层面，在操作系统层面进行浏览可能是 LLM 能够帮助的事情。当我们谈到智能体时，我们提到它们可能不太可靠，因为随着步骤越来越多，失败的概率会增加，稳定预测是一个值得关注的问题。

从更长远来看，我认为我们可以用一个测试来衡量我们已经走了多远，那就是 AI 驱动的客服是否真正有用这一常见用例。我不知道你们怎么样，但每次我遇到问题打电话时，听到一些 AI 助手，可能是 LLM 驱动的机器人，我就会想"快点，我要人工服务"。我不想要那个。我认为这说明了这个问题空间有多困难，因为人类具有比 LLM 能带来的更多维度的价值。比如同理心、基础性，有些事情你和我都感知为有意义的东西，尽管它们不在我们的系统提示中。

即使向前发展，我们在当前架构上也面临一些关键挑战。我们在课堂上看到，你必须经历一个训练过程来固定一些权重，但这些权重之后不会改变。我们使用 RAG 或工具等技巧来解决这个问题。但我们能否想象一个持续学习的系统？我认为这是一个开放的问题。然后是幻觉问题，我把它放在引号里，因为我不确定说 LLM 产生幻觉是否公平，仅仅因为我们训练 LLM 去预测下一个 token，本质上不是映射陈述到事实。所以幻觉在某种意义上是这些 LLM 的核心设计选择。个性化、可解释性、安全性，清单还在继续。

现在我想简要介绍一下你们如何从现在开始锻炼保持最新状态这块肌肉。你们有 arXiv，它通常包含所有这些最新最好的论文供你们查看。当然，像现在的 NeurIPS 这样的会议非常适合突出论文，然后我强烈鼓励你们除了论文之外，还要看作者提供的相关代码库。现在提供你们所提议内容的实现已经很常见，我认为学习概念是非常有洞察力的。

有一个过去存在的"Papers with Code"，现在已经被 Hugging Face 的热门论文所取代，我认为这是一个查看最新方法的好地方。在社交网络方面，Twitter 或 X 有很多经常讨论的最新内容。那里有一个强大的社区，如果你在那个社交媒体上有账户，你有很多优秀的人可以关注来保持更新。你们还有 YouTube 上的资源，特别推荐 Yannic Kilcher，我认为他是第一个在 2017 年详细介绍 Transformer 论文的 YouTuber，我认为这些 YouTuber 中的一些人非常擅长详细讲解论文。

另一个亮点是 Andrej Karpathy，他大约 10 年前在 Stanford，我认为他是最好的教育者之一。我强烈推荐他的视频。你们还有公司博客也很棒，还有我们与这门课相关的学习指南。我们今年有了它，我们将在未来几年努力至少每年更新一次。你们可以将这个资源视为某种伴侣，我们有机会与世界各地的专家合作，如果你们感兴趣的话，让它以其他语言提供。


---

*生成时间: 2026-01-03 05:23:03*
*由 YouTube Monitor & Translator (Claude CLI) 生成*