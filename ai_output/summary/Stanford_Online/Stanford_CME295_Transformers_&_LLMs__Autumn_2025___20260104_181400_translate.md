# Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 8 - LLM Evaluation

## 📹 视频信息

- **频道**: Stanford Online
- **发布日期**: 2025-12-02
- **时长**: 1:49:20
- **原始链接**: [https://www.youtube.com/watch?v=8fNP4N46RRo](https://www.youtube.com/watch?v=8fNP4N46RRo)

---

本文内容整理自斯坦福大学《Transformers 与大语言模型》课程第 8 讲，由 Ashin 和 Shervin 主讲，深入探讨 LLM 评估的方法与实践。

## TL;DR

LLM 评估的核心挑战在于自由文本输出的主观性和多样性。课程系统介绍了从人工评估到自动化评估的演进路径，重点阐述了"LLM as a Judge"方法的原理、偏差和最佳实践，并详细分析了智能体工作流的七种失败模式，最后概述了知识、推理、编程、安全和智能体五大类基准测试的特点与应用。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-04:48 | 课程介绍与回顾 | 回顾 RAG、工具调用和智能体工作流，引入评估主题 |
| 04:48-18:16 | 人工评估的挑战 | 探讨人工评估的主观性、评分一致性和成本问题 |
| 18:16-28:11 | 基于规则的评估指标 | 介绍 METEOR、BLEU、ROUGE 等传统指标的局限性 |
| 28:11-52:41 | LLM as a Judge 方法 | 详解使用 LLM 作为评判者的原理、偏差和最佳实践 |
| 52:41-59:56 | 评估维度与事实性检查 | 讨论任务性能、格式对齐和事实性验证的方法 |
| 59:56-1:23:17 | 智能体评估与失败分析 | 分析智能体工作流的七种失败模式及解决方案 |
| 1:23:17-1:49:20 | 基准测试分类与实践 | 介绍五大类基准测试及其在实际模型评估中的应用 |

## 📊 核心论点

### 1. 人工评估的根本挑战：主观性与成本的双重困境

- **核心内容**：人工评估虽然是理想的评估方式，但面临两大根本挑战。首先是主观性问题：同一个回答（如"泰迪熊是个不错的礼物"）可能被不同评分者给出截然不同的评价。其次是成本问题：大规模人工评估需要耗费巨大的时间和金钱。为了量化评分一致性，研究者引入了 Cohen's Kappa、Fleiss' Kappa 等统计指标，这些指标通过考虑随机一致性的基线（如两个评分者随机给分的一致率为 50%）来衡量实际一致性的显著程度。
- **关键概念**：评分者间一致性（Inter-rater Agreement）、Cohen's Kappa 系数、随机一致性基线、主观性任务、评分指南对齐
- **实际意义**：推动了自动化评估方法的发展；为构建高质量人工标注数据集提供了理论基础；启发了半自动化评估流程的设计，如定期校准和对齐会议。

### 2. 基于规则的指标：从翻译到摘要的局限性

- **核心内容**：传统的自动评估指标如 METEOR、BLEU 和 ROUGE 通过比较模型输出与参考答案来计算分数。METEOR 考虑了同义词和词序，使用 F-score 和惩罚项的组合；BLEU 主要关注精确度，计算 n-gram 匹配率；ROUGE 则更适用于摘要任务。然而，这些指标的根本局限在于无法处理语言的风格多样性——同一个意思可以用完全不同的词汇和句式表达（如"柔软的毛绒熊能在睡前安慰孩子"vs"许多小朋友拥抱着温柔的玩具伙伴时能更容易入睡"）。此外，这些指标与人工评分的相关性较低，且仍需要人工创建参考答案。
- **关键概念**：n-gram 匹配、精确率与召回率、简洁性惩罚（Brevity Penalty）、风格多样性、参考答案依赖
- **实际意义**：促进了对更灵活评估方法的探索；在特定领域（如机器翻译）仍有应用价值；为理解 LLM 评估的复杂性提供了历史视角。

### 3. LLM as a Judge：范式转变的创新与挑战

- **核心内容**：LLM as a Judge 方法将评估任务本身视为一个文本生成任务，利用预训练模型内含的人类知识和偏好来评判其他模型的输出。该方法的核心创新在于不仅输出分数，还提供评分理由（rationale），使评估结果可解释。通过结构化输出（Structured Output）技术和受限解码（Constrained Decoding），可以确保输出格式的一致性。实验表明，先输出理由再给出分数的顺序能显著提升评估质量，这与推理模型中的思维链（Chain of Thought）原理一致。
- **关键概念**：评分理由（Rationale）、结构化输出、受限解码、思维链评估、可解释性评分
- **实际意义**：大幅降低了评估成本；实现了可解释的自动评估；为快速迭代和 A/B 测试提供了基础；推动了评估民主化，使小团队也能进行大规模评估。

### 4. 三大评估偏差：位置、冗长度与自我增强

- **核心内容**：LLM as a Judge 存在三种系统性偏差。位置偏差（Position Bias）：模型倾向于选择先出现的选项，可通过交换选项顺序并取多数投票来缓解。冗长度偏差（Verbosity Bias）：模型偏好更长、更详细的回答，即使简洁的答案可能更准确，需要在提示词中明确说明或添加长度惩罚。自我增强偏差（Self-enhancement Bias）：模型倾向于给自己生成的内容更高评分，建议使用不同的模型进行生成和评估，且评判模型通常应比生成模型更大、推理能力更强。
- **关键概念**：系统性偏差、位置敏感性、长度偏好、模型自评、交叉验证
- **实际意义**：为设计公平的评估流程提供了指导；帮助研究者识别和纠正评估中的系统性错误；推动了更稳健的评估方法论发展。

### 5. 事实性评估的分解策略：从整体到原子事实

- **核心内容**：评估长文本的事实性面临粒度挑战——一段话可能包含多个事实，有对有错。解决方案是采用分解-验证-聚合的三步流程：(1) 使用 LLM 将文本分解为原子事实列表；(2) 对每个事实进行二元判断（正确/错误），通常结合 RAG 或网络搜索；(3) 通过加权聚合计算整体分数，权重可反映事实的重要性。例如，"泰迪熊首次创建于1920年代"包含两个事实错误（应为1900年代，总统拒绝射杀而非自豪地想要射杀），最终得分为 50%。
- **关键概念**：原子事实分解、事实检查流程、加权聚合、RAG 集成、重要性权重
- **实际意义**：为评估 LLM 幻觉提供了量化方法；支持细粒度的错误分析；为构建更可靠的 AI 系统奠定基础。

### 6. 智能体评估的七种失败模式：从工具预测到结果推理

- **核心内容**：智能体工作流可分为观察-计划-行动三个步骤，每个步骤都可能失败。工具预测阶段的四种失败：(1) 工具路由错误：需要工具但未选择；(2) 工具幻觉：调用不存在的函数；(3) 工具混淆：选择了错误的工具；(4) 参数错误：工具正确但参数错误。工具执行阶段的两种失败：(5) 执行错误：工具内部错误或返回错误信息；(6) 无响应：工具执行后未返回有意义的输出。结果推理阶段的失败：(7) 综合失败：无法正确解释工具返回的结果。每种失败都有相应的诊断和修复策略。
- **关键概念**：工具路由器、API 设计、错误分类、系统化调试、端到端追踪
- **实际意义**：为智能体系统的调试提供了系统化方法；帮助开发者快速定位和修复问题；推动了更可靠的智能体架构设计。

### 7. 五大基准测试类别：从知识到智能体的全方位评估

- **核心内容**：现代 LLM 基准测试分为五大类别，每类测试模型的不同能力。知识类（MMLU）：包含 57 个学科的多选题，测试预训练质量。推理类：包括数学推理（AIME）和常识推理（PIQA），测试思维链能力。编程类（SWE-bench）：使用真实 GitHub issue 和测试用例，测试代码理解和生成能力。安全类（HarmBench）：评估有害行为识别和拒绝能力，但因政策主观性难以跨模型比较。智能体类（TAU-bench）：通过模拟用户交互测试工具使用能力，引入 Pass^K（所有 K 次尝试都成功的概率）指标强调可靠性。
- **关键概念**：多任务评估、测试驱动开发、政策对齐、用户模拟、可靠性指标
- **实际意义**：为模型选择提供多维度参考；推动了专门化模型的发展；帮助识别模型的优势和劣势领域。

### 8. 基准测试的实践智慧：从 Pareto 前沿到数据污染

- **核心内容**：模型评估需要考虑多个维度的权衡，如性能、成本、延迟、安全性等。通过绘制不同维度的 Pareto 前沿，可以找到在给定约束下的最优模型选择。例如，Claude Sonnet 在编程任务上表现优异，而 Gemini Flash 在需要快速廉价响应时更合适。数据污染是基准测试的严重威胁，需要通过哈希值验证、网站黑名单、使用新测试题等方式防范。Goodhart 定律警示我们："当一个度量成为目标时，它就不再是一个好的度量"，因此不应过度优化基准分数。
- **关键概念**：Pareto 最优、多目标优化、数据污染、Goodhart 定律、真实世界验证
- **实际意义**：指导实际应用中的模型选择；防止过拟合基准测试；推动更真实、更有意义的评估方法发展。

### 9. 从评估到改进：测量驱动的 LLM 发展

- **核心内容**：评估不仅是衡量性能的工具，更是推动 LLM 改进的关键驱动力。通过系统化的评估，可以识别模型的薄弱环节，指导训练数据的收集、模型架构的改进和微调策略的制定。评估还推动了新技术的发展，如结构化输出确保了评估的一致性，思维链提示提高了评估质量。持续的人机对比校准确保自动评估不偏离人类判断。评估方法本身也在不断演进，从简单的规则匹配到复杂的神经评估，反映了对语言理解深度的不断追求。
- **关键概念**：评估驱动开发、持续改进循环、人机协作评估、评估方法演进、质量保证体系
- **实际意义**：建立了 LLM 开发的科学方法论；促进了行业标准的形成；为 AI 系统的可靠部署提供了保障。

### 10. 评估的未来：多模态、个性化与实时适应

- **核心内容**：LLM 评估正向多个方向演进。多模态评估不仅考虑文本，还包括图像、语音等模态的理解和生成质量。个性化评估认识到不同用户群体有不同的需求和标准，推动了可定制的评估框架。实时评估使模型能够在部署中持续学习和改进。新兴的评估方法包括对抗性评估（测试模型鲁棒性）、因果评估（理解模型决策机制）和社会影响评估（考虑 AI 系统的广泛影响）。评估正从单一指标向综合评估体系转变，更好地反映真实世界的复杂性。
- **关键概念**：多模态基准、用户中心评估、在线学习、对抗鲁棒性、社会技术系统
- **实际意义**：推动 AI 系统向更通用、更可靠的方向发展；支持 AI 的负责任部署；为下一代 AI 系统的设计提供指导。

## 🔬 提及的技术/方法/论文

| 技术/论文 | 讨论语境 | 重要性 |
|----------|----------|--------|
| LLM as a Judge (2023) | 使用 LLM 评估其他 LLM 输出的开创性方法 | ⭐⭐⭐ |
| MMLU Benchmark | 大规模多任务语言理解基准，涵盖 57 个学科 | ⭐⭐⭐ |
| METEOR/BLEU/ROUGE | 传统的自动评估指标，主要用于翻译和摘要 | ⭐⭐ |
| Cohen's Kappa | 衡量评分者间一致性的统计指标 | ⭐⭐ |
| SWE-bench | 基于真实 GitHub issue 的代码生成评估 | ⭐⭐ |
| TAU-bench | 智能体工具使用能力的评估基准 | ⭐⭐ |
| Constrained Decoding | 确保 LLM 输出特定格式的技术 | ⭐⭐ |
| ReAct Framework | 智能体的推理和行动框架 | ⭐⭐ |
| AIME/PIQA | 数学和物理常识推理基准 | ⭐ |
| HarmBench | 评估模型安全性和有害行为 | ⭐ |

## 💬 经典金句

> "If we don't know how to measure the performance of our LLM, we don't really know what to improve."
> — Ashin

> "A measure when a measure becomes a target it ceases to be a good measure."
> — Goodhart's Law (cited by Shervin)

> "These benchmark results don't necessarily tell you whether a model is good for you or not... ultimately you should be the one trying out these best models and see for yourself which one corresponds to you best."
> — Shervin

## 👤 主要人物

#### Ashin

**身份**：斯坦福大学 CME295 课程讲师
**背景**：LLM 评估领域专家，在自然语言处理和机器学习评估方法方面有深入研究
**核心观点**：强调评估是 LLM 开发的核心，详细阐述了从人工评估到自动化评估的演进，特别推崇 LLM as a Judge 方法的创新性和实用性，同时也客观分析了其局限性和偏差

#### Shervin

**身份**：斯坦福大学 CME295 课程讲师
**背景**：智能体系统和基准测试专家，对工具使用和智能体评估有深入理解
**核心观点**：系统分析了智能体工作流的失败模式，强调了方法论在调试复杂系统中的重要性，并全面介绍了现代 LLM 基准测试体系，提醒不要过度依赖基准分数

## 📺 视频类型判断

**演讲独白**：大学课程讲座形式，两位讲师系统地讲解 LLM 评估的理论与实践

---

## 📝 完整翻译

### (0:00 - 4:48) 课程介绍与回顾
> 回顾 RAG、工具调用和智能体工作流，引入评估主题

大家好，欢迎来到 CME295 的第 8 讲。今天的主题是 LLM 评估，我认为这堂课可能是本季度最重要的课程之一，因为如果我们不知道如何衡量 LLM 的性能，我们就不知道该改进什么。所以这堂课将重点关注如何量化 LLM 在各种不同情况下的表现。

话说回来，我们将像往常一样通过回顾上周的内容来开始这堂课。如果你记得，上周我们看到了 LLM 如何与 LM 本身之外的系统进行交互。我们看到了一个核心技术叫做 RAG，它允许我们的 LLM 从外部知识库中获取信息。RAG 代表检索增强生成（retrieval augmented generation），我们看到了如何改进检索系统。我们看到它由两个主要步骤组成。一个是候选检索，通常使用编码器设置来完成。Sentence BERT 就是人们设计这种模型的一个很好的例子。这第一步通常用于为给定的输入查询筛选出潜在的相关候选。

然后我们看到了第二步，即重新排序（reranking），这个步骤更复杂，涉及更精密的交叉编码器。我们还看到了一些量化检索系统表现的方法。然后我们还看到了叫做工具调用（tool calling）的东西，这是模型知道用哪些参数调用哪个工具的能力。

如果你记得，如果我们给 LLM 提供可用工具的知识，它可以根据输入查询确定需要输入到函数中的参数，然后运行该函数，然后以自然语言向用户输出结果。然后我们还看到了智能工作流（agentic workflows）是如何构成的。剧透一下，这是前两种方法的组合，即 RAG 和工具调用。特别是给定一个输入，我们允许模型进行多次调用，调用不同的工具从其他知识库中获取相关数据，我们看到了一个从当前应用中相当成功的例子，即 AI 辅助编程，它依赖于这个原理。

### (4:48 - 18:16) 人工评估的挑战
> 探讨人工评估的主观性、评分一致性和成本问题

React 通常是人们会使用的框架，即推理加行动（reason plus act），将其分解为观察、计划和行动步骤。很好。这就是我们上次看到的内容，我们上次也是从这张幻灯片开始的。如果你记得，我们的 LLM 有优势，但也有我们试图缓解的弱点。特别是，第六和第七讲的重点是改进模型推理的方法，以及模型从其他系统获取知识和执行动作的方法。今天我们将重点关注评估部分。特别是，给定模型给出的响应，我们如何量化 LLM 给出响应的质量？

很好。首先，我想定义评估这个术语以及我们在这个讲座中将使用的含义。当我们说我想评估我的 LLM 时，它实际上可以有很多不同的含义。当你说让我们评估 LLM 时，它可以意味着让我们评估性能、输出，让我们基于连贯性、事实性来评估它。让我们基于延迟来评估它，更多系统相关的指标或定价或它多久在线等等。为了确保我们在同一页面上，这个讲座将主要关注输出质量部分，特别是我们将专注于量化实际响应的好坏。

在这里你会注意到这是一个具有挑战性的问题，因为正如我们之前看到的，我们的 LLM 是一个文本到文本的模型，基本上可以输出任何东西。它可以是自然语言，可以是代码，可以是数学推理等等。所以很难想出通用的指标来评估这一点。我们将看到人们在实践中是如何做到这一点的。很好。考虑到我们的 LLM 生成自由形式的输出这一事实，人们可以想象，评估 LLM 输出的理想场景是每次都请人类对响应进行评级。

理想的场景是我给我的 LLM 一个提示，它给出一个响应，我请人类对其进行评级，然后我一遍又一遍地重复，我所做的是在一天结束时收集所有这些人类响应，并尝试量化我模型的整体性能。正如你可以想象的，主要问题是这样的系统成本会非常高。但让我们更详细地看看这个问题。

如果你记得，LLM 输出真的是自由形式的，可能存在即使人类判断也可能是模糊的情况，因为也许评级任务本身是主观的。让我们举以下例子。假设我问我的 LLM 我应该买什么生日礼物？假设 LLM 回答泰迪熊几乎总是一个甜蜜的礼物，只需选择一个你觉得合适的。假设我想在有用性维度上评估这个响应。我可能有一个人类评分员说："是的，这很有用，因为泰迪熊很好地指示了用户应该得到什么作为礼物。"但另一个评分员可能说："不，实际上这没有用，因为也许响应没有具体说明哪个泰迪熊。我应该选熊吗？应该选大象、长颈鹿吗？我应该买哪个毛绒动物？"

所以有这个评分员间一致性的概念，我们基本上关心确保每个人在如何评级这些响应方面保持一致，因为有时就像在这个说明性例子中，可能会有点主观。所以响应可能会有所不同。人们想要做的是确保指导原则足够清晰，让每个人都能以一致的方式评级这些响应。所以人们提出了一致性类型的指标。你可能想到的一个很自然的指标是所谓的一致率。例如，你有两个评分员。你所做的就是衡量两个评分员给出相同响应的时间比例。假设这里的响应是二进制的。比如说是好的或不好的。你看到这种指标的问题了吗？这是一个好指标吗？

我想问这个问题的另一种方式是，如果我给你一个给定的一致率数字，你能告诉我这是一个好数字还是一个坏数字吗？让我们以两个评分员为例。假设 Alice 和 Bob，假设这些评分员可以给出两种不同类型的评级。要么说输出是好的，要么输出不好。如果我们假设第一个评分员以某种概率 P_A 给出随机响应表示好，以 1-P_A 的概率表示不好，然后假设 Bob 有 P_B 的概率表示好，1-P_B 的概率表示不好。

### (15:00 - 30:00) Part 2

计算一个基于偶然一致率的函数量，并获取观察到的一致率，这样如果我们观察到的一致率大于偶然一致率，那么我们的系数就是正数。所以当它是正数时，你至少知道它是朝着正确方向发展的。所以这里如果观察到的一致率等于1，那么 kappa 等于1。但如果我们观察到的一致率低于我们在黑板上看到的纯随机偶然一致率，那么我们的系数就会是负数。

总而言之，有一系列指标试图使用这些类型的公式来量化评分员间一致性率，能够使这些量相对于随机方式下会发生的情况。这就是为什么你可能会看到很多这样的指标。这里人们使用 Cohen's kappa 来处理两个评分员的情况，然后你还有一些扩展，比如 Fleiss' kappa 和 Krippendorff's alpha，你可能会看到。它们都依赖于这样的想法：我们应该有一些基线，也就是我们的评分员只是随机选择答案，然后试图看看我们实际的一致性与此相比有多好。

所以这有意义吗？是的。我想说的是，要求人类评分 LM 输出的第一个限制，有时任务是主观的，这可以通过这种评分员间一致性指标来量化。所以人们通常做的是跟踪这种一致性有多好。如果我们有一个不令人满意的量，人们就会举行一些所谓的一致性会议，让评分员在如何评分答案方面保持一致。所以这可以被看作是一个健康指标，用来跟踪你的评分有多一致，这通常是人们在实践中使用的东西。

### (18:16 - 28:11) 基于规则的评估指标
> 介绍 METEOR、BLEU、ROUGE 等传统指标的局限性

到目前为止，我们已经看到了人类评分的一个限制。第二个限制我想我之前也说过。它真的很慢。你知道，如果你要求某人评分一千个 LM 输出，那会花费他们很长时间，当然也很昂贵。所以，这一切都表明，我们理想中让人类评分每个 LLM 输出的场景并不实用。

但我们可以以某种方式利用人类评分，因为我们已经看到，即使任务是主观的，我们也有办法让我们的评分员保持一致。现在让我们转向另一种方法，即使用一些基于规则的指标。这里我将修改之前提到的设置，不再要求我们的人类对每个 LLM 输出进行评分，这次我只要求他们为给定的一组提示写出参考答案或理想输出。将其固定下来，然后使用某种指标来比较 LLM 输出与这些参考答案。

这里的主要区别是，假设我有一个固定的提示集。我可以在我的模型上进行迭代，并始终将我的 LLM 输出与这个固定的参考答案进行比较，而不是总是要求人类一次又一次地评分。所以，这已经是一个改进，我们将看看有哪些基于规则的指标。理想情况下，这些指标应该以最优方式反映 LM 输出的性能。我所说的最优方式，是指考虑到自然语言并不总是可以用一种给定方式表达的事实，要有一点灵活性。

例如，当我对给定提示提供响应时，很可能出现这种情况：我可以略微不同地表述响应，但它仍然同样好。所以这些指标背后的想法是让这种比较有一点灵活性。让我们从翻译案例中人们使用的一个常见指标开始。这个指标叫做 METEOR，代表"具有显式排序的翻译评估指标"。这里的想法是比较参考答案和预测结果，我们将看到它是如何完成的，并且惩罚单词顺序不同的情况，这解释了为什么该指标被称为"具有显式排序"。

公式如下。它是某个 F-score 乘以 (1 减去某个惩罚项)。这里的 F-score，你可能熟悉 F1 score。所以它就像等权重的调和平均数。这个是可变权重的。所以它是精确率和召回率的函数，其中精确率是你预测序列中与参考答案匹配的 unigram 的比例，召回率是参考答案中与预测结果匹配的 unigram 的比例。所以它基本上匹配你知道的通常的精确率召回率指标，然后我们有另一个量，即惩罚项。

我提到这个惩罚项试图激励良好的排序。所以如果参考答案和预测结果中的顺序相同，那就很好，否则就不好。这里有一系列量。gamma 和 beta 是人们任意选择的超参数。它是 C 的函数，C 是匹配的连续块数除以匹配的 unigram 数。理想情况下，你希望 C 尽可能低，因为如果你的连续匹配数很少，这意味着你的连续序列很长，这意味着排序是相同的。

所以你希望 C 很低，然后匹配的 unigram 很高。所以你希望对于与参考答案具有相同排序的预测，该惩罚项要低。我想更高的 METEOR 分数意味着根据这种方法更好的翻译。所以当你看这个公式时，首先它看起来非常任意，对吧？我有 alpha 作为超参数，gamma、beta，所以这有点像一个配方。这是一个问题。第二件事是它不允许风格变化，因为这里我们测量匹配 unigram 的数量。

尽管该指标通过考虑诸如彼此同义词和相同词根的词等内容来扩展所谓匹配 unigram 的范围，但在这个意义上仍然不是极其令人满意的。所以 METEOR 是这样一个指标。你还有另一个在翻译任务中使用或已经使用的指标，叫做 BLEU，你可能知道。BLEU 代表双语评估研究。你可以把它想象成一种以精确率为重点的指标，它查看匹配的 n-gram 数量除以预测中的 n-gram，这就是为什么它是一种精确率类型的指标。

### (28:11 - 52:41) LLM as a Judge 方法
> 详解使用 LLM 作为评判者的原理、偏差和最佳实践

它还有一个惩罚项。它被称为简洁性惩罚，因为鉴于它更像是一种精确率类型的指标，如果你翻译的内容很短，你可能能够欺骗这个指标。所以你想惩罚翻译过短。我们不会详细介绍，但我只想向你展示现有的指标类型。METEOR 是一个，BLEU 是另一个，还有 ROUGE，你可能听过，也是另一个通常用于摘要任务的指标。同样的想法，它有一系列你可能会看到的变体，但总而言之，所有这些指标都比较输出与参考答案。

正如我们看到的，一个关键限制是它们不允许风格变化。让我们举个例子。假设我说"毛绒玩具熊可以在睡前安慰孩子"。同样的事情我可以用非常不同的方式表达。"柔软的毛绒熊通常帮助孩子在入睡时感到安全"或"许多年轻人在与温和的玩具伙伴拥抱时在夜晚休息得更好"。在所有这些情况下，我们看到的指标表现会非常差。

### (30:00 - 45:00) Part 3

与之前方法的关键区别在于，我们能够解释为什么指标或模型给出了特定的分数。这非常有用，因为在其他基于规则的方法中，你会有各种公式和乘法运算，有时得出的数字不太容易解释，幸运的是，LLM 作为评判者解决了这个问题。

总结一下，我们想要使用 LLM 来评分回答。通常你会有以下类型的提示词。你会说，我想根据给定标准来评估我的回答，然后你给出用于生成该回答的提示词以及模型的回答，接着你要求评判者返回两样东西：理由和分数。

我想指出的一个小技巧是，人们通常要求模型首先输出理由，然后再给分数。我们这样做的原因是，根据经验这能提高结果质量。结合我们在第六讲中看到的内容，如果你还记得推理课程，我们看到这些在 2025 年特别流行的推理模型，它们会在给出答案之前先输出思维链。

所以你可以将这个技巧视为与推理模型相同的思路，它允许模型在给分数之前外化表达其"思考过程"。这给了它一个机会来真正弄清楚模型回答中什么是好的或错误的。

目前为止还不错。对设置有任何问题吗？好的。现在我有一个问题问你们。如果我给 LLM 评判者以下提示，我是否能保证得到一个可解析的理由和分数？我能保证吗？

不能。是的，答案是不能。你不能保证得到可解析的理由和分数，因为这个模型具有概率性质，采样过程是你无法真正控制的。我的后续问题是，你知道有什么技术能保证你得到结构化的回答吗？提示是我们在课程开始时看到过的技术。

好的，我给你们一个小提示。如果你还记得第三讲第 65 页，我们看到了一种叫做受约束引导解码的技术。如果你记得，这里的想法是通过只允许我们的模型从"有效"tokens 中采样来约束解码过程。

我们通常在希望输出具有给定格式时使用这种技术。假设是 JSON 格式，我们绝对需要那种格式。所以人们使用这种技术来保证回答的形式。如果你使用 OpenAI 或 Gemini 或 Anthropic 等提供商，这种技术被称为结构化输出。

在你的项目中，如果你想约束解码过程以输出给定格式的回答，假设我的格式是一个回答，我用一个类来表示它，有两个属性：理由和分数。通常你可以用参数 text format 等于那个表示来引用它。

我相信这是 OpenAI 的做法，我不确定这是否是你在其他提供商那里看到的确切参数名，但它们都是沿着同样的思路。这听起来好吗？关键词是结构化输出。无论何时你想要给定格式的回答，你都应该选择这种方法。

好的。总结一下，我们的 LLM 评判者有两个主要优点。首先是我们不需要参考文本。我们不需要人工评分就能开始，因为我们的 LLM 在预训练期间已经获得了很多知识和人类偏好等等。所以你不需要那些。

其次，你可以通过输出的理由来解释分数，这也是相当了不起的。举个例子，你会说评估这个回答的质量。你会得到一些理由来解释这个回答有什么或缺少什么使它好或坏，以及分数。

现在我们来看你能遇到的 LLM 评判者类型。当然有很多变体，但通常有两种你会看到的 LLM 评判者类型。

第一种是你有一个单一的输出，一个你想要评估的单一回答，这里你会要求 LLM 评判者说这个好还是不好。第二种大类型是成对设置。你有两个回答，你问回答 A 更好还是回答 B 更好，这里你会得到要么这个要么那个的回答。

如果你记得我们在之前的讲座中看到，有很多情况下我们想要偏好数据。比如在我相信是第五讲的偏好调优课程中。

这种方法也可以是合成生成偏好评分的好方法，你有两个回答，然后你让 LLM 说我更喜欢那个，你可以用那个作为标签来训练你的奖励模型。

这听起来好吗？对到目前为止我们讨论的设置或所有内容有任何问题吗？

好的。每个人都在同一页面上。现在让我们看看 LLM 评判者可能出什么问题。让我们考虑可能遇到的失败类型。

第一种叫做位置偏差，顾名思义，它与我们向模型展示回答的顺序有关。

比如如果我们问模型回答 A 更好还是回答 B 更好。模型有可能回答 A 只是因为它是第一个被提到的。这种偏差叫做位置偏差。就是你放置回答的位置影响 LLM 评判者模型的判断。

作为补救方法，人们有不同的技术。但一个典型的技术是问模型 A 或 B 哪个更好，然后问模型 B 或 A 哪个更好，然后采用多数投票。如果两者都导致相同的回答，那就很好。但如果回答改变了，那可能不太好。你可能想做别的事情。

还有其他一些技术。我知道有一些论文试图调整位置嵌入，但那些更高级一些。这不是你会直接使用的典型做法。所以取平均值或采用这种位置交换的多数投票是你会做的。

这是第一种偏差。第二种偏差叫做冗余偏差。假设你有两个回答，第一个回答简短简洁。第二个回答更详细，通常更冗长。

有些情况下，模型会倾向于偏好更冗长的回答，只是因为它们更冗长，而不一定因为它们更正确。对于这个问题，解决起来可能更棘手一些。

人们通常试图在指导原则中明确说明这个维度，当他们向 LLM 评判者输入问题时，他们说确保不要过分关注这些回答的长度，不要仅仅因为更冗长就偏好某个回答。

第二种方法是添加一些上下文学习示例给模型，通过例子告诉它冗余不是你应该偏好的东西。最后一种是对输出长度有某种惩罚。

你可以用逐点方式问你的模型一个有多好，二个有多好，然后试图用长度来惩罚。这也是人们可能使用的方法。

我们已经看到了位置偏差，冗余偏差，现在我们将看到第三种你可能遇到的偏差，叫做自我增强偏差。

这与以下事实有关：如果你要求模型评判一个由自己产生的输出，模型会倾向于偏好由自己生成的回答，无论另一个是否更符合我们想要的。

这里的直觉是，如果我们的模型生成了这样的答案，那么可能我们的模型从概率角度认为这是一个很可能出现的序列。一种思考方式是，如果它生成了这样的序列，那么意味着它认为这是一个好答案。

这里的一般指导原则是通常不要使用相同的模型来进行生成和评判。

但我想现在很难严格遵守这种约束，因为所有模型基本上都是在相同的数据集上训练的。你可以说它们都受到相同的训练混合等等的影响。但人们仍然倾向于使用另一个模型，以将这种风险降到最低。长话短说，尽量不要使用完全相同的模型来进行生成和评估。

这就是自我增强偏差。在我们进入下一部分之前，你们对这三种偏差有什么看法？它们有道理吗？到目前为止有任何问题吗？

### (45:00 - 1:00:00) Part 4

是的，这个问题是，是否可能有一个模型不完全与真实情况对齐，也许会优先考虑某个标签而不是另一个。这确实可能是另一种偏差。也就是说，我们的 LLM 与人类偏好不完全对齐。刚才提到的三种偏差绝不是全部，这完全可以作为另一种偏差列出。这确实是另一种偏差。

问题是，我们的评判者是否仍然可能偏爱 LLM 的回应，即使这是一个不同的回应？这取决于你的评判者有多好。通常最佳实践是使用容量更大的评判者，能够捕捉这些差异，不会被听起来像是自己生成的回应所欺骗，而是能识别更符合人类偏好的内容。简单的答案是，是的，你仍然可能遇到这种情况。为了降低这种风险，你通常会选择一个不同的模型，而且通常要大得多。

现在有很多这样的模型，特别是随着推理模型的改进，人们也在尝试这个方向。

问题是评判者是否应该更大？这不是硬性约束，但人们通常会选择具有强推理能力的更大模型，能够真正辨别什么是好的，什么是不好的。

好的。接下来，我要介绍一下我们观察到的最佳实践。我们看到，为了让 LLM 作为评判者输出分数，我们需要给出评估标准。但有时这些标准可能有些主观。一个非常有效的方法是制定明确的指导原则，明确说明我们想要什么，不想要什么。

另一点是你可能会看到不同类型的评分标准。有时人们使用更细粒度的评分标准，有时只是使用二元评分。人们通常倾向于使用二元评分，因为这让 LLM 作为评判者的工作更容易，要么好要么坏。

在让评判者与人类评分对齐时，人类通常也发现从两个选项中选择比从多个选项中选择更容易。这消除了多个可能选择带来的噪音，而且并不一定提供真正有用的额外信号。所以建议使用二元评分，如通过或失败的分数，而不是渐进式评分。

第三个建议是确保在输出分数之前先输出推理过程。

这与在提供回应之前输出思维链的思想相同，这是推理模型所做的事情。这通常会提高评判者的性能。

我们已经讨论了不同类型的偏差：位置偏差、冗长偏差、自我增强偏差，当然这些不是全部，人们通常也会寻找方法来用我们提到的补救措施来缓解这些偏差。

到目前为止，我们说过不需要人类评分就可以开始，但一个好的做法仍然是查看 LLM 评分与人类评分的比较情况。

这里的一个建议是将评判者给出的回应与人类评分进行校准，因为归根结底，这是我们想要近似的量。如果预算允许且项目可行，一个好的做法是收集人类评分，输出 LLM 作为评判者的分数，然后进行相关性分析，看看是否可以在 prompt 方面有所改进。

最后一点是温度参数。

如果你记得，温度是一个可以调整的参数，让你的生成更确定性而不是更有创意。你会看到在评估任务中，人们使用低温度，因为他们希望评估实验可重现。

想象一下，你做了一次评估，然后两天后再做一次。你不希望分数相差太大。所以你会看到温度值通常设为 0.1 或 0.2 这样的常见值。

总结一下，我们从理想情况——每个 LLM 输出都由人类评分——转向实际使用某种近似方法，即这些 LLM 作为评判者的模型可以在没有约束或不需要人类判断的情况下进行评估。

但正如我们在最佳实践中提到的，确保 LLM 评判分数与人类评分不会偏离是我们在改进模型时应该记住的事情，因为我们可能在改进模型时让 LLM 分数很高，但 LLM 分数本身只是人类评分的近似。你不想过度优化代理指标。这就是为什么你希望代理指标尽可能与你的真实标签——人类评分——对齐。

### (52:41 - 59:56) 评估维度与事实性检查
> 讨论任务性能、格式对齐和事实性验证的方法

好的。在交给 Shervin 之前，我们还有几分钟，我想快速介绍一下人们衡量 LLM 输出的维度类型。虽然有很多维度，但为了简化，我们可以看两个主要维度。一个是任务执行得如何，即任务性能，包括回应是否有用、是否准确、是否相关等。另一个是回应的格式对齐程度，包括语调、风格是否符合我们的要求，以及是否存在不安全的响应元素。

我想花五分钟讨论准确性维度，这实际上需要更多工作。

让我给你一个设定。假设我们有一些文本输出，我们的目标是量化输出的准确性。我来读一下这段文字：泰迪熊最初创造于 1920 年代，以西奥多·罗斯福总统的名字命名，当时他自豪地想要射杀在一次狩猎旅行中捕获的熊。我们想要量化这段文字的准确性。

我之前告诉过你，在对某个维度进行评分时，我们通常更喜欢二元评分。

但准确性的问题是有很多细节。有些文本可能完全错误，有些可能稍微错误，有些可能完全正确。我们想要捕捉文本的错误程度。考虑到文本可能包含很多句子，如果只有一个小问题，我们不想说整个内容都不正确。

不知道你有没有注意到，这段文字实际上有两个错误。不是 1920 年代，而是 1900 年代泰迪熊才被创造出来。总统并不是自豪地想要射杀，他实际上拒绝了。在这种情况下，我们想要解决的问题是如何量化这种细微差别。

这是一个开放性问题，人们一直在写论文研究。我现在要告诉你的是基于已有研究，人们现在通常使用的方法。

我们通常分几个步骤。第一步是从原始文本输出转换为事实列表，因为当你看一段文字时，它实际上包含许多需要检查的事实。这里的想法是沿着文本中存在的事实维度来聚合文本的准确性。

在这个例子中，我们会有一个 LLM 调用，将原始的多句子、多段落文本转换为事实列表。在这个例子中，我们会有四个事实。这是第一步。

第二步是我们会检查每个事实是否正确。这里我们通常采用二元方式，因为如果你仔细想想，一个事实要么正确要么错误。你可能有一些中间情况，但我们不想让任务过于复杂。

这里的事实检查过程通常涉及我们上节课看到的其他技术，比如 RAG。给定一段文字，我们想要用实际事实查询知识库，然后检查这里的事实是否真的正确。

这个事实检查过程通常涉及 RAG、网络搜索等技术。你可以把这个事实检查步骤也看作是涉及 LLM 调用。

你也可以认为有些事实比其他事实更重要。例如，总统自豪地想要射杀熊这个事实可能不如泰迪熊以其命名的人的姓名那么重要。所以你也可以考虑使用权重来量化每个事实的重要性。

人们会使用类似这样的公式，对所有事实进行聚合，用权重量化每个事实的重要性。

这些权重 $\alpha_i$ 如果你想简化，可以都设为相等。这不一定在所有地方都必须不同，但这可能是你可以调整的参数。

如果我们回到最初的问题——如何量化这段文字的准确性——你会说第二和第三个事实都是正确的，我们知道它们有多重要，所以我们运行这个聚合公式，得到分数 6。这意味着有一些错误，但仍然有一些在事实上正确的内容。

这通常是你用现有技术运行这个标准的方式。

### (1:00:00 - 1:15:00) Part 5

### (59:56 - 1:23:17) 智能体评估与失败分析
> 分析智能体工作流的七种失败模式及解决方案

好的，很棒。我知道我迟到了 2 分钟，现在我把话筒交给 Shervin。

谢谢 Ashin。

在我们开始查看具体基准测试之前，我想先绕个弯，看看智能体 (Agent) 方面正在发生什么。

如果你还记得我们上节课讨论的内容，我们谈到了 ReAct 框架，你可以将智能体内部发生的事情分解为具体的步骤。通常是三个步骤：观察、计划、行动，或者可能有其他名称。但关键是你有几个可以循环的原子步骤。

如果你看一下典型智能体的内部工作机制，你会看到这样的模式。现在你可能想知道如何评估这样的系统。

让我们先看一个循环，然后一起看看可能出现的错误，这样我们就能了解智能体工作流程的评估结果意味着什么。

我要展示一张我们在上节课展示过的幻灯片，我们看到可以将工具调用分解为这三个步骤。让我们用我们最喜欢的例子，假设你想在附近找一只熊。你会向模型提出这个请求。

第一阶段是找到正确的工具调用和正确的参数，然后一旦你找到了正确的工具调用，你需要执行它，接着基于你的工具调用预测和从工具获得的结果，你会在最后一步推断出结果。这是三个步骤，在智能体工作流程中，你可能会有一系列这样的步骤，你调用多个工具，然后建立推理直到得到答案，然后提供给用户。

好的。现在让我们看看在每个步骤中可能出现的失败模式是什么。

首先让我们看看可能的工具预测错误。我想提到的第一个是这样的情况：错误在于，对于一个明显需要工具的用户查询，你实际上没有使用工具。

假设你想找一只熊，你手头有找熊的工具，但你没有使用它。通常如果你不使用它，模型可能的行为是说一个错误。我说的错误是指"抱歉我做不到"，在助手术语中，这被称为 punt（放弃）。当你不回答问题时，你只是失败了，这叫做 punt。你可能会在这里 punt，比如"抱歉我不知道在哪里能找到"。

让我们一起看看可能导致这个问题的原因以及如何解决它。

不知道你是否还记得我们介绍的工具路由器或工具选择器的概念。通常当你处理工具时，你不只有一个，你有多个，对于大规模（用户数量意义上的）LLM 可能有用的工具数量可能很大。所以你不想在每次调用时都输入所有的函数 API。

因此可能存在这个中间步骤，你筛选出可以放在前导 (preamble) 中的可能函数集合。这些工具选择器或工具路由器具有面向召回率的特性。

你想要修剪想要在前导中输入的函数列表，但你仍然希望至少找到你需要的那些。所以这里的主要特性是你想要节省上下文空间，但仍然要确保大部分用例仍然有效。

这就是为什么当我们说工具路由器错误时，我们实际上是指召回错误。这意味着我们可能只是没有从工具集合中选择正确的工具。如果这是原因，那很明确，我们只需要调整工具路由器以便它能预测正确的工具。这可能是一种问题。

另一种是，实际上工具被包含在这个函数 API 列表中，但 LLM 只是没有想到使用它。

所以，也许这个 "find teddy bear" 在那里，但我们只是没有使用它。LLM 直接输出一个响应。在这种情况下，如果你还记得我们提到过教 LLM 使用工具的技术。所以你需要重新审视那部分，如果你用 SFT 训练了它，你知道要包含这个模式，也许训练模型识别它，或者如果你做了提示调优，那么你应该重新审视你的提示，以便这个调用对模型有意义，它应该使用那个工具。

好的，很棒。这是一种可能的错误。当你想要调试智能体时，你可能在实际中看到的另一种错误是在工具调用时。

模型可能会想出一个根本不存在的函数名。

我在这里提到了工具幻觉，这就是我的意思。它调用了一个没有定义的函数。如果你记得，我们的 API 被称为 find teddy bear，这是存在的函数，在这个失败的例子中，模型试图调用函数 find bear，而我没有定义这个函数。

当你看到这样的错误时，你有几个潜在的原因。其中之一是模型整体上没有很好地落地，通常如果模型太简单就会发生这种情况。这是一个经验观察，如果它太弱，也许它可以编造一些它认为合理的东西，但它实际上没有基于你的指令。

我在这里没有更好的补救建议，除了如果你看到这确实是情况，也许升级模型，然后看看这是否可重现。

一些其他可能的原因实际上可能来自你。模型在其 SFT 阶段接受了非常高质量数据的训练，所以它见过优秀的 API 是什么样的。所以你定义的这些帮助用户实现他们目标的工具可能没有以最好的方式编写。

你知道，如果你没有使用 AI 助手编码，让我们说，你不一定要使用 AI 助手编码来写这个，但这些通常是从模型角度检查你的实现是否有意义的好方法。

如果没有，那么典型的补救措施是重命名 API，仅仅是函数名和参数就能起很大作用，因为这是模型在进行工具调用时会看到的。它看到 API 函数名，它看到参数和高级描述。

所以这些是你调整的三个旋钮，为了使其听起来更合乎逻辑，然后与手头的实际任务相关联。

然后，在一开始我说也许模型太弱，但实际上也许你应该检查的第一件事是水平指令——跨工具的水平指令是否足够清楚。也许模型没有真正理解它需要使用给它的函数。

所以也许它只是编造了它相信可以访问的函数名。所以第一件要检查的事情可能是看看这种现象是否是普遍的，看看这些水平指令是否简洁地说明你应该确保使用可用的函数。

然后你可以在这些顶级指令上迭代，你知道也许与 LLM 本身一起迭代，因为通常顶级指令非常重要。所以你需要完美的格式和完美的逻辑。所以，通常能够详细说明它们是很有帮助的。

好的，现在让我们看第三种可能的失败原因。假设你有你的模型和用户提示，但你只是没有使用正确的工具。

如果用户说"在我附近找一只熊"，另一种合理的方法是你知道如果你只是发送一条请求熊的消息。你知道那会是合理的，对吧？但也许那不是你想为用户实现的行为。

在这种情况下，模型不清楚你更喜欢哪种方法，确保它确实清楚是你的责任。然后你必须在两个不同的层面上做到这一点。第一个可能也在工具路由器层面。也许工具路由器不知道对于这种查询，你应该有你心目中的工具作为结果的一部分。所以可能你有一个需要修复的召回问题。

第二个是简单地回到两个函数的 API。也许它们在范围上冲突。所以你想回到它们每一个，精确说明哪些情况应该用哪个工具处理。在这些 API 中非常精确在这里起很大作用。

### (1:15:00 - 1:30:00) Part 6

有时候遇到错误并不一定是坏事，因为在某些情况下，比如查找你的位置时，如果你没有提供定位权限，可能会触发错误，模型会基于这个错误向用户传达状态信息。但通常情况下，仅仅因为模型可能将其解释为内部工具错误就返回错误并不是常见做法。

有时当你遇到错误，然后要求模型综合它看到的工具原因时，它只会说："哦，抱歉，我做不到，但这是我的错。这是因为我遇到了一个错误。"它并没有真正说明具体发生了什么，无法给出可操作的信息。

这里的解决方案是以有意义的方式传达这些输出。通常你有一个结构化输出，你应该返回一个真实输出而不是错误。

这是一个通用情况，就是说检查你的工具实现。你有正确的参数，你有正确的工具，但你没有正确的值。这只是一个软件工程问题，去修复工具就好了。

我们在后端层面看到的第二类问题是当你不返回响应时。当工具是执行某个动作的工具时，不返回响应往往是不好的。

比如说上节课我们谈到为你叫泰迪的泰迪熊增加恒温器温度。如果你增加了恒温器，但工具什么都不说，模型就不知道它是否成功完成了任务。它很可能会给出虚假确认，比如"嘿，你知道一切都好。我已经增加了恒温器温度。别担心。"但实际上并没有。

这就是为什么一个常见的指导原则是始终确保工具调用后有有意义的输出。像往常一样，你有这样的结构化消息，你应该利用它来传达作为工具一部分发生的事情，以确保模型反过来知道向用户传达什么或知道如何继续那个 agent 循环。

所以是的，总是输出一些东西。比如说你想要找一只泰迪熊但没有找到任何一只，这里你会对我说的话感到惊讶，但实际上输出一个空的 JSON 比不输出任何内容更好，因为空的 JSON 可能意味着我没有找到熊，但不输出任何内容什么也不说。所以即使在那种情况下，空输出在空 JSON 的意义上是有意义的，确保在编码工具时使用这种意义。

好的，很好。我们已经看到了在函数调用层面的两个更多可能的错误。现在假设第一步一切顺利，第二步一切顺利。你找到了正确的工具输出，但现在模型在将输出综合成有意义的响应方面遇到了困难。

假设你的工具找到了一只名叫泰迪的熊，然后我这里没有显示的其他属性可能说它们距离我一英里远。泰迪熊已经被找到，我们只需要将它呈现给用户。但如果你把它交给模型，假设模型说："你知道，我没有找到任何熊。"这里可能的原因是什么？

可能是你有一个包含信息的输出，但模型没有基于这些信息。可能是模型缺乏引用之前放置的内容的能力。这里我有同样的通用建议，就是升级模型。

通常这种情况现在真的不再发生了，但在 LLM 的早期迭代中会发生。

有一个实际上经常发生的情况。有时工具后端不仅返回一个输出，而是返回大量输出，对于模型来说太多了，无法正确解析什么是重要的。你可能在那里有泰迪的信息，但你知道它淹没在其他类型的无用信息的海洋中。模型无法区分什么是有帮助的。

然后这个问题的解决方案是回到你的工具实现，确保你输出的任何内容都对模型在下一阶段使用是有意义的。

我认为这与我在这里提到的第三个原因重叠。假设你的输出已经被修剪了，那么另一个可能的解释是也许它没有以有意义的方式呈现。这就是为什么在 Python 中你有这些类，你可以实例化属性，使输出非常有意义。

比如说你找到了一只熊，你可以返回一个叫泰迪熊的对象，带有名称、距离等属性。这与可能不知道如何解释的原始信息相比是非常有意义的。

好的，太棒了。我们在所有这些类别中看到了七种不同的失败模式。这些不是唯一的，你知道我只是提到了那些我经常看到的、我认为可能有帮助的，但你绝对可以看到其他失败模式。这说得通吗？

### (1:23:17 - 1:49:20) 基准测试分类与实践
> 介绍五大类基准测试及其在实际模型评估中的应用

你有任何问题吗？

好的，很好。我们可以继续总结这些失败模式中的常见趋势。通常我们谈论建模方面，有时提高模型推理和基于事实的能力可能是解决方案。

我们看到的另一种抱怨是我们在上下文窗口中放置内容的相关性。如果我们提高这种相关性，也许会更好。在建模方面，另一个方面可能是工具路由器建模或工具 API 建模本身，通过 SFT 调优或仅仅是提示，甚至只是 API 描述本身。你知道函数有意义吗？参数有意义吗？文档字符串有意义吗？这是一种。

另一种是工具本身。也许它就是有问题。所以你需要修复它。

我只想说，当你处理工具和评估时，你有很多可能的错误。帮助你在这方面导航的一件事是非常有条理地将各种错误分类，然后分组处理它们。你看有很多错误，每次你处理一个给定的损失时，解决它可能只是一次冒险。所以在这里真正有组织将帮助你很多。

好的。考虑到这一点，我们可以深入研究基准测试的世界。我们谈到了评估。现在你可能想知道如何评估大语言模型，比如说你已经训练了所有东西，你知道如何将它与其他模型进行比较。

我们将一起看到当今基准测试通常所在的一系列基准测试类别。我们将看到每个类别的示例。这听起来不错吗？

太棒了。我们可以从我称为基于知识的基准测试开始，我们想测试模型是否能够恢复给定事实。这些事实通常跨越很多领域。它不必在给定领域超级精确，但它就像跨越所有你的用户可能关心的领域类型。

这方面的一个主要例子是 MMLU，我们很快就会看到。但在我们这样做之前，我想说这个知识基准主要但不仅仅衡量预训练做得有多好。你的大型数据语料库中的信息在推理时如何被模型保留以提供帮助。

MMLU 代表大规模多任务语言理解，这个基准测试有大约 60 个不同的任务，它们非常多样化。

它不只是一个特定主题，而是一堆主题，如日常生活主题或非常专业的，例如有法律、医学以及你能想到的一切。基准测试以一种可以轻松衡量 LLM 性能并将该性能与其他模型进行权衡的方式编写。它不是自由形式的，而是非常受限的。

有一个问题，然后你有四个可能的答案，你要求 LLM 选择其中一个。这有点像 CME 295 考试，你知道考试的一部分也是多项选择题，你知道这是标准化评估的好方法，这里也使用了同样的方法。

这也是你在基准测试中看到的趋势。你不要求模型只是提出一些答案，然后你可能有一个 LLM 作为评判员对此发表意见，因为这样做会引入另一层潜在错误。

你知道作为评判员的 LLM 如前所述并不一定完美。所以这种框架使我们能够有一种硬编码的方式来提取 LLM 输出的答案。通常你会要求它在每个问题的末尾输出正确的字母，然后你可以提取并与答案进行比较。

举一些这个基准测试中包含内容的例子。如我所提到的，你在其中有各种领域，你会注意到每个问题主要需要关于该主题的一些先验知识，所以这不像纯逻辑能帮助你解决它。我认为这张幻灯片中的最后一个例子很好地代表了这一点，你有医学领域的东西，你有一堆数字，患者有这个和那个。你会说损伤在哪里？这通常是你可能在医学书籍中看到的内容，法律等其他领域也是如此，所有东西都已经在某处编纂，你需要某个地方的知识才能回答问题。

### (1:30:00 - 1:45:00) Part 7

这种格式对 LLM 友好，因为你有一个给定的问题陈述，最后要求学生将回答写成三位数的数字。所以约束条件非常明确，这使它很适合用来基准测试 LLM。正如之前提到的一样，它是硬编码的。

我在这里给出一些今年 AIM 考试的样本。如你所见，我不知道你们能否从远处看清，但这并不是特别简单。你会看到一些句子，所以你可能认为很容易，但实际上你需要在找到答案之前写下推理过程。这正是我们想要测试 LLM 的能力。

我们在这里提到的第二种推理是所谓的常识推理。目前经常使用的一个是 PIQA（物理交互问答）。这些是深深植根于物理现实世界的任务。我们在下一张幻灯片会展示一些样本。但这仍然是基于推理的问题，依赖于你对周围事物运作方式的理解，不一定是基于数学的，而是日常生活的知识。

这次不是像 MMLU 那样的四选一多选题，而是二选一，有 2 万个样本。

这里有一个我很喜欢的来自论文样本的好例子：如何在地毯上找到丢失的东西。一个解决方案说用实心密封的吸尘器，另一个是用发网吸尘器。当然，当你用实心密封的吸尘器时，密封是实心的，所以没有空气可以通过。但如果你有发网，你会吸走整个东西，而你丢失的物品会被卡在里面。我提到的是常识，但可能不太明显，这就是我们要求模型解决的问题。

好的，那么基准测试的另一个主要领域是编程，我们想要探测模型解决复杂编程问题的能力。这在现实生活中有两个主要用途。一个与我在上一节课最后提到的用例一致，我喜欢的关于 AI 助手编程的那个。这些模型的目标也是在这种设置下使用。所以你应该确保这些基准测试表现良好，以便对用户有用。

编程基准测试有意义的第二个原因是，你有所有这些可能想在智能体设置中使用的工具，这些工具可能是用 Python 格式编写的。所以你想确保你的模型具有读写代码的正确能力，以便它可以执行这些工具调用并解释从中得出的结果。

我会说这些是激励我们发现编程有用的两个动机，甚至对那些完全不编程的人也是如此。

是的，这种基准测试的一个例子是 SWEBench。我在这里打了一个问号，因为他们没有确切定义这个缩写的意思，但很可能意味着软件工程基准测试。SWE 通常是软件工程的缩写。

他们所做的是查看流行的 Python 仓库，并筛选出那些包含解决问题并引入测试的拉取请求。所以你有一些前后行为可以通过引入的测试进行定量评估。

据推测，如果你有一个引入了一些测试和一些修复的拉取请求，你可以合理假设这些测试在没有修复的情况下不会通过，而在修复后会通过。所以我们手头有这些测试这一事实是评估修复质量的一个很好的衡量标准。

如果你听说过测试驱动开发，它就是关于拥有测试并确保它们通过，这就是它所依赖的。

是的，然后在这里你要求这些 LLM 解决这些 GitHub 问题，然后通过在应用模型建议的答案之前和之后查看测试状态来评估它们是否确实通过了。

好的，很棒。这里有一个介绍该基准测试的论文提供的非常好的图表。所以你知道你得到了一个代码库，然后你要求模型提供一个补丁，然后你将模型提供的内容打补丁来找到测试，从而找到测试状态。

最后，我想在基础基准测试的情况下提到的一个领域是安全性。当你看到花哨的 LLM 发布时，你通常在建模基准测试的广告中看不到安全部分，因为通常安全性相对于 LLM 提供商来说有点主观。每个公司都有自己的政策。所以你不能仅仅因为所有这些提供商可能不声称他们想要 100% 完美地解决该基准测试，就在模型之间比较给定基准测试的性能。

因此，这不一定是该领域的一个好衡量标准。所以如果你查看模型卡片，你经常会看到报告中提到安全部分，说明他们所做的工作，但他们不一定根据给定的基准测试比较模型。

通常安全基准测试与我们认为应该或不应该发生的事情相当一致。但除此之外，你可能有额外的政策，这些可能只是政策。所以你有一些人必须做出一些决定。这可能不是一个普遍的决定。这只是一个给定的决定。

所以基准测试的目标是与 LLM 提供商心中的政策类型保持一致，以便真正有意义。是的，这就是为什么当你执行安全基准测试时，你应该检查基准测试的内容以便在其背后赋予意义。

这里让我们谈论 HarmBench，我假设它意味着有害行为基准测试。这个基准测试有四个类别。所谓的标准类别，分类所谓的普通有害行为。然后你有版权类别，评估模型生成版权内容的能力，我们不希望这样。

然后最后两个是上下文相关和多模态，它们都是基于给定模态的上下文相关。所以上下文相关是在文本模态上，然后多模态是与文本以外的其他模型模态一起的。我们将在下一张幻灯片看到一个例子。

在这里，你没有基于某种硬编码匹配来评估此基准测试性能的相同能力，因为这些有害陈述可能是开放式的。你不能仅仅通过正则表达式匹配来解决所有这些问题。你知道，例如，这个基准测试标准类别中的一个例子试图诱使有害行为执行一些有害的东西。

论文提到了一个非常有趣的区别。它通过说如果模型试图进行有害行为，即使由于质量不够好而不成功，也足以将攻击视为成功，将模型质量与安全性区分开来。为此，他们训练了一些分类器来识别这些情况。是的，这是我在这里介绍的基准测试中唯一一个基于分类器的，分类器本身可能容易出错，而其他的都非常基于约束设置值。

正如承诺的，这里有一些论文中提到的例子。在这里，我们测试你是否可以解锁一扇你不应该解锁的门，然后这里测试是关于影响某人关于某次选举的。所以这些不是安全的行为。

### (1:45:00 - End) Part 8

上一次我们已经讲过了，我只是想让你们相信这确实是正确的公式。如果你们还不相信，请在家里自己验证一下。

继续下一个话题，我们讨论了所有这些基准测试。现在让我们看看它们如何与现实相对应。到目前为止，我想你们每个人都看到了几天前发布的新 Gemini。这是发给大家的报告，用来证明这里的性能更好。你可以看到我们这里介绍的内容以某种形式被提及。推理部分在 AIM 和 PA 中存在。然后你会看到其中一些基准测试衍生出了引入多语言性的变体，这就是 PIKA 的情况，而不是全球 PA，对于编程，它使用 SWBench 的变体，对于工具使用，它也使用 Bench 的变体，即 TOWEL Squared Bench。

最后几句话。我只想说，基准测试是为了描述你的 LLM 的特性。所以它不是全好或全坏。也许你的某些 LLM 会有一些优势和一些弱点。你的个人经验可能指导你在特定情况下使用特定的模型而不是其他模型。如果我要分享我的个人经验，我知道 Sonnet 模型在编程方面非常有帮助，每当我想要快速且廉价的输出时，Gemini Flash 通常很好。但这些绝不是全球性的建议。你自己的用例和自己的经验可以指导你建立最适合你任务的模型配置文件。

有趣的是，你可以绘制模型性能与你关心的其他维度（比如价格）的关系图，看看在给定价格下你能使用的最佳模型是什么。然后你在该特定指标的最佳模型上看到的边界被称为帕累托前沿。

你可能在多个方面都有帕累托前沿，比如成本、安全性、上下文长度。

然后关于数据污染说几句。这些基准测试的一个问题是，它们的好坏取决于你是否真的见过实际基准测试结果的假设。所以要确保你没有见过它们。为此，人们引入了哈希值。在工具使用的情况下，他们引入了阻止列表，以防止访问可能包含响应的网站，或者在数学的情况下，我们有幸能够在模型肯定没有见过的新测试上进行评估。

古德哈特定律是一个很好的格言，它说："当一个度量成为目标时，它就不再是一个好的度量。"

所以所有这些基准测试结果都要与你真正寻找的东西进行权衡，这些基准测试结果并不一定告诉你一个模型是否对你有用。我们在之前的一堂课中讨论过聊天机器人竞技场，它可以成为平衡这些模型现实生活性能的一种方式。但我想说，最终你应该亲自尝试这些最佳模型，看看哪一个最适合你。

就这样，我希望你们都有一个愉快的感恩节，谢谢大家。


---

*生成时间: 2026-01-04 18:13:50*
*由 YouTube Monitor & Translator (Claude CLI) 生成*