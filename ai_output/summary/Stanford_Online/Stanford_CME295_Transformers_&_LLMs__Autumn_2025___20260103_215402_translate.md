# Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 4 - LLM Training

## 📹 视频信息

- **频道**: Stanford Online
- **发布日期**: 2025-10-21
- **时长**: 1:47:23
- **原始链接**: [https://www.youtube.com/watch?v=VlA_jt_3Qc4](https://www.youtube.com/watch?v=VlA_jt_3Qc4)

---

本文内容整理自斯坦福大学在Stanford Online频道的《CME 295: Transformers与大语言模型》课程第4讲 - LLM训练。

---

## TL;DR

斯坦福大学CME 295课程深度讲解LLM训练全流程：从预训练的万亿token规模数据处理，到分布式计算优化（数据并行、模型并行、Flash Attention），再到监督微调和高效参数调优技术（LoRA、量化），系统阐述了现代大语言模型从零到部署的完整训练体系。

---

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-07:17 | 课程介绍与上节回顾 | 介绍期中考试安排，回顾MOE架构和推理优化技术 |
| 07:17-24:30 | LLM预训练基础 | 从传统任务特定模型到迁移学习范式，预训练数据规模和成本分析 |
| 24:30-31:12 | 预训练挑战与扩展 | 预训练的成本挑战、知识截止日期问题和硬件需求 |
| 31:12-38:28 | 分布式训练：数据并行 | 数据并行和ZeRO优化技术，解决GPU内存限制问题 |
| 38:28-52:20 | Flash Attention优化 | 利用GPU内存层次结构的IO感知注意力计算优化 |
| 52:20-59:22 | 量化与混合精度训练 | 浮点精度表示、量化技术和混合精度训练策略 |
| 59:22-1:09:03 | 监督微调(SFT)概述 | 从下一词预测到指令遵循，监督微调的数据和目标函数 |
| 1:09:03-1:26:23 | SFT数据与评估挑战 | 指令调优数据类型、分布外泛化和模型评估的复杂性 |
| 1:26:23-1:37:15 | 评估基准与对齐 | MMLU等基准测试、ChatBot Arena排名机制及其局限性 |
| 1:37:15-1:47:23 | 高效微调：LoRA技术 | LoRA低秩适应技术原理、实现细节和量化LoRA优化 |

---

## 📊 核心论点

#### LLM预训练的规模定律：Chinchilla最优配比

- **核心内容**：Chinchilla论文通过大规模实验发现，给定固定计算预算，模型参数量与训练数据量存在最优配比关系，约为1:20（即20倍数据量对应1倍参数量）。GPT-3使用1750亿参数训练3000亿token被认为是"训练不足"的。现代LLM如Llama 3使用8B参数训练15万亿token，更接近最优配比。这一发现改变了业界从单纯增加模型规模转向平衡参数和数据规模的策略。
- **关键概念**：计算最优性、参数-数据比例、训练效率、样本效率、Chinchilla定律
- **实际意义**：指导LLM训练资源分配决策；影响模型架构设计；成为评估训练策略的重要指标；推动从"越大越好"到"最优配比"的范式转变。

#### Flash Attention：内存IO感知的精确注意力计算

- **核心内容**：传统注意力计算在长序列下面临O(n²)内存复杂度瓶颈。Flash Attention利用GPU内存层次结构（HBM vs SRAM），通过分块计算（tiling）将注意力矩阵分割为小块，在快速SRAM上完成端到端计算，最小化慢速HBM的读写次数。核心技巧是增量softmax计算，无需完整矩阵即可精确计算softmax。结合重计算策略，在增加计算量的同时显著减少内存使用和IO操作，实现速度和内存的双重优化。
- **关键概念**：分块计算、内存层次结构、IO感知算法、增量softmax、重计算策略
- **实际意义**：突破长序列处理瓶颈（支持100k+ tokens）；降低训练和推理成本；成为现代LLM标准组件；催生Flash Attention 2/3等后续优化。

#### 分布式训练：数据并行与ZeRO优化

- **核心内容**：LLM训练需要存储模型权重、激活值、梯度和优化器状态，单GPU内存（如H100的80GB）无法满足大模型需求。数据并行将批次数据分散到多GPU，每GPU维护完整模型副本，通过梯度聚合保证一致性。ZeRO（Zero Redundancy Optimizer）进一步消除冗余：ZeRO-1分片优化器状态，ZeRO-2分片梯度，ZeRO-3分片模型参数。虽然增加通信成本，但实现显著内存节省，使大模型训练成为可能。
- **关键概念**：数据并行、模型并行、ZeRO分片、梯度聚合、通信开销
- **实际意义**：使数千亿参数模型训练可行；降低硬件门槛；影响模型架构设计；推动分布式深度学习发展。

#### 监督微调(SFT)：从语言建模到指令遵循

- **核心内容**：预训练模型虽掌握语言结构，但缺乏遵循用户指令的能力。SFT通过高质量指令-回应对训练模型，改变损失函数计算方式：仅对输出部分计算损失，输入部分不参与梯度计算。数据包含多样任务（故事创作、诗歌生成、数学推理、代码编写、安全对齐）。与预训练相比，SFT数据量小（千万级样本 vs 万亿token），但质量要求极高，通常需人工标注或高质量模型生成。这一步骤将通用语言模型转化为有用的AI助手。
- **关键概念**：指令调优、监督学习、损失函数设计、数据质量、任务泛化
- **实际意义**：实现从语言模型到AI助手的关键转换；定义现代LLM产品形态；影响用户体验和模型实用性；成为AI安全和对齐的重要环节。

#### LoRA：低秩适应的高效微调

- **核心内容**：传统微调需要更新所有模型参数，计算成本高且存储需求大。LoRA（Low-Rank Adaptation）将权重更新分解为低秩矩阵乘积：ΔW = B×A，其中B和A的隐藏维度r远小于原始权重维度（通常r=4-8，而原始维度为数千）。冻结预训练权重W₀，仅训练LoRA权重，参数量减少数百倍。前向传播计算W₀x + ΔWx = W₀x + BAx。实践中需要更高学习率（10倍）和较小批次大小，最新研究显示前馈层比注意力层更适合应用LoRA。
- **关键概念**：低秩分解、参数高效微调、权重冻结、任务特定适应、计算效率
- **实际意义**：大幅降低微调成本；支持多任务模型部署；推动个性化AI发展；成为工业界标准微调方法。

#### 量化与混合精度：精度与效率的平衡

- **核心内容**：模型权重通常使用FP32表示，但许多应用场景下FP16甚至更低精度足够维持性能。混合精度训练保持权重为FP32高精度，但前向和反向传播使用FP16，权重更新时转换回FP32。量化技术进一步压缩模型：4位量化（如NF4）假设权重服从正态分布，使用分位数而非固定区间量化，双重量化还压缩量化常数。GPU硬件对不同精度有不同计算能力（FP16比FP32快2倍），实现内存节省和速度提升的双重收益。
- **关键概念**：数值精度、混合精度训练、量化算法、硬件加速、内存优化
- **实际意义**：显著降低训练和推理成本；提高硬件利用效率；支持边缘设备部署；推动模型压缩技术发展。

#### 模型评估的复杂性：基准测试与用户偏好

- **核心内容**：LLM评估面临多维挑战：1）传统基准（MMLU、GSM8K等）易被针对性训练影响，存在"训练在测试任务上"的问题；2）ChatBot Arena等人工评估存在噪声、可操控性和主观偏见；3）用户偏好高度个人化（如对emoji的态度分歧）；4）安全性与有用性的权衡。专家标注与普通用户判断存在系统性差异。评估分布与实际使用分布不匹配导致基准表现与用户体验脱节。单一评估指标无法全面反映模型能力，需要多维度综合评估。
- **关键概念**：基准饱和、用户偏好、评估分布偏移、多维评估、主观性量化
- **实际意义**：影响模型开发方向；指导产品设计决策；推动评估方法创新；强调以用户为中心的评估理念。

#### 训练数据的质量与规模权衡

- **核心内容**：LLM训练呈现明显的数据质量与规模梯度：预训练使用海量低质量数据（万亿token级别的Common Crawl、Wikipedia、GitHub等），目标是学习语言基本结构；SFT使用高质量小规模数据（百万级样本），每个样本都经过精心设计或人工标注。数据成本随质量指数级增长：网络爬虫数据几乎免费，专家标注数据成本极高。现代LLM越来越依赖AI生成的SFT数据，使用已训练模型生成候选回应，再通过人工或模型审查筛选，这种"自举"方法平衡了成本与质量。
- **关键概念**：数据质量梯度、人工标注成本、AI生成数据、数据自举、质量控制
- **实际意义**：定义训练策略和成本结构；推动数据工程发展；影响模型能力上限；引发AI数据依赖性讨论。

#### 硬件优化与算法协同：计算效率的系统性思考

- **核心内容**：现代LLM训练需要硬件与算法的深度协同优化。GPU内存层次结构（HBM vs SRAM）、计算能力（不同精度的FLOPS差异）、通信带宽等硬件特性直接影响算法设计。Flash Attention利用内存层次，混合精度利用硬件加速能力，分布式训练平衡计算与通信。算法创新（如重计算、梯度累积、流水线并行）与硬件发展（如TPU、专用AI芯片）形成正反馈循环。未来LLM训练效率的提升需要算法、硬件、系统软件的整体优化。
- **关键概念**：硬件算法协同、内存层次优化、计算通信平衡、系统性优化、软硬件协同设计
- **实际意义**：指导AI基础设施发展；影响硬件设计需求；推动系统软件创新；塑造AI计算生态。

#### 迁移学习范式的范式转变

- **核心内容**：LLM代表了机器学习从"任务特定"向"通用基础模型"的根本性转变。传统方法为每个任务（垃圾邮件检测、情感分析等）训练专门模型，LLM采用"预训练+微调"的两阶段范式：第一阶段学习通用语言表示，第二阶段适配特定任务。这种范式的优势是知识复用、快速适配和零样本泛化能力。预训练模型成为"AI基础设施"，支持各种下游应用。但也带来新挑战：知识截止、灾难性遗忘、微调数据需求等。
- **关键概念**：基础模型范式、两阶段训练、知识复用、零样本学习、AI基础设施
- **实际意义**：重塑ML工程实践；降低AI应用门槛；推动AI民主化；影响整个AI产业结构。

---

## 🔬 提及的技术/方法/论文

| 技术/论文 | 讨论语境 | 重要性 |
|----------|----------|--------|
| Mixture of Experts (MOE) | 稀疏激活架构，降低推理成本 | ⭐⭐ |
| KV Cache | 推理优化技术，避免重复计算 | ⭐⭐ |
| Scaling Laws for Neural Language Models (2020) | 计算-性能关系，模型规模指导 | ⭐⭐⭐ |
| Chinchilla Optimal | 参数与数据量最优配比（1:20） | ⭐⭐⭐ |
| ZeRO (Zero Redundancy Optimizer) | 分布式训练内存优化 | ⭐⭐⭐ |
| Flash Attention (2022) | IO感知的精确注意力计算 | ⭐⭐⭐ |
| Mixed Precision Training | FP16/FP32混合精度训练策略 | ⭐⭐ |
| LoRA (Low-Rank Adaptation) | 参数高效微调方法 | ⭐⭐⭐ |
| QLoRA (Quantized LoRA) | 量化LoRA，16x显存节省 | ⭐⭐ |
| MMLU (Massive Multitask Language Understanding) | 多任务语言理解基准 | ⭐⭐ |
| GSM8K | 数学推理评估基准 | ⭐ |
| ChatBot Arena | 基于人类偏好的模型排名 | ⭐⭐ |
| NF4 Quantization | 正态分布假设的4位量化 | ⭐⭐ |
| Prefix Tuning | 参数高效微调变体 | ⭐ |
| Adapter Methods | 另一种参数高效微调方法 | ⭐ |

---

## 💬 经典金句

> "Pre-training is by far the most expensive both in terms of compute cost, you know, everything part of the training."
> — 课程讲师

> "Memory is not unlimited. Memory is limited."
> — 课程讲师（谈及GPU内存限制）

> "Flash attention is quite a common trick and I think it's a very good thing to know."
> — 课程讲师

> "The more compute you have, the better your model learns about predicting the next token."
> — 课程讲师（谈及扩展定律）

> "Evaluation is a hard problem... it's not one number that's going to tell you what you care about."
> — Shervin（谈及LLM评估挑战）

---

## 👤 主要人物

#### 课程讲师（第一部分）
**身份**：斯坦福大学CME 295课程讲师
**背景**：专注于大语言模型训练和优化技术，具有深厚的机器学习理论和实践经验
**核心观点**：强调LLM训练的系统性优化，从数据规模、计算资源到算法创新的全面考量。重视理论与实践结合，注重学生对核心概念的深度理解。特别强调Flash Attention等Stanford本校创新技术的重要性。

#### Shervin（第二部分讲师）
**身份**：斯坦福大学CME 295课程讲师
**背景**：专门研究LLM微调、对齐和评估方法，对监督微调和参数高效方法有深入研究
**核心观点**：强调从语言建模到AI助手转换的关键作用，重视数据质量对模型性能的决定性影响。对模型评估的复杂性有深刻洞察，认为单一评估指标无法全面反映模型能力，需要多维度综合考量用户需求和实际应用场景。

---

## 📺 视频类型判断

**教程示范**：斯坦福大学正式课程，系统性技术教学，包含理论讲解、实践指导和学生互动问答。

---

## 📝 完整翻译

### (0:00 - 7:17) 课程介绍与上节回顾
> 介绍期中考试安排，回顾MOE架构和推理优化技术

大家好，欢迎来到 CME 295 第四讲。今天是 10 月 17 日星期五，这意味着期中考试就在一周后。在开始之前，我想先过一下一些后勤安排，确保大家都了解期中考试的安排。

期中考试将在下周同一时间进行。考试时长是一个半小时，而不是平常的一小时五十分钟。时间是下午 3:30 到 5:00，就在这间教室里。

考试内容涵盖第一讲、第二讲、第三讲以及今天的第四讲。

期中考试会包含一些选择题和一些开放性问题，但主要考查我们在课堂上讲过的内容。如果你观看了录像或参加了课程，并且复习了幻灯片，掌握了重要的公式，我觉得你应该没问题。

我知道你们到下周可能还会有问题。所以这节课结束后，我和 Shervin 会举办答疑时间。欢迎大家来问任何问题。当然，从现在到下周我们都会全力配合。如果有任何问题，欢迎在 Ed 上联系我们，我们会确保及时回复。

我也知道有一些同学在旁听这门课。如果你们因为某种原因仍然想参加期中考试，也许是因为有即将到来的面试，请告诉我们，这样我们可以预估需要打印的考卷数量。我们会在周一打印考卷，所以如果你们有兴趣的话，请在这个周末告诉我们。

关于期末考试，我们之前说过正在确定日期。现在日期已经确定了，没有变化。是 12 月 10 日星期三，晚上 7:00 到 8:30。地点与这里不同，是在另一个教室。

期末考试只涵盖课程的下半部分，基本上是第五讲到第九讲。

学生：有什么问题吗？

好问题。考试是闭卷吗？是的，闭卷考试。

问题是选择题的格式是什么？我们还没有完成试卷的编写，但大概是你有一个问题，然后有三四个可能的答案，你选择正确的那个。这样的形式。你们还会有一些开放性题目，需要用你们自己的话来回答。

学生：我们可以带任何东西吗？

闭卷考试，所以什么都不能带，只能带一支笔。

问题是不允许带计算器吗？你们不需要计算器。

不过说到备忘单，我不确定我们之前有没有提到过，我想我们提到了。这次考试有一个备忘单，我们不能把它带到考试中，但你们可以用它来学习。它在课程网站上。我建议大家看一下。

好的，大家都清楚了吗？很好。

和往常一样，我们开始这节课时先回顾一下上节课的内容。如果你们还记得的话，我们基本上学习了一种新的架构，叫做 Mixture of Experts (专家混合)，这种架构的特点是，当你有输入时，你不一定需要激活所有参数，所以你有多个专家，在前向传播中只激活其中一些，这就是稀疏 MoE。你也有密集 MoE，它基本上根据门控的输出对输出进行加权。

我们看到这种架构被用在 LLM 中，主要是为了能够扩展这些 LLM，而不会在推理时产生昂贵的成本，因为你不想激活所有参数。

我们看到的第二个内容是定义什么是 LLM，特别是如何决定下一个 token 的预测。我们看了三种方法。第一种叫做贪心解码，总是选择概率最高的 token。

第二种方法是 beam search (束搜索)，我们跟踪 k 个最可能的序列。

第三种是采样。我们不是选择最可能的，也不是跟踪概率最高的序列。我们做的是根据我们得到的输出分布采样下一个 token。然后我们看到有一个叫做 temperature (温度) 的超参数，允许你调整你想要的分布有多尖锐。

### (7:17 - 24:30) LLM预训练基础
> 从传统任务特定模型到迁移学习范式，预训练数据规模和成本分析

我们还看到了一些在实践中使用的推理优化技术，以避免在解码时产生很大的成本。我不会提到所有内容，但我想说 KV cache (键值缓存) 是一个重要的方法。我建议大家了解一下这个以及其他方法。

说到这里，我们要开始第四讲了。我真的很期待今天的内容，因为第一讲我们看到了什么是自注意力，什么是 Transformer。第二讲，我们看到了人们今天使用的一些技巧和 Transformer 的一些变体。上一讲我们介绍了什么是 LLM，这一讲我们终于要看到这些 LLM 是如何训练的了。今天我们要专注于 LLM 训练。

我要说的第一件事是，如果你在机器学习领域已经超过几年了，你可能已经注意到，传统上，如果你有一个任务，你会专门为那个任务训练一个模型。

比如 10 年前，假设我们有一个检测垃圾邮件的任务。你会专门训练一个模型来检测垃圾邮件。所以你会在训练集上训练，在验证集上评估，然后在测试集上测试。如果你有另一个用例，比如情感提取，你会专门为此训练一个模型，以此类推。

但是人们可以争论说，这些任务并不是完全不相关的。它们都涉及理解文本。所以人们可以争论说，我们可以找到一种方法，以某种方式利用我们在训练期间获得的知识，比如对一个任务，然后将其重用于另一个任务。

这种方法有一个名字。它已经存在了一段时间，叫做迁移学习 (transfer learning)。

迁移学习的目标是不要总是从头开始。如果你有一个新任务，要从某个预训练模型开始。我们将看到什么是预训练，然后针对你的任务调优它，而不是从头开始。

这基本上就是 LLM 训练的范式。这里的想法是所有这些任务都涉及理解语言。所以，我们要做的是有一个我们称之为预训练阶段的东西，它涉及在大量数据上训练你的 LLM，以理解什么是语言，什么是代码，然后有第二个调优阶段。

我们将看到调优是什么。但在第二个阶段，我们将采用我们的预训练模型，并以某种方式找到调整权重以适应特定任务的方法。

作为这里的一个例子，我们会预训练一个巨大的模型，然后假设对于垃圾邮件检测，我们会以某种方式为此调优它，情感指令也一样，我们会为此调优它，以此类推。

这只是取之前的例子。这里的想法是，为了获得这些模型，我们不会从头开始。

好的。现在我们要看看什么是预训练。预训练是迄今为止最昂贵的，无论是在计算成本还是其他训练成本方面。

它所做的是取大量数据并训练你的 LLM 来预测下一个 token。

这里的数据是指基本上你能找到的一切。可以是英文文本，可以是其他语言的文本，甚至可以是代码，可以是不同语言的代码，基本上可以是整个互联网。

我们将看到人们用于此目的的一些数据集。但你可以把这看作是训练你的模型来尝试预测任何写出来的东西。

如我所提到的，这里的目标是预测下一个 token。如果你记得，我们的 LLM 是一个文本到文本的模型，很可能在超过 90% 的情况下是一个仅解码器模型。所以它做的是接受一些输入文本，并尝试在迭代的基础上总是预测下一个 token。

就使用的数据集而言，你会在论文中经常看到 Common Crawl 这个术语，它基本上是一个由你在互联网上能找到的任何东西组成的数据集。我想他们每月有大约 30 亿页面。如果你去他们的网站，他们有一个巨大的档案。还有很多其他网站你也可以在那里找到。

例如，Wikipedia 文章、任何社交媒体如 Reddit，我知道那些数据集中有很多 Reddit 对话，你有很多代码，当然你有很多地方可以获得，你有 GitHub，你有 Stack Overflow，所有这些讨论代码的论坛，所有这些都是为了让你的模型理解语言和代码的结构。

在规模方面，它是以 token 数量来衡量的，我希望你们记住的一个数量级是数千亿甚至数万亿甚至数十万亿 token 的量级。

我给你们一个例子。GPT-3 是在 3000 亿 token 上训练的，例如 Llama 3，我相信是去年发布的，是在 15 万亿 token 上训练的。

这些都是巨大的数据集。

### (15:00 - 30:00) Part 2

你可以把它想象成一个函数，它关于token数量和参数数量的函数。大致是两者乘积的O复杂度。

接下来我想介绍第二个记号，同样是FLOPS但含义不同。这里FLOPS代表每秒浮点运算次数（floating point operations per second）。

这是计算速度的度量，基本上是指你的硬件能以多快的速度执行这些运算。这里也有一些数量级的概念。如果你了解GPU的话，你会看到在GPU的规格描述中，它们总是标明FLOPS，我们待会会看到。

但我想指出的是，这里的FLOPS通常是全大写的。虽然你可能会在一些论文中看到两种用法混用，这确实令人困惑。所以我建议根据句子的上下文来理解这个记号，因为有时人们确实会搞混这两个概念，但这是常见的记号规范。

目前为止还好吧。

很好。现在我们知道了有一个预训练步骤，我们知道它涉及大量计算，涉及大量数据，我们知道模型很大。所以人们开始尝试研究性能如何随着模型大小和训练规模的变化而演变。

有一篇2020年发表的论文叫《神经语言模型的缩放损失》，通过改变这些参数进行了大量实验。他们发现计算资源越多，模型预测下一个token的能力就越好。数据集大小也是如此，训练集越大越好。模型越大也越好。

所以在2019年到2024年这段时间，你会看到模型越来越大，人们就是在构建越来越大的东西，因为根据这些实验，性能确实在不断提升。

他们还注意到的另一点是，更大的模型往往具有更高的样本效率。

这意味着在处理相同数量的token时，大模型相比小模型会有更好的性能表现。

但接下来你可能会想，我们没有无限的计算资源，计算是昂贵的，有很多缺点。所以如果你有固定的计算预算，人们也试图回答这个问题：给定一定的计算量，你如何以更优的方式确定训练集大小和模型大小？

因为这里你需要决定模型有多大。所以他们做的是固定一个计算单位，也就是这些曲线的颜色，然后尝试用不同的训练集大小训练不同大小的模型。他们发现总是存在一个最佳点，遵循某种关系。

特别是，这张表总结了所谓的最优参数数量和训练集大小的设定，有时称为Chinchilla规律。

他们意识到，如果你的训练集大小大约是模型大小的20倍，那么你就在以所谓的"最优"方式使用计算资源。特别是GPT-3，我记得是1750亿参数，但它只在3000亿token上训练。所以按照这个标准，GPT-3实际上是"训练不足"的。

我想有个问题。

问题是他们是否适配了神经架构。我认为现在大家都认同LLM是基于Transformer的纯解码器模型。所以每个人都使用相同的模型。

是的，所以你可以假设当我说LLM时，基本上指的是纯解码器的基于Transformer的模型。

问题是架构变化是否起重要作用。这正是他们在论文中所说的。他们说变化最大的是训练的token数量和模型大小。

还有其他问题吗？

好问题。问题是不同版本模型之间是否存在某种迁移学习？

对于很多这些模型，它们实际上是闭源的，所以他们不会准确透露这些信息。但这确实是个有趣的问题。

我无法以一般性的方式回答这个问题。也许这是我能给你的最佳答案。但无论如何，当你查看这些论文时，它们总是说明训练成本是多少，总是数百万美元的量级。无论如何，这总是一个昂贵的步骤。

很好。说到这个，预训练面临很多挑战。其中之一是成本。当我说数百万美元时，这是最低限度。我认为甚至可能花费数千万美元，有时甚至数亿美元。它需要很长时间，人们也一直在关注对环境的影响。所以他们也开始考虑生态成本。

另一个挑战是预训练步骤使用的数据只能到预训练模型的时间点。这意味着你从训练这个数据集中获得的知识只能到你切断数据集的日期。

这个日期称为知识截止日期。这意味着你的基础模型、你的预训练基础模型本身无法知道这个日期之后发生的知识。

说到这一点，很多论文都试图编辑知识、注入知识。这总是很棘手，因为没有一种清晰的方法来改变权重而不惩罚某些部分。我想人们想要做的是注入知识但不在其他域中退化。这是一个非常困难的问题。

当然，你知道这些模型试图预测下一个token，这就产生了一个问题：如果它只是生成在训练时见过的内容怎么办？我们称之为抄袭，所以总是存在风险。

这些都是我刚才想要说明知识截止日期时的挑战。如果你访问OpenAI网站或Google网站查看模型卡片，你总会看到——我不确定你们从这里能否看到——但总是有一行关于知识截止日期，告诉你这个模型的预训练是什么时候完成的。比如这里，GPT-4在几周前发布，这里说知识截止日期是9月30日。所以你可以猜测他们在那个阶段完成了预训练。

很好。

第一部分有什么问题吗？

大家都好吗？

完美。在第一部分中，我们看到预训练是LLM训练过程的关键步骤，我们看到了所有这些大数字，人们可能会想，如何在如此大量的数据上训练如此大的模型，人们是如何做到这的？

这就是我们接下来要看的。

正如我提到的，你可以把LLM看作是纯解码器的基于Transformer的模型。要训练你的模型，你需要大量数据，但如果你看看架构，你会发现很多操作都涉及矩阵乘法。

我想问你们一个问题：什么样的硬件最喜欢矩阵乘法？

GPU。是的。所以你还需要GPU。实际上不止一个。你有问题。

问题是关于推理用的GPU。这里我们专注于训练。但对GPU的要求在训练和推理之间略有不同。但在这一部分，我们完全专注于训练。

说到GPU，我想不是到处都是GPU，因为比如Google开发了自己的硬件叫TPU。但任何非Google的模型，它们很可能都是在GPU上训练的。

### (24:30 - 31:12) 预训练挑战与扩展
> 预训练的成本挑战、知识截止日期问题和硬件需求

很好。为了训练你的模型，你要做什么？首先，你有你的LLM，现在我们用一个方框来表示只是为了简单起见，你初始化它，它有很多参数，你可以想象规模大概是数十亿到数千亿参数。一个巨大的模型。

训练模型涉及哪些步骤？你试图调整权重，让模型能够学习如何生成下一个token。

你有一个叫做前向传播的步骤，你有一批数据试图通过网络传递。在做这个的时候，我想指出一些重要的需要注意的事情，我们需要在内存中保存某些东西。

当你做前向传播时，你有叫做激活的东西，基本上是每一层需要用来计算损失的值。

损失告诉你相比你想要训练的标签，你偏离了多少。你在这里使用的内存量取决于很多因素：它取决于模型大小，这影响激活的数量；它取决于训练时数据批次有多大；它取决于上下文长度有多长，因为如果你记得，由于self-attention操作，我们有$O(n^2)$的复杂度，其中n是序列长度。所以你有所有这些参数在起作用。

一旦你完成前向传播，假设你计算了损失，你知道相对于标签你偏离了多少。下一步是以某种方式调整权重来最小化损失。你如何做到这一点？有另一个过程叫做反向传播。

这个过程量化了损失最小化的方向。这叫做梯度。你计算损失对每个参数的梯度。

梯度也需要保存在内存的某个地方。

然后你最后进行权重更新，这是你知道损失最小化方向的地方。所以你将该更新应用到你的权重上，通常你使用像Adam优化器这样的优化器。你们听说过Adam优化器吗？

Adam优化器只是一个花哨的版本，有一些额外的量来跟踪，这些基本上是梯度的函数。所以你有第一矩和第二矩，基本上是梯度和平方梯度的移动平均，以及所有这些量。所以第一矩、第二矩，你也需要在内存的某个地方保存它们。

### (30:00 - 45:00) Part 3

所以有很多东西需要保存。好吧，突发新闻：内存不是无限的。内存是有限的。所以我们面前看到的是一个 GPU 的描述。我想是的，是 H100，这是一个非常好的 GPU。你会在描述中看到有一行关于 GPU 内存的信息。GPU 内存是每个 GPU 的内存容量，这个是 80GB，相当大。所以大约是几十 GB 的数量级。

你需要在 80GB 中存储所有这些东西，这并不算多。

### (31:12 - 38:28) 分布式训练：数据并行
> 数据并行和ZeRO优化技术，解决GPU内存限制问题

那么我们在做什么？你们会做什么？

我想这个想法是利用不止一个，而是多个 GPU，以某种方式在 CPU 之间分配负载。为了做到这一点，你有几种方法，我们稍后会看到。

第一组方法叫做数据并行 (data parallelism)，也称为 DP。这组方法的作用是在 GPU 之间分配数据，这样前向传播和反向传播都可以独立完成。

这里的想法是将数据批次在设备间分割。为了做到这一点，当然你需要在每个设备上都有一个模型的副本，因为你需要计算激活值，你需要计算所有这些东西。但是当你这样做时，你能够减少与批大小相关的内存。

所以这就叫数据并行。是的，问题是梯度更新怎么办？这是个很好的问题。

那么当你在各处有独立计算时你会怎么做？梯度只是这些梯度的平均值。所以你在 GPU 之间有一些通信，基本上聚合梯度用于更新。

我有个问题问你们。这是解决一切问题的答案吗？比如如果我们像这样扩展到很多 GPU，这是否总是很好，还是我们有一些缺点？

哦是的，很好的观点。所以是的，你必须适配一个模型，这是很好的观点。我要补充的第二点是你有额外的成本，叫做通信成本，因为你需要在 GPU 之间进行某种通信来聚合一些量。

所以你的训练会更慢。这很好，你可以扩展内存。当然你需要在设备上适配一个模型，我们会看到如何做到这一点，但你会产生这些通信成本，所以并不是完全好的。

说到内存以及我们想要能够至少在每个设备上存储一个模型的事实。所以人们意识到实际上有很多重复，已经有论文想要消除这种重复信息，这种方法叫做 ZeRO (Zero Redundancy Optimization)。

想法是在每个 GPU 上，你存储相同的参数，你存储相同的梯度，你存储相同的优化器状态。

所以这里的想法是，我们如何分片，在 GPU 之间分割这些量。第一个变体是围绕共享优化器状态。意思是我们在 GPU 之间分割这些状态。这大大减少了内存。我们也可以分割梯度，我们也可以分割参数。

所以这里我们没有冗余信息。东西只是被分割了。问题是你会有更多的通信成本，但至少它允许我们减少每个 GPU 上的内存负载。

所以这就是 ZeRO。有 ZeRO-1、ZeRO-2、ZeRO-3。我想你选择的变体将是你对训练时间敏感程度的函数，以及你的模型有多大，以及这是否真的是个问题，或者你只是可以存储一切。

所以这是一组方法。这组方法又是数据并行。所以基本上你有由不同 GPU 处理的独立数据集。

### (38:28 - 52:20) Flash Attention优化
> 利用GPU内存层次结构的IO感知注意力计算优化

你还有另一组方法叫做模型并行 (model parallelism)。模型并行试图并行化甚至在一个批次内的操作。

有很多方法。我不想听起来像目录，所以我们不会逐一介绍，但我会指出一些值得注意的。

如果你记得上一讲我们谈到了基于 MOE 的 LLM 以及序列如何被发送到不同的专家。有一种方法可以通过专家并行技术在 GPU 之间分发，即在一个设备上有一个专家，在另一个设备上有另一个。

这是值得注意的一点。另一个我要说的是 tensor parallelism，当你有大的矩阵乘法时，以某种方式切分以减少所需的内存。

也许我要说的最后一个是 pipeline parallelism。这是当你将前向传播视为涉及多个层时。你会说一个 GPU 只负责比如第 1、2、3 层，然后另一个负责第 4、5、6 层，依此类推。

所以你也有这种并行性，但无论如何有很多技术，我提到的这些都属于模型并行的范畴。

有意义吗？不需要了解详细信息，但我认为知道有几种方法以及粗略的想法是值得牢记的好事。

很好。我们做了什么？我们意识到在训练过程中，我们必须在内存中保存很多东西。我们看到的是减少每个 GPU 内存负担的技术。我们试图在 GPU 之间分发。我们看到了数据并行，然后是有一些额外优化的 ZeRO 方法，我们也看到了模型并行。

现在我们要看另一种利用 GPU 结构的技术。你可能听说过这种技术叫做 Flash Attention。它实际上是 2022 年在斯坦福开发的。

为了向你们介绍这种技术，我想告诉你们更多关于 GPU 组成的信息。如果你看看底层，GPU 非常复杂，我肯定也不了解所有东西，但我知道的是我们在 GPU 中有两种内存。你有一种内存很大但相对较慢，在 HBM 中，然后另一种内存很快但小得多，在芯片上靠近计算发生的地方，叫做 SRAM。

所以你有 HBM 和 SRAM。HBM 有大约几十 GB，就像你在描述中看到的 GPU 内存。SRAM 要小得多，大约几十 MB 左右。所以它小得多，但速度快 10 倍。这个是每秒几 TB，而 SRAM 是每秒几十 TB。所以速度差异很明显。

我们想要的是以某种方式利用这些内存的优势，以精确的方式加速注意力计算。我说的精确方式是什么意思？我的意思是我们不对计算做任何近似。我们所做的只是利用这些组件的优势，以聪明的方式安排计算。

### (45:00 - 1:00:00) Part 4

因为如果你仔细想想，假设你有一个完整的矩阵，然后你有不同的，比如说列或子矩阵 S1 到 SN，那么这个大矩阵的 softmax 就等于这个矩阵，其中 softmax 是对每个子矩阵分别计算的，乘以某个缩放因子。

这就是核心技巧。如果你想确信这一点，只需看看 softmax 公式，它就像某个数的指数除以在行中共享的某个量。所以这个缩放因子会相应地修正这一点。

考虑到这一点，我们要做的是取这些矩阵的每个相应切片，进行整个计算，然后填充输出矩阵中相应的条目。所以我们会在查询的第一个切片与键和值的第一个切片之间进行计算，然后对其他切片重复这个过程直到结束，然后对其他查询也重复这个过程直到结束。

论文解释的是如何计算这个缩放因子。这个公式我没有放在幻灯片上。你不需要记住这个公式，重要的是理解这个思想，而思想正是这个技巧。

一旦你这样做了，你基本上只需要从 HBM 读取一次，这些分块的量存储在 SRAM 中，然后从 SRAM 中读取，速度非常快，进行计算后再返回到 SRAM，最后为了累积结果，它们被发送回 HBM。

为了确保我们理解清楚，绿色部分基本上表示从 SRAM 读取，蓝色部分表示从 HBM 读取。

**学生问题**：你是取整行还是其中的一部分？

你可以取其中的一部分，但这里只是为了说明目的。你可以把你的矩阵想象成一个网格，然后相应地进行乘法。这只是为了说明目的。

**学生问题**：你是在运行中计算 alpha 和所有这些量，还是有一些估计？

这些都是精确的，它们以迭代的方式计算。工作原理是当你填充输出时，你会不断有一些额外的量来调整。把它想象成某个有效的公式。

我强烈推荐看一下论文。他们实际上解释得相当详细。论文标题是"Flash Attention: Fast and Memory-Efficient Exact Attention with IO-Awareness"。所有链接都在幻灯片中。如果你想确信这一点，我强烈推荐查看论文中的确切公式。

总的来说，这个想法是有道理的。对此有什么问题吗？

好的，这就是 flash attention。

但论文实际上还有另一个想法。刚才只是第一个想法，是关于让注意力计算更快的。第二个想法是，既然注意力计算更快了，现在让我们在反向传播方面更聪明一些。

因为当你在反向传播中计算损失相对于参数的梯度时，链式法则会涉及到一些你需要在内存中保存的激活值。在这里，考虑到计算这些激活值非常快，一个想法是不要存储前向传播的激活值，或者至少不要存储所有东西，而是在反向传播中重新计算这些激活值。

这叫做重计算。

当你做前向传播时，你计算激活值以便能够计算损失，但与其保存它以在梯度更新期间重用，你会直接丢弃它，然后在反向传播期间使用这种非常快的技术重新计算激活值。

当你这样做时，结果非常显著。你确实进行了更多的操作。千兆浮点运算（gigaflops）是浮点运算的派生，表示你正在进行的操作数量。使用 flash attention 你会进行更多操作，因为你实际上在重新计算东西。

但你从 HBM 的读写次数更少。例如，在标准方法中是 40.3，而现在是 4，几乎减少了 10 倍。但你还看到运行时间也更短了。

### (52:20 - 59:22) 量化与混合精度训练
> 浮点精度表示、量化技术和混合精度训练策略

这非常显著，因为通常当你重新计算东西时，你确实节省了内存，但代价是运行时间，你花费更长的时间。但这里你不仅花费更少的时间，还节省了内存。你基本上拥有了一切，这是所有世界中最好的。

这就是 flash attention 提到的。你会看到有一些 flash attention 的变体，比如 flash two、flash three。我认为这些更多是这些方法对当前基础设施的适应，因为每个新的 GPU 都有新的优缺点、优势和劣势，总是有方法让这些优化变得更好。

我认为 flash attention 是一个相当常见的技巧，我觉得了解它是件好事。

这说得通吗？我把这当作"是的"。

好的，最后一件事。不，我们还有几件事要讲。你知道，你有你的 LLM 和一堆权重。这些权重都是浮点数。你可以把它们想象成在小数点后有一堆数字的数字。一个自然的问题是，你真的需要知道小数点后那么多精度才能做好工作吗？换句话说，你能以某种方式削减精度以节省内存，同时保持相同的性能吗？

这就是量化背后的想法。量化是将数字的精度从一个设置转换到另一个设置的过程。为了更好地理解这一点，我认为了解浮点数是如何编码的很重要。

在实践中，它们只是一堆位，这里有一些位负责指数，一些负责尾数（基本上是你的数字有多精细），还有一个关于符号的位。

你有一堆浮点数的表示方法。最常见的在这个表格中。你有单精度、半精度、浮点 64、brain float 16，它们在三个维度上都有不同的精细度。

如果我们看前两行，这是你在外面会看到的两种最常见的表示方法，浮点 16 只用 16 位表示，而不是 32 位。所以如果你这样想的话，它基本上占用一半的内存。但它不太精确，意味着不太精细。所以我们可能有的一个想法是以某种方式降低这些权重和这些数字的精细度，希望它不会太影响性能。

我想提到的另一件事是回到那个 GPU 描述，你看到还有很多关于计算速度的信息。我不确定你能否在幻灯片上看得很清楚。我来读一下。计算速度作为你使用哪种表示方法的函数。如果你使用 FP64，这是表示数字的超精细方式，你只有 34 teraflops 的计算速度，但如果你使用 FP32，是它的一半，你可以大致双倍你的计算速度，等等，你还有所有这些其他数字。

想法是你可以节省内存，也可以运行得更快。

考虑到这一点，在交给 Shervin 之前，我只想谈一下最后一个技术，就是混合精度训练。

混合精度训练背后的想法是利用不同精细度的浮点表示，这样不会太影响性能，但允许你节省内存并更快地做事情。

混合精度训练背后的想法是你有你的模型，你将权重保持在高精度，即 FP32，而你在前向和反向传播中进行的所有操作，你将以较低精度进行，在这种情况下是 FP16，但权重更新仍然以 FP32 进行。

论文作者这样做时，他们基本上意识到性能没有降级太多，但你在内存上有很多节省，运行也更快了。

现在你可能想知道，为什么你将权重保持在高精度，而不是激活值等。我可以给你一点直觉。

每当你执行前向传播时，你是在一组数据上执行的，但数据集本身可能是有噪声的。所以也许小数点后的所有数字可能不那么有用。

你可以把这个更新想象成权重应该朝哪个方向移动以达到更优状态。所以它不需要超级精确，但保持模型权重的高精度要重要得多，以免由于量化而累积错误。

这是一种思考方式。长话短说，如果你减少一些数字的精细度，你会有很多好处，在性能方面不会有很多劣势。

**学生问题**：你是将这种技术应用于所有层的所有权重，还是只应用于其中一些？

这是个好问题。关于这个有很多论文，答案是有一些变体。答案不一定是。我有一些参考资料很乐意分享给你。但有些部分可能比其他部分更重要。

这个策略在某种程度上是一个高层想法，但从设置到设置总是有一些变化。

**学生问题**：你的问题是人们依赖于 chinchilla 论文中计算和参数与 token 之间的关系，但它可能与他们的设置不同，这可能引入一些东西，这基本上是你的问题吗？

### (1:00:00 - 1:15:00) Part 5

### (59:22 - 1:09:03) 监督微调(SFT)概述
> 从下一词预测到指令遵循，监督微调的数据和目标函数

问题是是否也存在最优的精度设定？我认为人们使用不同的方法，没有一个固定的标准做法，但这些缩放定律，比如 chinchilla 定律，实际上一些作者会尝试为他们自己的模型进行复现。

实际上，比如 LLaMA 3 论文中有整整一个部分尝试建立某种关系，即在给定固定计算量的情况下，什么是最优的 token 数量和参数数量，这对他们的设置来说是独特的。

人们所做的是在小规模的训练集和较小的模型上自己进行这种实验，这样成本不会太高。他们尝试找出在他们的设置下的关系，然后进行外推。我相信 LLaMA 3 就是这样做的。我记得有一个 350 亿参数的模型，他们有整整一个章节来论证他们通过一些实验得出这个数字。所以回答你的问题，我认为这很大程度上取决于某人正在训练的模型类型，他们可能会针对自己的设置进行这种分析。

这回答了你的问题吗？好的。

关于范围，你能做些什么吗？有几种量化类型。我们这里不会详细讨论，但我可以给你零点量化和绝对最大值量化，这些是处理这些范围的不同技术。还有 Shervin 会讲到的量化技术也涉及这个。所以请继续关注，但我认为零点量化和绝对最大值量化是你可以研究的方法。

好的，接下来我把话交给 Shervin。

Shervin：谢谢 Afin。现在我们已经看到了预训练是如何工作的。我们将一起来看训练的下一个阶段。正如我们与 Afin 看到的，预训练是一种让模型建立对语言如何构建的直觉的方法，以及关于训练数据中包含的主要特征的理解，这些特征是足够通用的。这就是为什么我们有跨越互联网上通常能找到的巨大文本语料库。通常非常大且原始，传达着语言的各个方面。

现在你可能想问自己，我们如何让这样的知识变得真正有用。这是我们马上要讨论的内容。但在我们讨论什么可能激发进一步训练之前，我想回到一个非常简单的例子，这将揭示这种需求可能是什么。

让我们用我们最喜欢的泰迪熊例子，假设我们有一个非常实际的用例。我们非常喜爱我们的泰迪熊，但现在它变得有点脏了。所以我们需要清洗它。你可能想问你最喜欢的 LLM 是否可以把它放进洗衣机。根据 Afin 提到的 LLM 预训练描述，我想问大家，你们认为这个输出可能是什么？

### (1:09:03 - 1:26:23) SFT数据与评估挑战
> 指令调优数据类型、分布外泛化和模型评估的复杂性

有什么猜测吗？是的，很好。猜测可能是另一个问题。我认为这是一个很好的猜测，绝对可能是这样。在这个例子中，我们放了一些其他可能的句子，但要点是一样的。它是在下一个 token 预测上训练的，而不是成为助手或对你有帮助的人。这就是为什么它会尝试模仿这里可能的模式以及可能的下一个词。

例如，我们这里放的例子涉及泰迪熊的组成，因为它与泰迪熊在同一个领域，当你谈论洗衣机时，也许它在包含泰迪熊材料的数据上训练过。所以可能是这样。我有一个问题想问大家：你们对此满意吗？

不。是的，完全正确。这就是为什么我们要看看可以对模型做什么来让它对你有帮助。

这个阶段叫做微调（fine-tuning）。我们将看到它使我们能够，就像 LLM 的一般情况一样，让 LLM 成为一个有用的助手。但如果你有特定的用例，你也可以使用这种技术将语言的一般表示调整到你感兴趣的任务上。但我们首先关注 LLM 的一般做法。

首先我想定义一些术语。模型微调通常被称为 SFT。SFT 是监督微调（supervised fine-tuning）的简称。监督部分表明我们需要一些标签，确实如此。这是我们提供给模型训练的输入输出对。这就是为什么它被称为监督的。微调是指精炼已经训练过的权重的术语。你从预训练权重开始，在额外数据上进一步训练模型以获得微调模型。

有趣的是，尽管目标函数相同，但与预训练任务会有所不同。在预训练任务中，你从 BOS token（句子开始）开始，拟合所有语料数据试图预测下一个 token。但这里由于你有这种监督微调设置，你希望模型在感兴趣的情况下做一些有用的事情。

### (1:15:00 - 1:30:00) Part 6

你可以根据处理的感兴趣任务有更多类型。但我这里列出的是通常面向通用助手的 LLM 所收集的内容。它们往往是一些通用任务，这些模型会在这些任务上进行训练。

对数据有什么问题吗？

学生：故事写作的例子没有明确说明故事应该是关于什么类型的诗歌。所以可能会有歧义。它如何泛化？

这是一个很好的观点。我们依赖模型在预训练时积累的知识，使其能够泛化到超出它接受过的示例范围。假设现在你有一个关于诗歌的故事，从预训练中它了解所有存在的各种诗歌类型。所以你可以想象，在监督微调时拟合这样一个例子后，你可以为这个 prompt 提供更多细节，然后模型就有能力根据那些其他属性泛化故事的生成。

我认为你提到的例子是 LLM 适应自然语言这种神奇能力的绝佳例证。这一切都与它过去看到的分布类型以及我们教给它的内容有关。我认为写故事这个概念本身是模型能够泛化的关键学习。

这说得通吗？好的，很好。我想这里还有另一个问题。

学生：关于对齐这个术语的问题。这个训练后的过程是被称为对齐的吗？

你抢在我前面了。几张幻灯片后我们会讨论这个。好的，非常好的问题。还有其他问题吗？

很好。

就像 Afin 为预训练部分做的那样，我要给出一些数量级的概念。Afin 引用的同样的 GPT-3 和 LLaMA-3 论文实际上并没有以 token 数量的形式给出指令调优方面的统计数据，而是以示例数量的形式。你看，GPT-3 使用了大约 13k 个示例，LLaMA-3 使用了 1000 万个。

让我们做一些快速估算，假设每个示例大约有一千个 token，当你将其与示例数量相乘时，你会看到用于 SFT 的数据集大小比用于预训练的数据集小几个数量级。

你需要有的心理模型是预训练需要大量数据。你想学习关于语言的一般特征，而 SFT 更确切地说是——刚才有人提到的关于对齐的内容——是将模型的目标对齐为适合你的任务。

通常是非常高质量的数据集，在数量方面更加简洁和精确。所以数量级更少。

好的，很好。现在让我们再次尝试这个练习。你认为我们现在经过指令调优的模型对同样问题的回答会是什么？

有人想猜测一下吗？你想再试试吗？

学生：答案是可以把泰迪熊放进洗衣机，除了你不应该把泰迪熊放进洗衣机。你应该手洗它。它会给出对用户有帮助的答案，确实这次回应了查询。

这非常不错。

现在我们讨论了监督微调的所有优点。现在我要更详细地说明什么可能使它变得困难或具有挑战性，然后激发我们之后要看到的优化部分。

首先我们稍微谈过这一点。当我们有这些数据集、这些用于 SFT 训练的数据混合时，我们需要高质量的数据，当你说高质量数据时，通常意味着人类参与循环中，你需要确保它符合用户关心的所有规则和特征。通常这是高度参与的。最初这些第一批模型几乎全部是人类生成的，但现在你可能有一些人类和生成的混合，但这些东西的一个好处是它们训练的数据集是可重复使用的。

这是你做一次的工作，你可以随着时间补充，但在时间和资源上仍然非常昂贵。

我想提到的第二点回到刚才有人提出的关于你的 SFT 数据集分布以及它如何与实际推理分布对齐的观点。在关于特定类型诗歌的故事案例中，明确指定诗歌类型的 prompt 分布似乎足够接近，可以进行泛化。但你可以想到相差很大的例子。假设你要求一个关于某些电影情节的故事，它与你可能在教科书中看到的故事不同。这可能稍微超出分布范围，在泛化方面可能会有困难。所以 prompt 分布非常重要，将该 prompt 分布与目标任务对齐是有意义的。

学生：如果你已经对模型进行了 SFT 调优，现在你把训练输入放回模型，你会得到相同的故事吗？

很好的观点。这与 Afin 提到的记忆现象有关。

在实践中，你很可能不会看到相同的故事，因为你进行的采样使用的是非零温度，它可能有相同的风格但不是逐字逐句的。然后如果你有完全相同的 prompt，该故事的风格可能是相同的。这取决于它在预训练时看到的内容。但是，如果我必须对那个精确例子给出猜测，它可能是相同的风格但绝对不是相同的措辞。

如果采样经过空间的其他区域，它可能是一个不同的故事。故事高度创造性，预训练语料库的数据混合中有各种故事。所以特定的故事例子很可能生成不同的结果。

学生：模型游走多少取决于温度参数吗？

是的。通常当你有更高的温度时，它也被称为更有创造性，正因为如此，它也可能生成在其输出分布上相对于它学到的内容不太可能的东西，这正是我们学习的情况。所以答案是肯定的。

学生：有什么方法可以调节那个吗？

当你调节温度时，你正是在做那件事。或者你在想其他什么？

学生：除了调节温度，还有其他方法可以让响应有变化吗？

对此我有一个简单但困难的解决方案。你需要在该类别中有更多数据，这将使模型基于看到我们用这种查询针对什么样的分布，我认为这将是模型更好泛化的一种方式。我认为这将在数据部分起作用。

还有其他问题吗？都好。

这是一个非常相关的问题，因为它涉及泛化的话题。

数据混合非常重要，在分布空间中有足够稀疏的点来给模型提供它需要学习什么的要点，而不是一遍又一遍地重复同样的故事，这与泛化能力有很大关系。然后这是我们马上要看到的。如何评估这样的模型？作为用户你对模型有用性的感受往往是主观的。那么如何给它打分将是一个关键的兴趣话题，然后很快我们将看到如何使计算不那么昂贵。

Ashen 谈到了训练优化技术，但现在当我们在微调阶段时，也许我们可以更进一步，看看我们可以做出什么简化假设。

好的，很棒。现在让我们深入这些挑战之一，即评估部分。人们已经将他们关心的内容分解为类别。我在这里列出了一些被评估的维度，它们给出了一些定量数字。

你有通用语言，这个流行的基准测试通常是用户报告的一个分数。这是 MMLU，即大规模多任务语言理解。它有大约 50 个任务，模型会在这些任务上进行评估，你可以得到一些可以比较的分数。你有推理基准测试，数学推理以及代码生成。这里你有各种缩写，甚至还有更多。基准测试基本上是设立基准测试一直是一个研究领域。

### (1:26:23 - 1:37:15) 评估基准与对齐
> MMLU等基准测试、ChatBot Arena排名机制及其局限性

你总是看到越来越多的基准测试出现，因为人们倾向于针对那些存在的基准测试进行优化，然后可能会有一些空白需要进一步的基准测试来填补。GSM-8K 代表研究生院。研究生院数学，8K 代表示例数量。我想是的，或者也许 G 代表其他东西，但基本上是高中话题。

当这些模型出现并根据这些基准测试进行评估时，人们看到的一个非常有趣的模式是，有时对于同一类型的模型，你会突然看到在这些基准测试中的某些数字出现飙升，而背后没有明确的解释。

有一篇我强烈推荐看看的论文，它探讨了在测试任务（不是测试集，是测试任务）上训练的现象。当你有关于数学推理的基准测试时，模型是否在与该类推理相关的数据上训练过与否非常重要。这就是为什么当你查看这些基准测试时，你会看到有一些所谓的辅助训练集可以用于模型在同一领域上训练，这样就能够比较模型在特定能力方面的表现。

这篇论文试图传达的是，如果你想比较模型之间的差异，你需要比较它们训练的训练混合，然后确保在测试任务训练方面的平等。你需要确保例如两者都在测试任务上训练过或都没有训练过。如果一个训练过而另一个没有，那么比较它们可能不是好事。它不会给你模型的内在价值。

### (1:30:00 - 1:45:00) Part 7

Let me translate this academic content directly, focusing on the technical terminology and maintaining the proper format with timestamps.

这是一个有趣的现象。有什么问题吗？

好的，很好。那我们可以继续了。我之前提到的一点是，即使是通过基准测试，也很难了解模型到底有多好，因为经常发生的情况是，随着基准测试的出现，人们会在训练侧设计更像我们试图在基准测试侧解决的数据。所以有时你最终得到的模型在各个方面都得分很高，但作为用户，你不一定能看到它的附加价值。这不是模型的错，也不是基准测试的错，只是很难给出一个能传达模型对你价值的数字。

这就是为什么人们想出了其他技术来为模型评估打分，你可能听说过 Chatbot Arena。

这里有人听说过吗？

是的。所以，这是一个可以提交模型的网站，用户进来问他们的问题，他们会看到来自两个模型的回答，然后被要求判断哪一个更好，然后通过网站侧进行的一些配对计算，最终得出一个根据所谓用户偏好对模型进行排名的排名。

所以这是一种由用户在感受层面给出的数字，它完美吗？实际上不是。它受到几个难以处理的问题的困扰。其中包括当你有一个新模型进来时，在与其他哪个模型进行比较方面，开始时会有一些噪声，而这些最初的几步实际上会相当大地影响实际排名，这使得它成为一个相当脆弱的属性。

有一篇论文实际上表明，操纵这样的排行榜是很容易的。所以我不确定是否有证据表明过去曾经这样做过，但你知道你可以使用任何模型，如果你问"你是谁"这个问题，它只会说它是谁，如果你问 GPT"你是谁"，它会说"嘿，我是 ChatGPT，我是一个有用的助手"。

这篇论文观察到这个非常简单的属性，它能够检测出正在评估哪个模型，假设你有一个对抗性玩家，它可以通过选择正确的模型来操纵排名。所以这不是万无一失的。

然后在其他方面，一些评估这些模型的基准测试，它们实际上是由非常了解给定 Prompt 应该给出什么目标分布的专家策划的。所以它有一套明确说明的指导原则，好的，这是事实的，这是非事实的。所以他们能够确定什么是好的与坏的。

而作为用户，你可能不知道这一点，因为让我们就以你想放入洗衣机的泰迪熊为例。如果我不知道我需要手洗我的泰迪熊，如果我有一个详细的回答，关于你需要用机洗冷水洗它，我作为用户可能会发现这些建议很有帮助，因为它们对我来说是可行的，但实际上它们是否事实上正确是另一个问题，作为用户你在很多查询上都无法分辨。

另一个我们面临的挑战是用户偏好。那么，这里谁喜欢 LLM 回答中有 emoji？

是的。不。

是的。是的。就我个人而言，我喜欢。我认为有很强烈的意见，有些人完全不喜欢它。所以，他们会给这样的回答降级。但实际上，这是用户应该能够告诉和选择的东西。然后选择最佳模型的人的分布相对于将要使用这些模型的更广泛人群的分布是不同的。我认为 emoji 案例是一个很好的例子。因为我认为总的来说，emoji 在更广泛的人群中是受欢迎的，但我认为领域专家可能不会那么喜欢它。所以我认为这种不匹配是你在这里会看到的。

### (1:37:15 - 1:47:23) 高效微调：LoRA技术
> LoRA低秩适应技术原理、实现细节和量化LoRA优化

然后我要提到的最后一点是安全方面。所以作为用户，当模型拒绝你的 Prompt 时，你真的不喜欢。当你询问某事时，它说，"好的，嘿，抱歉，我无法回答"。所以会有偏向于实际回应你查询的回答，而不是尊重一些最终可能是预期产品决策的安全原则。所以这里也会出现这种偏见。

正如我提到的，评估是一个难题。所以你有所有这些角度，所有这些你可以探索的角度，没有一个数字能告诉你你关心的事情。这是所有这些的组合，最终它是为你实际需要的东西量身定制的。

所以你需要看到给定模型的优势和劣势，并确定哪一个对应于你的用例。

好的。所以我要回到一个关于对齐的问题。所以我们将在下一节课看到将模型对齐到做你想做的事情的进一步步骤，称为偏好调整（preference tuning）。然后 fine-tuning 和偏好调整的结合，这在 pre-training 之后，是我们称为模型对齐的东西。所以这两个步骤被称为对齐。

我想指出另一件事。有一个我们在这里没有提到的步骤叫做 mid training，最近刚刚出现，它包括在 pre-training 之后的一个步骤，将模型正在训练的数据种类对齐到你真正关心的任务。

所以它是相同的 pre-training 目标，但将任务种类和数据集种类对齐到你关心的东西。是的，所以这是一个新兴趋势，我没有提到它，但只是让你知道，mid-training 是你在 pre-training 和 fine-tuning 之间会有的东西。

都好吗？

好的，很好，所以现在我们要解决我们提到的关于 fine-tuning 挑战的一个方面，即计算开销，就像计算开销昂贵的事实，我们要用一个著名的技术叫做 LoRA 来看它，这是一种在 fine-tuning 阶段以高效方式 fine-tune 你权重的技术。所以它被广泛使用，它节省了大量计算。

所以当你查看你的权重矩阵时，LoRA 技术不是直接 fine-tune 整个权重矩阵，而是将 fine-tuning 分解为 pre-trained 模型的权重和额外的权重之间，它将其分解为低秩乘法。

所以在这个公式中，你拥有的 pre-trained 权重是冻结的。所以这个 W0 是冻结的，然后 B 和 A 是你需要调整的矩阵。所以 B 和 A 通常有匹配行数和列数的维度，分别对应于 B 的行数和 A 的列数。但是 B 的列维度和 A 的行维度是 R，这是这些矩阵的秩，通常取得很小。

所以 W 的维度通常是数百或数千，R 的维度通常是 10 或更少。所以如你所想象的，这导致需要训练的权重要少得多。

然后我只想提到其他提高效率和简化 fine-tuning 的技术，一些叫做 prefix tuning 和 adapters 的方法。所以，它们在课堂教科书中有解释，但我们不会深入研究它们，因为它们不太常用，但只是让你知道。

好的，所以让我们详细了解 LoRA 是如何工作的。所以当你想要 fine-tune 你的模型时，你有所有这些在 pre-training 时间已经学到分布的方式。

所以操作 fine-tuning 的一种天真方法是直接迭代这些权重。但我们在这里做的是，如我们所说，将其分解为我们已经 pre-trained 的权重和这个矩阵乘积。LoRA 说的是你可以对这两个项进行前向传递，然后在最后添加这些量。是的，A 和 B 将是你感兴趣任务的特征化。所以假设你有一个模型，正如 Tain 提到的你可能专门化你模型的任务种类。例如，垃圾邮件检测。所以你会取你的 pre-trained 模型，在你模型的权重中实例化这个 B 和 A，对它们进行 fine-tune，然后你将拥有的是 B 和 A 将特定于这个垃圾邮件检测任务，类似地用于情感提取等等。所以这是一个非常好的属性，你可以从基础模型开始进一步调整你的权重，然后拥有特定于任务的 A 和 B。

### (1:45:00 - End) Part 8

就像这种数量级的初始削减效果非常显著。然后这就是超参数调优的问题，你可以将给定设置中的给定 rank 视为一种设计选择。

好的，我们还有两分钟时间，我们将快速讲解量化 LoRA。Afin 刚才提到的关于量化权重以减少内存占用的技术，正是我们接下来要看到的。

当你观察这些矩阵 W0、A 和 B 时，人们在这篇论文中所做的是将 W0 的权重量化为一种非常巧妙的格式，然后以全精度（在这种情况下是 bf16）计算和迭代这些正在学习的矩阵 A 和 B。

这些冻结权重的量化非常巧妙。它采用一种称为 NF4 的格式，假设权重呈正态分布，并将空间分割为分位数而不是固定大小的桶，这样每个分位数包含大约相同数量的值。


---

*生成时间: 2026-01-03 21:53:49*
*由 YouTube Monitor & Translator (Claude CLI) 生成*