# Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 9 - Recap & Current Trends

## 📹 视频信息

- **频道**: Stanford Online
- **发布日期**: 2025-12-09
- **时长**: 1:51:25
- **原始链接**: [https://www.youtube.com/watch?v=Q86qzJ1K1Ss](https://www.youtube.com/watch?v=Q86qzJ1K1Ss)

---

> 本文内容整理自斯坦福大学教授埃辛·切尔科维克（Arsin Tcherkevitch）和舍文（Shervin）在 Stanford Online 频道的《Transformers 与大语言模型》课程第 9 讲（最后一讲）。

---

**TL;DR**

这是 CME295 课程的收官之作，系统回顾了从 Transformer 到现代 LLM 的完整技术路线图，并深入探讨了 2025 年的前沿趋势：视觉 Transformer 将注意力机制扩展到图像领域，扩散模型正在革新文本生成范式，实现 10 倍推理加速。

---

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-01:11 | 课程开场与结构介绍 | 最后一讲分为三部分：回顾总结、前沿趋势、未来展望 |
| 01:11-15:20 | Transformer 基础回顾 | 从分词、自注意力到完整架构的系统梳理 |
| 15:20-23:34 | LLM 训练三阶段 | 预训练、监督微调、偏好调优的完整流程 |
| 23:34-38:14 | 强化学习与推理能力 | PPO/GRPO 算法及其在推理链生成中的应用 |
| 38:14-47:30 | RAG 与工具调用 | 增强 LLM 实用性的两大核心技术 |
| 47:30-59:17 | 视觉 Transformer (ViT) | Transformer 架构在计算机视觉中的成功应用 |
| 59:17-1:23:38 | 扩散模型与文本生成 | 掩码扩散模型带来的新范式和性能突破 |
| 1:23:38-1:35:29 | 跨模态技术交流 | 视觉与文本领域的相互借鉴和启发 |
| 1:35:29-1:45:53 | 未来研究方向 | 硬件优化、持续学习、个性化等开放挑战 |
| 1:45:53-1:51:25 | 学习资源与结语 | 如何保持技术前沿和课程总结 |

---

## 📊 核心论点

### 1. 自注意力机制：Transformer 成功的根基

- **核心内容**：传统 RNN 存在长程依赖问题，因为早期编码的 token 信息会随序列增长而衰减。自注意力通过让每个 token 直接与序列中所有其他 token 建立联系来解决这一问题。具体实现通过 Query-Key-Value 机制：计算 $\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$，其中缩放因子 $\sqrt{d_k}$ 防止点积过大导致梯度消失。这种设计允许并行计算，充分利用现代 GPU 的矩阵运算能力。
- **关键概念**：长程依赖、并行计算、Q-K-V 机制、缩放点积注意力、位置编码
- **实际意义**：为 NLP 带来革命性突破；使得处理长文本成为可能；成为 GPT、BERT 等所有现代语言模型的基础；推动了多模态 AI 的发展。

### 2. LLM 训练的三阶段范式

- **核心内容**：现代 LLM 训练已形成标准化流程：(1) 预训练阶段在数万亿 token 上学习语言结构，使用下一个 token 预测任务；(2) 监督微调(SFT)阶段使用高质量指令-响应对教会模型遵循指令；(3) 偏好调优阶段通过人类反馈数据让模型学会什么不该做，使用 PPO/GRPO 等强化学习算法。这种分阶段训练策略解决了从原始语言能力到实用助手的转变问题。
- **关键概念**：预训练、SFT、RLHF、Bradley-Terry 模型、奖励模型
- **实际意义**：标准化了 ChatGPT 类产品的训练流程；降低了训练成本（不同阶段使用不同规模数据）；提供了可控的模型行为调整方法。

### 3. Flash Attention：硬件感知的算法优化

- **核心内容**：GPU 存在两种内存：HBM（大而慢）和 SRAM（小而快）。标准注意力计算需要频繁读写 HBM，造成内存瓶颈。Flash Attention 通过分块计算和重计算策略，将 HBM 访问从 $O(n^2)$ 降低到 $O(n^2/M)$（M 为 SRAM 大小）。核心思想是宁愿多计算也要少访问慢内存——这在 GPU 架构下反而更快。实验显示在 GPT-2 训练中速度提升 3 倍，同时保持数值精度。
- **关键概念**：HBM vs SRAM、分块计算、重计算策略、IO 感知算法、内存带宽
- **实际意义**：使长上下文模型（100k+ tokens）成为可能；显著降低训练成本；成为 PyTorch 2.0 标准组件；启发了更多硬件感知算法研究。

### 4. GRPO：专为推理优化的强化学习算法

- **核心内容**：与 PPO 相比，GRPO 做出两个关键简化：(1) 不使用价值函数，通过生成多个completions并比较它们的奖励来获得相对信号；(2) 专注于有可验证答案的任务（如数学），不需要训练奖励模型。这种设计特别适合训练推理能力，因为可以直接验证最终答案的正确性。算法通过比较不同输出的相对质量来更新策略，避免了价值函数带来的额外复杂性。
- **关键概念**：相对奖励、可验证任务、无价值函数、推理链生成、长度偏差问题
- **实际意义**：推动了 o1 等推理模型的发展；简化了 RL 训练流程；在数学和编程任务上取得突破性进展。

### 5. Vision Transformer：跨模态的架构迁移

- **核心内容**：ViT 将图像分割成固定大小的 patches（如 16×16 像素），每个 patch 通过线性投影转换为向量表示，加上位置编码后输入 Transformer 编码器。与 CNN 的局部感受野不同，ViT 允许所有 patches 之间直接交互，降低了归纳偏置。实验表明，在足够大的数据集（如 JFT-300M）上训练时，ViT 超越了传统 CNN，证明了通用架构的潜力。
- **关键概念**：图像分块、低归纳偏置、全局感受野、CLS token、跨模态迁移
- **实际意义**：统一了视觉和语言的模型架构；推动了多模态模型发展；证明了 Transformer 的通用性；影响了后续的 CLIP、DALL-E 等模型。

### 6. 扩散模型在文本生成中的应用

- **核心内容**：传统自回归生成需要 N 次前向传递生成 N 个 token，而扩散模型只需固定步数（如 10-50 步）。核心创新是将图像生成中的"噪声"概念映射为文本中的"掩码"。模型从完全掩码的序列开始，逐步预测被掩码的 token，实现从粗到细的生成过程。这种非自回归方式允许并行生成，某些任务上实现 10 倍加速。
- **关键概念**：掩码扩散、非自回归生成、并行解码、填充中间任务、粗到细精炼
- **实际意义**：大幅降低推理延迟；特别适合代码生成等长输出任务；为实时交互应用打开新可能；挑战了自回归范式的主导地位。

### 7. RAG：连接 LLM 与外部知识

- **核心内容**：RAG 通过两阶段检索增强 LLM 能力：(1) 候选检索使用双编码器计算查询和文档的语义相似度，快速筛选相关文档；(2) 重排序使用交叉编码器对查询-文档对进行精确打分。检索到的文档作为上下文加入提示词，使 LLM 能够利用最新信息回答问题。这种设计解决了 LLM 知识截止日期和幻觉问题。
- **关键概念**：语义搜索、双编码器、交叉编码器、向量数据库、上下文增强
- **实际意义**：使 LLM 能够访问私有数据；降低了微调成本；提高了事实准确性；成为企业应用的标准架构。

### 8. 评估范式的演进：从规则到 LLM-as-Judge

- **核心内容**：传统指标（BLEU、ROUGE）基于词汇匹配，无法捕捉语义等价性。LLM-as-Judge 范式利用强大的语言模型评估其他模型输出，输入包括原始提示、模型响应和评估标准，输出包括理由和分数。但存在位置偏差（偏好先出现的选项）、长度偏差（偏好更长的回答）和自我增强偏差（偏好自己的输出）等问题。
- **关键概念**：语义评估、评估理由、二元判断、偏差类型、基准测试
- **实际意义**：使复杂任务的自动评估成为可能；加速了模型迭代；但需要注意系统性偏差；推动了更好的人类对齐。

### 9. 持续学习与架构演进的开放挑战

- **核心内容**：当前 LLM 的静态权重限制了适应性，需要通过 RAG 或微调来更新知识。未来需要解决：(1) 持续学习机制，让模型能够不断吸收新信息；(2) 个性化适应，为不同用户提供定制体验；(3) 可解释性，理解模型决策过程；(4) 硬件协同设计，如模拟计算芯片可能带来数量级的效率提升。
- **关键概念**：终身学习、灾难性遗忘、个性化、可解释 AI、专用硬件
- **实际意义**：决定了 AI 助手的长期可用性；影响用户信任和采用；推动新一代 AI 系统设计；可能带来计算范式的根本转变。

### 10. 跨模态技术的相互借鉴

- **核心内容**：视觉和语言领域正在深度融合：(1) 架构层面，Transformer 统一了两个领域；(2) 技术层面，RoPE 从 1D 扩展到 2D，扩散从图像迁移到文本；(3) 表示层面，DeepSeek-OCR 显示视觉 patches 可以直接表示文本，挑战了分词器的必要性；(4) 训练技巧如归一化、激活函数、优化器都在跨领域传播。这种交叉授粉加速了两个领域的进步。
- **关键概念**：架构统一、技术迁移、多模态融合、表示学习、跨领域创新
- **实际意义**：加速了 CLIP、DALL-E、GPT-4V 等多模态模型发展；降低了研究门槛；推动了通用 AI 的实现；创造了新的应用可能。

---

## 🔬 提及的技术/方法/论文

| 技术/论文 | 讨论语境 | 重要性 |
|----------|----------|--------|
| Attention Is All You Need (2017) | Transformer 架构的开创性论文，自注意力机制 | ⭐⭐⭐ |
| Flash Attention (2022) | 硬件感知的注意力优化，3倍训练加速 | ⭐⭐⭐ |
| Vision Transformer (ViT, 2020) | 将 Transformer 应用于计算机视觉，超越 CNN | ⭐⭐⭐ |
| RoPE (Rotary Position Embeddings) | 相对位置编码，提升长文本处理能力 | ⭐⭐ |
| Group Query Attention | 减少注意力头的参数量，提高效率 | ⭐⭐ |
| GRPO (2024) | 专为推理优化的强化学习算法 | ⭐⭐ |
| BERT (2018) | 编码器模型，开创预训练-微调范式 | ⭐⭐⭐ |
| Mixture of Experts (MoE) | 稀疏激活，降低计算成本 | ⭐⭐ |
| LADM (Large Language Diffusion Models) | 掩码扩散模型，10倍推理加速 | ⭐⭐ |
| DeepSeek-OCR | 视觉 patches 直接表示文本 | ⭐ |
| Muon Optimizer | 可能取代 Adam 的新优化器 | ⭐ |

---

## 💬 经典金句

> "The sculpture is already complete within the marble block before I start my work. It is already there. I just have to chisel away the superfluous material."
> — Michelangelo（米开朗基罗）

> "Training is parallelizable, but inference time generation is not parallelizable because you always need what's before in order to predict the next one."
> — Arsin Tcherkevitch

> "Noise is to images what mask tokens are to text."
> — Arsin Tcherkevitch

> "Recomputation using what I described led to faster run times even though we were doing more computations."
> — Arsin Tcherkevitch

> "The transformer was a concept first introduced in machine translation, but it proved to perform very well for other text tasks and then was reused in a bunch of other domains."
> — Arsin Tcherkevitch

---

## 👤 主要人物

### Arsin Tcherkevitch（埃辛·切尔科维克）

**身份**：斯坦福大学 CME295 课程主讲教授
**背景**：深度学习和大语言模型领域的研究者，专注于 Transformer 架构和 LLM 训练技术
**核心观点**：强调了 Transformer 架构的通用性，从最初的机器翻译应用扩展到整个 AI 领域。认为自注意力机制是现代 AI 的基础，而硬件感知算法（如 Flash Attention）对于实际应用至关重要。对扩散模型在文本生成中的应用持乐观态度，认为这可能带来推理效率的革命性提升。

### Shervin（舍文）

**身份**：斯坦福大学 CME295 课程助理教授
**背景**：专注于跨模态 AI 和未来技术趋势的研究者
**核心观点**：强调不同模态（视觉、语言）之间的技术交叉授粉正在推动 AI 快速发展。认为当前的静态模型架构存在根本限制，未来需要持续学习能力。看好硬件-算法协同设计，特别是模拟计算等新范式可能带来的突破。强调保持技术前沿的重要性，推荐通过论文、代码和社区保持更新。

---

## 📺 视频类型判断

**教程示范**：这是一堂系统性的技术课程最后一讲，包含全面的知识回顾、前沿技术介绍和未来展望，具有很强的教育性和指导性。

---

## 📝 完整翻译

### (0:00 - 1:11) 课程开场与结构介绍
> 最后一讲分为三部分：回顾总结、前沿趋势、未来展望

大家好，欢迎来到 CME295 的第 9 讲。

如你们所知，今天是特殊的一天，因为这是整门课程的最后一讲。所以今天的安排会和平时有所不同。

我们将把这节课分为三个部分。第一部分，我们将回顾整个课程中学过的内容，看看不同的知识点是如何结合在一起的。第二部分，我们会讨论一些在 2025 年特别热门的话题，以及我们认为在不久的将来会成为趋势的内容。第三部分，我们将做一个总结，并讨论大家接下来的学习方向。

听起来不错吧？

好的，让我们从第一部分开始，也就是我刚才提到的回顾整个学期所学内容。

这里没有新知识，只是把所有内容串联起来。

### (1:11 - 15:20) Transformer 基础回顾
> 从分词、自注意力到完整架构的系统梳理

如果你们还记得，很多周前——我想大概是 10 周前——我们有第一节课，重点是理解什么是 Transformer。在课程的最开始，我们甚至不知道如何处理文本。所以我想我们看到的第一步是 tokenization（词元化）步骤，它包括将输入分解为原子单位。

在某种程度上，我们分割文本的方式是任意的。我们有不同的算法来实现这一点。我们看到最常见的 tokenization 算法是 subword level tokenizer（子词级词元化器）。

我们看到它的一些优势是词根可以被重用和利用，特别是在表示这些 Token 时。

说到表示，一旦我们能够将输入文本分解为原子单位（即 Token），下一步就是学习如何表示这些 Embedding（嵌入）。如果你们记得，我们看到了一些当时非常流行的方法。其中之一叫做 Word2Vec，表示是从代理任务中学习的，比如预测中心词或预测上下文词。

但后来我们看到这种学习表示的方式有一些局限性。

其中一个是这些表示不是 context-aware（上下文感知）的。意思是，如果一个词在某个句子中或在另一个句子中，它在两个句子中都会有相同的表示。

因此，我们看到了一些在 2010 年代流行的其他方法，其中之一是 RNN（循环神经网络）。RNN 有这种递归结构，它逐个处理 Token 并保持序列的内部表示。

但后来我们看到这有一个很大的局限性，就是长程依赖问题，特别是在序列变长时，早期编码的 Token 无法被保持。

这就是为什么我们看到了整门课的核心思想，即 self-attention（自注意力）的想法，其中 Token 可以彼此关注，无论它们在序列中的位置如何。你可以把这看作是一个直接链接。

这就是我们看到的。我们看到人们使用三个主要术语。Query（查询）、Key（键）和 Value（值）。通常你想知道查询与序列中的键有多相似，你通过计算一些点积来量化这一点，这些点积经过缩放和 softmax 处理，然后你得到相应的值。最终，我们得到序列中所有 Token 的某种加权平均。

然后你们现在可能也熟悉这个公式：$\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$。这是我刚才提到内容的矩阵表述，它能够以非常高效的方式处理这些计算，这是今天的硬件很适合做的事情。

然后我们通过介绍现代 LLM 基础的架构来结束第一讲，这就是 Transformer。我们看到 Transformer 中有两个值得注意的部分。一个是图左侧的 encoder（编码器），右侧部分是 decoder（解码器），我们看到这是如何应用于翻译的情况。

在第一讲结束时，我们看到了促使我们最终得到 Transformer 的动机，我们看到 Transformer 在翻译方面表现得相当好。

在下一讲中，我们看到了人们自 2017 年 Transformer 发布以来对这个架构所做的一些小改进。

人们做出的一个特殊改进是我们考虑位置的方式，因为在原始 Transformer 论文中，位置是以绝对方式编码的，即每个位置都有自己的 embedding，这个 embedding 被添加到 token embedding 中。

但如果我们仔细想想，对于位置，我们实际上并不真正关心绝对位置。我们关心的是 Token 之间的相对位置，特别是在 self-attention 计算中 Token 之间的距离。

这就是为什么我们看到了这个现在相当流行的方法，叫做 rotary position embeddings（旋转位置嵌入），也就是 RoPE，现在被广泛使用。

这是一种旋转 query 和 key 的方法，两者都在 self-attention 计算中发生。

这里量化的是纯粹两个 Token 之间相对距离的函数，不仅如此，它在我们关心的 self-attention 层中被处理。

这是一个重大改进。然后我们看到了一些其他改进，特别是在 multi-head attention（多头注意力）层的组成方面。我们看到，我们可以对学习的矩阵进行一些分组。我们不需要为每个头都有一个矩阵，比如对于 key 和 value，我们实际上可以将它们分组。这就是这里提到的 group query attention（分组查询注意力）。

然后我们还看到了其他一些技术，比如 Transformer 中的 normalization（标准化）层，这里它发生在每个子层之后，但现在人们尝试将 normalization 部分移到子层之前。这里是 post-norm 版本，而在子层之前的部分称为 pre-norm 版本。

我们看到的最后一件事是，从这个 Transformer 架构，有很多基于它的派生模型。我们看到，如果我们只保留 encoder 部分，我们可以计算非常有意义的 embedding。

如果你们记得，有一篇关于 encoder-only 模型的里程碑式论文，就是 BERT，它在分类上下文中被大量使用，因为它依赖于 CLS token 的编码嵌入。这是其中一种。

但我们也看到有许多其他类型的模型，或多或少都是从 Transformer 派生的。你可以只保留 encoder（比如 BERT），你可以只保留 decoder（比如 GPT），你也可以两者都有（比如 T5）。

这些模型的一个特殊方面是，encoder-only 在我们看到的方式中无法生成文本，但能够生成可用于下游任务的 embedding。但 encoder-decoder 模型（如 T5）或 decoder-only 模型（如 GPT）可以是自回归的并生成文本。这种模式可以是文本输入文本输出。

有了这些，我们然后专注于现在每个人都称为大语言模型（LLM）的东西，这些是基于 Transformer 的模型，特别是文本到文本模型。也就是 decoder-only 的基于 Transformer 的模型，我们看到人们现在提出了很多新技巧，因为这些模型，如名称所示，人们已经将它们扩大了规模。

但然后出现了一个问题，你真的需要所有这些参数只是为了做一次前向传播吗？

所以我们看到了一种基于 mixture of experts（专家混合）的变体。专家混合的含义是，不是让所有东西都通过整个模型运行，而是你将有一些专家，你将以稀疏的方式激活它们。

例如，对于一个输入，你只激活一个子集，然后对于另一个输入，你激活另一个子集，这样你就不需要总是做所有的计算。

我们看到这些专家混合被用于 LLM 中，特别是在前馈神经网络层中。这里你会有作为不同前馈神经网络的专家，你会有一个门控机制来重新路由到正确的前馈神经网络。

然后我们也看到一些论文能够产生一些不错的可视化，显示哪个 Token 被路由到哪个专家，因为我们看到这种重新路由是在 Token 级别完成的。

在 Token 级别完成的一个原因是能够智能地将专家放在不同的硬件片段、不同的 GPU 上，然后稍微并行化计算。

然后我们还看到这些 LLM 总是被分配预测下一个 Token 的任务。为了预测下一个 Token，我们对如何做到这一点很感兴趣。人们使用的一种特殊方法就是从输出分布中采样。

所以给定一个输入，你有一个关于下一个 Token 是什么的概率分布，这是模型输出的。你所做的是，不是比如说取最高概率（称为贪婪解码），而是实际采样。

这引入了一些随机性，允许模型产生更大种类的输出。

我们看到你可以通过调整一个叫做 temperature（温度）的超参数来调整你想要在输出中有多少变化。

非常低的 temperature 导致非常尖峰的分布，因此输出更确定；而较高的 temperature 则有点更随机，更有创造性。

### (15:00 - 30:00) Part 2

好的，到目前为止我们了解了什么是 LLM，它们如何基于 Transformer，如何与我们在第一讲中看到的架构相连接。然后在第四讲中，我们看到了人们如何实际训练这些 LLM，因为正如我提到的，这些 LLM 很大，所以你不能天真地将它们放入你的硬件中，你需要聪明一点。

### (15:20 - 23:34) LLM 训练三阶段
> 预训练、监督微调、偏好调优的完整流程

特别是，人们在 2020 年代初期注意到的是，你的模型越大，你的表现就越好。所以人们开始构建越来越大的模型。在这个图示中，我们看到 y 轴是测试损失，越低越好。我们看到你使用的计算越多，你的测试表现就越好，增加数据集大小和增加参数数量也是如此。

但是你知道计算不是无限的。所以社区中出现了一个自然的问题：如果我们给你一个特定的预算，一个特定的计算预算，你能选择一些所谓的最优参数数量和数据集大小来训练你的模型吗？

我们看到在 2020 年代初期发表了一篇论文，实际研究了数据集大小、模型大小和测试集性能之间的关系。然后我们看到，实际上当时的大多数模型都是训练不足的，因为相对于它们训练的数据集，它们太大了。训练它们的数据集没有应该的那么大。

特别是，从中得出了一个经验法则：如果你的模型有一定数量的参数，你应该至少在相当于参数数量 20 倍的 Token 上训练它。例如，如果你有一个 1000 亿参数的模型，你应该至少在 2 万亿 Token 上训练它，因为 2 万亿等于 1000 亿乘以 20。这就是人们使用的经验法则。

正如我之前提到的，这些模型很庞大。所以人们也试图让计算更高效。我们看到了一个实际上非常重要的方法，叫做 Flash Attention。Flash Attention 是一种利用底层硬件优势的方法，特别是它关注 GPU 拥有的不同类型内存。

GPU 有大而慢的内存和小而快的内存，分别是 HBM 和 SRAM。我们看到这个方法试图最小化对大而慢内存（HBM）的读写次数。它的做法是将计算分成小块，发送到 SRAM（小而快的内存），这样它就可以进行端到端计算，然后发送回去以完成完整的端到端计算。

### (23:34 - 38:14) 强化学习与推理能力
> PPO/GRPO 算法及其在推理链生成中的应用

这个方法是精确的方法，意味着我们没有对结果进行任何近似，但它导致了显著的加速。论文中还有第二个重要想法：有时候不存储结果是可以的，你可以丢弃它们，然后在再次需要时重新计算。这就是重新计算的想法，使用我描述的方法，即使我们做了更多计算，也能获得更快的运行时间。

这就是 Flash Attention。我们还看到了许多其他旨在并行化计算的方法。我们看到了数据并行，这是一种不让所有数据在单个 GPU 上处理，而是将其分配到多个地方的想法。然后我们有第二种方法，叫模型并行，即使对于给定的前向传播，你也会实际涉及多个 GPU。

总之，有很多非常有趣的技术，很多关于如何高效训练这些模型的不同想法。特别是，我在这里描述的主要对 LLM 训练过程的第一步很重要，这叫做预训练（pre-training），旨在教授模型语言结构和代码结构的知识。这个模型用大量数据进行训练，比如数万亿 Token 甚至数十万亿 Token。

第一步从初始化模型到能够自动补全的模型，因为它是用预测下一个 Token 的目标进行训练的。在第一阶段结束时，你有一个知道如何自动补全的模型，但这个模型不是很有用，因为它只知道如何补全东西。

为了让模型对我们的用例有用，我们有第二步，叫做微调（fine-tuning）步骤，我们在我们希望它表现良好的输入输出对上训练模型。这也被称为 SFT 阶段，即监督微调阶段。

在第二步结束时，我们有一个不仅知道文本和代码结构，还能够按你想要的方式行为的模型。但到目前为止，直到第二步，我们只教了模型该做什么，我们没有教它不该做什么。这就是为什么我们有第三步，叫做偏好调整（preference tuning）步骤，我们拿经过预训练阶段和 SFT 阶段的模型，现在想要注入一些负信号，比如我希望你更偏好这个输出而不是那个输出。

### (30:00 - 45:00) Part 3

而它们被训练展现这些高级推理能力的方式，实际上是利用了我们在第5讲中看到的许多技术，就像基于RL的技术一样。特别是，我们希望我们的LLM在产生最终答案之前，输出一个推理链。我们想要这样做的原因是因为人们发现这提高了模型的性能。

所以它实际上依赖于思维链（chain of thoughts）这个想法，我相信我们在第3讲中见过。这是一种提示技术，让你的模型在输出响应之前先输出推理过程。

简而言之，直到第6讲，我们的LLM是将提示作为输入，直接输出结果。但在第7讲，我们说，抱歉，在第6讲中，我们说，嗯，让我们的LLM实际上先输出一个推理链，用户可能有或没有访问权限，然后再输出最终答案。

所以你想要教会LM这样做。那么你怎么做呢？在做这个之前，我想向你展示这个图表，我们看到的是模型在我们教它产生这些推理链时的性能。人们通常通过将其与某些基准比较来衡量性能的提高，这个是一个流行的基准——AIM基准，这是数学基准，我们看到随着训练的进行，LLM输出的准确率在增加。

但回到我刚才说的，我们用来教授模型如何输出这些推理链的关键技术是利用我们在第5讲中看到的RL技术。特别是，到目前为止我们看到了PPO，这是人们直到可能去年还在使用的主要RL算法，现在人们开始优先使用GRPO作为RL算法，以教授模型在推理任务上表现更好。

有几个理由这样做，我现在就会明确说明。我们看到了这个图示，比较了GRPO与PPO的不同之处，如果你能在图表中看到，有几个不同之处。

第一个是GRPO不依赖于价值模型。那么，谁记得什么是价值模型？

是的。完全正确。价值函数试图预测如果你遵循LLM的策略会得到什么奖励。我想这是一种获得某些预测有多好的基线的方法。你想让它更加相对化。所以价值函数是我们让这些奖励彼此更加相对的一种方式。这就是PPO的做法。所以它有一个价值模型在进行这些预测，然后我们有这个广义优势估计方法，将奖励预测与价值函数预测结合起来，以获得我们称为优势的东西。所以优势是你的输出与某个基线相比有多好。

但与此相比，GRPO说："好吧，我们不需要价值函数，因为训练和维护它太昂贵了。我们将要做的是生成几个完成结果，然后有某个公式来比较这些完成结果的奖励。"

所以它将在某种意义上具有相对效果，它会让事情变得更相对化，这样做你实际上不需要维护和训练价值函数，这是与PPO相比的一个大不同。

第二个大不同，在这个图示中没有表示出来，是GRPO通常是人们在教授模型在推理任务上表现更好的背景下使用的算法。

我们看到这些类型的问题有可验证的奖励，因为当你完成一个数学问题时，你实际上知道你需要得到的答案。所以你不需要训练一个奖励模型来告诉你你的最终答案有多好，因为你已经知道答案了。

我们看到GRPO特别用于当你实际上甚至不需要奖励模型，当你实际上有可验证奖励的情况。所以归根结底，你需要保留的只有两个模型：策略模型和参考模型，以便能够比较你与参考模型的距离。

很好。我知道这也是一个具有挑战性的课程。到目前为止还好。这也在期末考试中。这就是为什么我在复习的第二部分进行得更慢。到目前为止一切都好吗？

好的。很好。我们还看到了GRPO的一些扩展。如果你记得，有一些偏差是GRPO损失函数的结果，该函数有一些归一化项，会惩罚较短输出中的token。

我们看到如果你使用GRPO的原始形式，在某个点之后，算法会激励你的模型产生越来越长的答案，越来越长的错误答案。

### (38:14 - 47:30) RAG 与工具调用
> 增强 LLM 实用性的两大核心技术

它这样做的原因是，相对于短的错误答案，它对长的错误答案惩罚较少。这就是为什么今年人们开发了一些扩展。其中一个是GRPO done right。我们看到他们基本上移除了归一化项，还有另一个我们看到的方法叫做DPO，它也有一些变种，那是针对推理模型的。

然后在第7讲中，我们有一个模型，你知道我们知道如何训练它，我们知道如何将其用于推理任务，如何训练它变得更好，但现在我们希望模型在与外部系统交互时有用。

我们看到了一个基本技术叫做RAG，即检索增强生成的缩写，它意味着你能够从某个知识库中获取相关文档以回答问题或回应提示。

你想要这样做的原因是你的LLM的知识包含到知识截止更新为止，这是你的LM接受训练的最大日期。从实际角度来看，我想从我们现在看到的情况来看，你通常不会每天或持续地训练你的LLM。在你需要你的LLM知道最近发生的事情或知道不在你的LM训练数据中的事情的情况下，你希望你的LLM能够访问这样的信息。这就是RAG如何非常有用。

我们看到RAG非常依赖于它检索数据的方式。我们看到检索部分主要由两个步骤组成。第一个是候选检索，使用双编码器设置，你基本上在进行语义搜索。所以你计算查询的嵌入，你有知识库中文档的一些预计算嵌入，你取那些最大化某种相似性得分的文档，比如余弦相似度。

第一步允许你检索潜在文档的过滤版本，然后通常你有第二步叫做排名或重新排名，因为第一步已经给你一个排名，通常有更复杂的设置。

这是一个交叉编码器设置，你的查询和文档都被输入到某个模型中，产生更精确的得分，然后你使用这个最终得分来排名最终结果，你通常选择前K个，然后将它们添加到你的提示中。这就是增强部分。检索是我到目前为止提到的一切，一旦你有了相关文档，你将它们添加到你的提示中，这是增强部分，然后你生成答案。

我在RAG上花这么多时间的原因是RAG是一个如此重要的概念，如果你要面试或者你知道也许在考试中谁知道呢，所以我认为这是一个要牢记的重要概念。

我们看到的第二个是工具调用（tool calling）。工具调用允许你的LLM利用工具。它通过两个步骤来做到这一点。第一步是让你的模型知道外面有哪些API。

在此结束时，你的LLM说，好的，我想使用这个API，我想用这些参数来使用它。

然后你有一个中间步骤，就是你用这些参数运行你的API。

然后第二步是你将这个操作的结果反馈给LLM，然后产生最终答案。这就是工具调用的工作方式。所以如果你对你的LM说好的你可以使用这个API，这就是你的LM如何利用它的。

然后我们看到现代代理工作流程利用RAG和工具调用作为执行操作的关键方法。我们看到了一个详细的例子，你有一些输入，然后你的LLM有一系列不同的调用来执行某些操作，最后它检索，抱歉，返回一个答案。

很好。最后一讲我们看到了如何评估LLM，现在LLM可以做很多不同的事情，这是一个更困难的事情。我们首先看到在LLM出现之前人们使用的一些基于规则的指标。你可能听过的指标如BLEU、ROUGE、METEOR等等，但主要限制是它们没有考虑到语言可能不同但仍然正确。

所以我们看到的关键想法是为什么不利用LLM来评估输出。所以有这个关键想法叫做"LLM as a judge"，你接收提示、模型响应以及你希望响应被评估的标准作为输入。

### (45:00 - 1:00:00) Part 4

然后你希望你的LM消息输出两样东西。第一个是对为什么给出某个分数的理由阐述，以及那个分数。

如今，LM作为评判者通常输出二元响应，要么通过或失败，要么真或假，因为这样更容易。我们也让理由在分数之前输出，因为在实践中这也会稍微改善LM作为评判者的性能，如果你想要像推理模型那样在输出答案之前输出推理链。

但我们也看到这种方法存在一些偏见。我们看到位置偏见，即你呈现元素进行比较的方式很重要。如果你先呈现某个东西，那么LLM可能会优先考虑第一个。还有冗长偏见，即你的LLM只是偏好更长的输出。

自我增强偏见是另一个，它偏好自己的输出。然后我们也看到了一些基准测试，人们现在用它们来说明他们的LLM有多棒。如果你看看发布的内容，通常有一堆指标涵盖人们知道的许多不同基准。这包括知识、推理能力、编程（这非常重要，因为很多应用都与编程相关），然后是安全性，这还不是一个详尽的列表，实际上还有更多维度。

我认为这就是我们上次停下的地方，这是最后一讲，这也是你们期末考试需要知道的全部内容。之后的所有内容都不会成为期末考试的一部分。

### (47:30 - 59:17) 视觉 Transformer (ViT)
> Transformer 架构在计算机视觉中的成功应用

到目前为止有什么问题吗？

很好。我期待每个人期末考试都能拿到满分。我想说我讲过的内容将是期末考试的基础。如果你理解了我说的一切，我认为你就为期末考试做好了准备。如果你有任何问题，Shervin和我随时都在这里...噢，你有问题吗？

是的。问题是期末考试的范围是第5讲到第8讲吗？是的。中期考试是第1、2、3、4讲，这次是第5、6、7、8讲。我想规模是相等的。

很好。

好的，很棒。说到这里，我们刚刚完成了对整个季度讲座的回顾，现在我们要进入今天菜单的第二项，即查看一些热门话题。

我将从第一个开始。我要这样介绍它。

如果你记得，我们看到Transformer是一个概念和架构，首次在机器翻译的背景下被引入。它表现得很好。人们说既然它在机器翻译上表现很好，为什么不在其他文本任务上尝试呢？所以他们试了，表现很好，但现在的问题是，你能不能将它用于文本以外的东西？这是一个自然的问题，对吧？

为了回答这个问题，我只想让我们提醒自己，这个架构依赖于自注意力这个概念，这正是使Transformer工作得如此出色的原因。

如果我们简单回顾一下什么是自注意力，这个插图很好地完成了这项工作。你有一个查询，然后你有一堆其他元素，由你的键和值表示，你想知道哪些其他元素实际上是相关的，以便计算该查询的嵌入。

到目前为止，我们只使用了Token，你知道，文本Token，但文本Token实际上是向量。

如果你取那些向量，并且实际上表示除了文本以外的其他东西，比如图像的部分。问题是，基于那种输入的Transformer也会表现良好吗？

所以这里我想问的关键问题是，我们如何调整我们的Transformer来处理非文本输入，例如这里我们可以考虑图像理解输入。你有一些图像，这是一个传统的计算机视觉任务，你想知道这个图像属于哪个类别。

你想知道拥有一些基于Transformer的架构是否在那种情况下工作良好。

答案是，首先为了将其适应这个任务，你会取Transformer的编码器部分，因为为了理解图像中的内容，你需要在某种意义上对该图像进行分类。

如果你记得，在我们看到的模型中，有一个在分类方面工作得非常好的模型是BERT，因为BERT只有编码器。它计算有意义的嵌入，然后可以用于投影目的或分类目的。

这是我们在这里拥有的非常自然的选择。所以这里我们只会保留Transformer的编码器部分，然后让自注意力机制发挥作用，计算有意义的嵌入，然后我们可以将其投影到我们相关的任务上。这正是一组研究人员在2020年所做的。

你们听说过ViT vision transformer吗？

是的。没有。是的。我在这里描述的正是他们所做的。

他们拿了一张图像，将该图像分成patches。这些patches由一些向量表示。当然你有一些位置信息，允许你知道你的patch在图像中的位置。

然后你只需将其通过Transformer编码器。所以Transformer的编码器部分，你计算与CLS类对应的表示，与BERT非常相似，你只需将该表示投影到一些感兴趣的类上，然后你会执行你的计算。

那篇论文发现，如果你在大量图像数据上训练这样的模型，你会超越这些传统的卷积神经网络方法。

这种结果非常引人注目，为什么引人注目呢？因为在视觉案例中，有这个归纳偏见的概念，你想要引导你的模型朝向查看某些东西以推断结果。

卷积神经网络是一种模型，其设计方式是让你以某种滑动的方式查看图像。你知道，你查看图像的方式有点像你作为人类在实践中查看它的方式。

人们曾假设这样的偏见，这样的归纳偏见实际上对像视觉任务这样的东西是有意义的。

将此与vision transformer对比，它实际上让图像的所有部分相互关注，另一方面具有非常低的归纳偏见。这篇论文显示的是，如果你给你的模型足够的数据，它实际上会学会如何在这些类别中分类你的图像。这是一个相当了不起的结果。

我认为这非常了不起，是我们所看到的一切的很好扩展。

考虑到这一点，我想让我们通过一个端到端的例子，说明如何处理图像并通过那个ViT，即vision transformer，以做出预测。

这里你会拿你最喜欢的图像，将其分割成patches。这里你可以考虑预定义一些固定尺寸的patches。这里我会说像3x3，然后每个patch都有一些固定数量的像素，然后你要做的是为每个patch尝试获得一些向量表示。

你可以将每个patch想象成...什么是patch？它由像素组成。如果每个像素有三个值，对应红、绿、蓝，那么你可以找到一种方式将这些投影到某个低维扁平空间上，你可以学习如何通过某种线性层来投影。

长话短说，你只是找到一种方式将向量关联到每个这些patches，然后你代表你的每一个输入。然后你当然有一个特殊的CLS token嵌入，你也可以学习。

你添加位置嵌入。你对所有输入都这样做，然后与BERT非常相似，只需将其通过编码器。让每个人与每个人互动，然后最终你关心的是输入的有意义表示。通常人们采用CLS token的编码嵌入。他们采用它的原因一是因为这是一个约定，二是这个CLS token。编码嵌入实际上是一个通过这种自注意力机制与所有其他Token互动过的嵌入。

它看到了一切，然后你会将那个CLS token编码嵌入通过前馈神经网络投影到某个类上，以预测你的最终类别。

在这种情况下，我们知道这是一张泰迪熊的图片。所以这里我们希望模型将其分类为泰迪熊。

到目前为止还好。这有意义吗？

很好。现在，我们知道如何处理图像输入，对吧？现在，另一个问题是你如何让你的LLM回答关于你的图像的问题，这实际上是你现在可以做到的。比如如果你打开ChatGPT，你可以输入图像并向它提问。

你会有两种输入。你会有一个图像，我们看到我们可以找到一种方式来表示，然后是文本，你现在非常清楚如何用Token表示。

允许模型处理所有这些的方式通常如下。有几种方法。第一种是最常见的，你只是将所有内容作为输入提供。

### (1:00:00 - 1:15:00) Part 5

### (59:17 - 1:23:38) 扩散模型与文本生成
> 掩码扩散模型带来的新范式和性能突破

你只是将所有内容作为输入提供。因此图像token作为输入，文本token作为输入，你有某种表示方式让模型知道这些是图像token，这些是文本token，然后你让它以仅解码器的方式生成答案，完全像你做的那样，以自回归方式。

这是第一种方法，很多这样的模型都是这样设计的。例如，有一个非常受欢迎的开源视觉语言模型（VLM），叫做LAVA，这就是它们的做法。它们在图像部分有一些编码器，产生一些token，然后与输入到LLM的文本token连接起来。这是一种方法。

第二种方法不太常见，就是让图像在交叉注意力层输入。在这里你要做的是有你的文本输入，然后你的图像输入。你不把它放在输入中。你实际上让它在交叉注意力层内与文本token交互。例如，Llama 3在他们的论文中表现了这种技术。这种技术通常不太常见。第一种更常见。

我想说的是，CME 295专注于Transformer，特别是在文本到文本问题的情况下。文本生成就是你知道的这个课程的全部内容，但我们也有用于非文本应用的Transformer。这里我们看到了图像理解，即使用ViT的视觉理解，然后我们现在没有时间说这个，但对于图像生成任务，我们也可以让Transformer的部分被用在那个架构中。

所以你可能会听说Diffusion Transformer或多模态Diffusion Transformer，它们实际上依赖于自注意力机制。这实际上不是一个详尽的列表。这也是在其他领域使用的东西，比如推荐系统、语音等等。所以我想让你们从中记住的是，Transformer是一种在机器翻译任务中表现非常好的架构。

但是它被证明在其他文本相关任务中也表现得非常好，然后它被重新用于一堆其他领域，这些领域也被证明相当成功。所以我只是鼓励你们在这门课后也对非文本相关的Transformer应用保持开放的心态，我在这里提到的那些可能只是你们可以查看的论文类型的前几个指针。

很好。所以这里我们说Transformer是来自文本世界的东西，在其他世界中也很有用和被使用。现在我想告诉你们一些其他的东西，这些东西在非文本世界中被使用和有用，可能在文本世界中也有用。我想告诉你们基于扩散的LLM。

有谁听说过扩散这个术语？谁知道扩散？好的，很酷。我们将看到如何将其应用于LLM。这是一个非常流行的话题。我相信第一篇论文开始于2020年代初，但我想只是现在人们开始让这个真正起作用。

我只想从一个动机开始，就是到目前为止，我们理所当然地认为我们的LLM是自回归LLM，自回归是什么意思？它接受一些输入，LLM试图做的是预测下一个token。因此，给定到目前为止的一切，我们预测下一个token。我们这样做，然后我们取刚刚预测的那个token以及到目前为止我们预测的一切，然后我们再次预测下一个token。

我们预测它，然后一次又一次地进行，直到你知道用序列结束token完成序列，这使得生成停止。所以这是真正的自回归生成，因为我们取到目前为止的输入来预测下一个token，然后重复这个过程直到结束。这是人们现在试图给它起名字的东西，比如自回归模型类型的模型。

所以，ARM，如果你看到这个符号，那就是它的意思。这种范式的问题是推理时生成实际上不是你能并行化的东西，因为你总是需要前面的东西来预测下一个。但我只想说推理时生成是不能并行化的。但训练是可以并行化的。

如果你记得我们进行训练的方式，我们输入所有我们想要我们的模型预测的token，然后我们让模型从中生成token。所以基本上在仅解码器设置中，你有这个因果掩码，如果你想的话，它让你的模型不作弊，不使用未来的那些。所以我只想说，当我说这种范式不能并行化时，我只想强调这是推理时间，我说的训练时间你实际上可以很好地并行化。

### (1:15:00 - 1:30:00) Part 6

因此，你需要学习某种模型，让你能够以重构原始句子的方式来解除这些掩码token。这里涉及一些数学原理。显然在短短几分钟内我们没时间深入探讨，但我想强调的关键思想是：你想要进行扩散，但要以对文本输入有意义的方式。合理的方式是将图像噪声视为文本输入的掩码token。

很好。考虑到这一点，现在有很多模型被发布，称为掩码扩散模型，MDM。所以当你看到MDM时，你就知道我们在谈论什么类型的模型了。

符号表示仍然很不规范，所以将来可能会改变。但你会看到的另一个术语是DLLM，即基于扩散的LLM，这就是它在做的事情。它不是以自回归的方式一次预测一个token，而是在推理时从完全掩码的输入序列开始，试图预测这些掩码token背后的token是什么。

当然，在现实设置中你会有一些提示。为了预测这个答案，你需要一些条件。所以你会告诉你的模型：好的，给定这个提示，你要从所有这些掩码token开始，试图预测答案是什么。

如果你对直觉有困难，我一开始也有困难，为什么用扩散方式解决文本问题是合理的，因为通常当你写作时，你是一次写一个词。一个有用的思考方式是：假设你想写一篇演讲。

你不会直接以线性方式写你的演讲。你首先会有一个粗略的计划，你说：好的，我要先谈论这个，然后第二、第三，你有某种草稿，然后你尝试完善每个部分的内容。

所以你可以把扩散想象成这样工作的，它试图对输出进行从粗糙到精细的优化，所以它可以预测在某个尚未预测的token之后的内容。但你可以把这想成是从非常草率的版本到非常精细版本的过程。

这就是我对这个过程的理解，希望这有帮助。这里的关键优势是解码现在用更少的前向传递就能完成，因为之前你需要做与要预测的token数量相同的前向传递次数。

但这里对于扩散，你只需要做与扩散过程中的步数相同的传递次数。步数是你可以固定的。步数越高，输出质量越高，但通常比输出长度要低得多。

这就是这个模型比自回归模型快得多的核心原因。当然，我们没有太多时间，但如果你好奇，如果你在期末考试后完全解脱时有兴趣，我提供了一些可能有帮助的参考文献。今年早些时候有一篇论文叫LADA，大语言扩散模型与掩码。实际上我不记得完整的缩写，但它实际上深入探讨了数学原理以及为什么我刚才提到的方法有效。

还有一堆其他论文也会有帮助。链接在幻灯片底部，如果你有兴趣的话。我想在交给Shervin之前讲最后两件事。

首先是这个新范式的优势。第一个是速度。正如我们提到的，它比传统的自回归模型快得多，特别是对于较长的输出。一些基准测试甚至说它大约快10倍。

对于编码等情况，它可能非常强大，因为你可能需要进行多次模型调用，作为用户你只是在等待代码生成，拥有更低的延迟会产生很大的差异。

另一件事是这种方法的性质实际上是将文本作为一个整体来考虑以进行预测。所以有一类编码任务叫做填充中间，即试图弄清楚你有一堆代码，你想知道中间缺少什么。

填充中间任务和扩散模型通常更适合这类任务，因为它们可以考虑来自多个方向的输入，这就是为什么这种方法可能对某些应用有用。

就目前的工作而言，这些模型看起来很棒，我向你提到的内容看起来很棒，但性能至少在一段时间内还没有达到当前前沿模型的水平，但这可能会改变。

### (1:23:38 - 1:35:29) 跨模态技术交流
> 视觉与文本领域的相互借鉴和启发

我提到的论文实际上发布的性能正在追赶自回归模型。所以那里是有希望的。然后另一条工作线就是适应人们想出的所有技术，比如推理链。你如何为扩散适应它？

有很多技术本质上更适合自回归类型的模型，可以为此进行适应，这就是人们正在做的工作。

长话短说，我们看到的是在这门课中看到的东西可以用于其他领域。比如，我们看到了视觉Transformer，它将Transformer借用于视觉相关任务。但我们也看到其他领域的东西也可以用于文本世界。

这就是我们在扩散LMS中看到的。当然，这可能只是正在发生的一切的一个子集。

因此，我认为我们正在结束我们菜单的第二项。我要把时间交给Shervin。

Shervin：谢谢Afin。欢迎来到CME295季终集的最后部分。正如Afin提到的，现在是一些结束思考的时间，看看我们能从这门课中获得什么，以及与之相邻的概念。

首先，Afin讲解了图像中的扩散概念，我们看到了与文本可以建立的一些相似性。现在我们将看到这两种模态从彼此那里获得了什么样的启发。我们将看到实际上很多东西都可以被重用。

我想在重用方面提到的第一件事是架构部分。Afin提到了这个扩散概念，它诞生于图像领域，但被用于文本，能够产生更低的延迟，更高的加速，这对用户来说很棒。这是一个成功的例子。

然后在另一个方向上，传统上图像主要处理卷积作为模型架构类型，但这些论文发现用Transformer替换卷积非常好，甚至产生更好的结果。

所以图像领域的所有最新基于扩散的论文通常都使用Transformer，这里我链接了Afin已经简要介绍过的一篇论文。

但不仅仅是架构方面是这些模态之间交叉传播的主题。你甚至可以考虑其他类型的组件，比如输入。为此我想提到DeepSeek OCR的例子。我不知道你是否听说过这篇论文，它刚刚发布。与名称建议的相反，OCR代表光学字符识别，这通常是一个试图将扫描图像转换为文本的领域，但实际上那篇论文并没有夸耀在OCR任务本身上的改进，而是展示了你可以学习一些基于视觉token重构文本token的函数，而且不仅是基于视觉token，是基于很少的视觉token。

所以它展示了图像patches作为token的表示能力非常强。一些研究人员为此提供了理论依据，比如tokenizer反正不是最好的工具，而且patches中的内容已经通过表情符号等例子传达了文本的含义，否则你需要用更多的文本token来表示。

然后我想提到的另一个例子是，即使当你查看架构内部时，一些技巧可以在每个领域中重用和适应。这里我提到了RoPE的例子，Afin在回顾中提到了它用于文本中表示token的相对位置，在图像或甚至多模态设置中，当你在架构内同时存在文本和图像时，你能够通过在2D中重新表述来适应这个技巧。这里图表显示了如何在2D网格中分配RoPE位置，以及如何放置文本token使得位置的相对计算仍然有意义。

### (1:30:00 - 1:45:00) Part 7

现在你使用的是 pre-norm，它将归一化操作提前到层的更早位置。但除了归一化位置的设计选择之外，归一化的类型也在发生变化。Transformer 论文使用了 layer norm，但现在你可能会看到其他类型的归一化技术，比如使用更少参数的 RMS norm 等。所以背后的理论还没有完全确定。

你还有其他种类的参数。Ashin 提到了 grouped query attention 论文，现在在 LLM 论文中你看到的不是固定的设计，而是每篇论文都采用自己的技术。有时你会在某一层看到一种 attention，但随后它会切换，不同的论文采取不同的设计决策，所以这并非一成不变的。然后你还有激活函数。传统上在深度学习中，ReLU 受到很多重视，它非常简单且运行良好。但在 LLM 领域，转向了类似 ReLU 的激活函数，但不完全是 ReLU。

你有 Gaussian Error Linear Units，还有其他类型，这方面的研究仍在进行中。你仍然会看到新的激活函数不时出现。然后你还要考虑是否将 LLM 设计为某种特定架构的设计选择，以及你的 LLM 的层数和其他超参数，比如 attention heads 的数量、FFN 中单元的大小，所有这些在设计决策方面仍有争议。所以这并非固定不变。

然后我想提到的另一个研究领域是至关重要的数据部分。最初的 LLM 享有相对干净的状态，因为你可以抓取互联网并希望获得大量确实由人类生成的数据。所以你可以从所谓的高质量来源学习这些模式，尽管当你查看典型的互联网数据时，它的格式并非高质量，但它仍然是由人类生成的。如今情况已经改变。你在你最喜欢的搜索浏览器中输入任何内容，第一批结果很可能80%都是 LLM 生成的。

### (1:35:29 - 1:45:53) 未来研究方向
> 硬件优化、持续学习、个性化等开放挑战

我们注定失败吗？也许不会，因为实际上你看到在数据策划方面出现了越来越多的工作。过去你只是抓取整个互联网并在其上训练下一个 token 预测。但现在你有越来越多策划感兴趣数据集的工作，有公司专门从事这项工作，还出现了新的 fine-tuning 模式。过去你有 pre-training 然后 fine-tuning，现在你有 pre-training、mid-training 和 fine-tuning。Mid-training 部分仍然在大型数据语料库上训练，但质量更高。

所以人们正在寻找解决办法。我要说的是情况并不全是悲观的，只是我们需要做更多工作才能拥有有意义的数据。幻灯片底部链接的论文处理的现象是如果你在 LLM 生成的数据上训练会怎样，它讨论了一个叫做模型崩塌的概念，它说 LLM 生成的文本通常多样性较少。所以你在训练时看到的数据分布发生了变化，导致训练时学习效果较差，这就是为什么它通常是有害的，并激发了在数据部分做更多工作的需要。

好的。很好。然后你知道，甚至退一步看我们一直在使用的架构本身，它是最好的吗？这并不清楚。所以这本身就是一个研究领域，未来的突破可能来自重新设计这种架构。

在过去几年中，我们看到的很多研究都在不断改进基准测试。你有一组基准测试，每个人都试图获得最佳结果。这是一个自然趋势，因为你想要拥有越来越强大的模型来满足所有用例。但假设我们达到了一个所有我们关心的用例都被解决的点，然后呢？

我认为我们将看到帕累托前沿第二边界的出现，我们最关心的是让 LLM 的预测具有成本效益且仍然保持非常高的质量。所以我们看到越来越小的 LLM 的出现，我想文献中称其为小型语言模型 SLM。有时你会听到 LLM 提供商说，即使在最高等级的计划中他们也在亏钱。我认为这反映了一个事实，即你需要在测试时为服务 LLM 查询所花费的计算上更加智能，我认为这将在未来几年激发越来越多这一研究方向。

然后还有一个我们在这门课中完全没有涉及的领域，就是硬件部分。通常你用来训练这些 LLM 的设备类型是 GPU，它们擅长一件事：矩阵乘法。但问题是，即使架构不仅仅需要矩阵乘法作为计算的基础原子单元，我们仍然保持这些架构来训练我们的模型。自注意力机制和 Transformer 世界有所有这些特殊需求。你知道我们看到的 Q K 转置部分实际上非常昂贵，这激发了像 flash attention 这样的论文，正如 Afin 提到的，实际上为了不在内存方面做太多移动而放弃了一些数据，即使这意味着之后要重新计算相同的东西。

大量工作都在优化 GPU 内存流的位置。这表明也许你需要更优化的硬件架构来解决这些用例。最近有一篇论文出现，实际上将所有这些操作编码为硬件的一部分。过去 GPU 擅长的核心操作是矩阵乘法，基于此你试图构建所有你想要的输入输出。但这里这篇我认为在九月份发表的论文展示了一个概念证明，你可以将所有这些计算作为用模拟信号实现输入和输出的副作用来完成。你将所有这些计算嵌入到硬件中，基于模拟你的数组值的脉冲作为输入。这些硬件架构具有一些物理特性，比如你可以想到强度领域的基尔霍夫定律，你可以将它们相加，它使用这样的特性来获得你需要的输入，并直接读出结果作为输出。当论文模拟这种架构时，他们观察到在延迟和节能方面都有相当大的改进，这并不令人意外，这两者都有解释，因为你不需要自己进行计算，你只是将它们作为硬件的副作用获得。

### (1:45:00 - End) Part 8

也许明天你会有某种证书来保证网站对 AI 助手浏览是安全的。甚至在更高层面上，也许在你的桌面或手机上，在 LLM 层面，在操作系统层面，LLM 都能提供帮助。当我们讨论 agent 时，我们提到它可能不太可靠，因为随着步骤越来越多，失败的概率会增加，稳定预测是一个关注点。

### (1:45:53 - 1:51:25) 学习资源与结语
> 如何保持技术前沿和课程总结

从更长远的角度来看，我认为有一个测试可以衡量我们走了多远，就是看由 AI 服务的客服这种常见用例是否真正有用。我不知道你们怎么样，但每次我遇到问题打电话，听到某个 AI 助手，也许是 LLM 驱动的机器人时，我就想"快快快，我要找人工客服"。我不想要那个。我认为这表明这个问题领域有多困难，因为人类在 LLM 能带来的价值方面有更多维度。比如同理心、扎根性，有些东西你我都认为是合理的，即使它们不在我们的系统提示中。所以我认为这里有很多难题需要解决。

即使展望未来，我们在当前架构上也面临一些关键挑战。我们在课程中看到，你必须经历一个训练过程来固定一些权重，但这些权重之后不会改变。我们使用诸如 RAG 或工具等技巧来解决这个问题。但我们能想象一个持续学习的系统吗？我认为这是一个开放问题。然后是幻觉问题，我加了引号，因为我不确定说 LLM 产生幻觉是否公平，仅仅因为我们训练 LLM 来预测下一个 token，本质上不是将陈述映射到事实。所以在某种意义上，幻觉是这些 LLM 的核心设计选择。个性化、可解释性、安全性，列表还在继续。

现在我想简要介绍如何从现在开始锻炼保持最新的能力。你有 arXiv，通常包含所有这些最新最好的论文供你查看。当然，像 NeurIPS 这样的会议现在是突出论文的好地方。然后我强烈建议你除了论文之外，还要查看作者提供的相关代码库。现在提供你提出内容的实现已经很常见，我认为学习概念非常有见地。有一个叫"Papers with Code"的网站过去存在，现在被 Hugging Face 的热门论文所取代，我认为这是查看最新方法的好地方。在社交网络方面，Twitter 或 X 有很多最新讨论。你在那里有一个强大的社区，如果你在那个社交媒体上有账户，你有很多优秀的人可以关注来保持更新，你还有 YouTube 资源，特别推荐 Yannic Kilcher，我认为他是第一个在 2017 年详细介绍 Transformer 论文的 YouTuber，我认为这些 YouTuber 非常善于详细讲解论文。

另一个亮点是 Andrej Karpathy，他大约 10 年前在斯坦福，我认为他是最好的教育者之一。所以我强烈推荐他的视频，还有公司博客也很棒。还有我们与这门课程相关联的学习指南。我们今年有了它，我们将在未来几年尝试至少每年更新一次。所以你可以把这个资源当作某种伴侣，我们有机会与世界各地的专家合作，使其有其他语言版本，如果你感兴趣的话。


---

*生成时间: 2026-01-04 23:23:57*
*由 YouTube Monitor & Translator (Claude CLI) 生成*