# Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 2 - Transformer-Based Models & Tricks

## 📹 视频信息

- **频道**: Stanford Online
- **发布日期**: 2025-10-17
- **时长**: 1:47:15
- **原始链接**: [https://www.youtube.com/watch?v=yT84Y5zCnaA](https://www.youtube.com/watch?v=yT84Y5zCnaA)

---

> 本文内容整理自斯坦福大学 CME295 课程《Transformers & LLMs》第2讲，由Afin等教授在Stanford Online频道讲授。

## TL;DR（一句话核心洞察）

斯坦福教授深度解析现代Transformer架构的三大核心改进：从固定位置编码到RoPE旋转位置嵌入、从后归一化到RMSNorm预归一化、从全注意力到分组查询注意力，揭示了从2017年原始Transformer到2025年大语言模型的演进路径。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-10:37 | 课程开场与上节回顾 | 回顾Transformer自注意力机制和多头注意力的基本概念 |
| 10:37-42:30 | 位置编码的演进 | 从学习式位置嵌入到正弦位置编码，再到现代RoPE旋转位置嵌入 |
| 42:30-50:44 | 层归一化的变革 | 从后归一化转向预归一化，从LayerNorm到RMSNorm的优化 |
| 50:44-1:02:27 | 注意力机制优化 | 滑动窗口注意力和分组查询注意力(GQA)降低计算复杂度 |
| 1:02:27-1:09:32 | Transformer架构分类 | 编码器-解码器、仅编码器、仅解码器三大架构类别及应用 |
| 1:09:32-1:47:15 | BERT深度解析 | BERT的双向编码、掩码语言模型、分类微调全流程剖析 |

## 📊 核心论点

#### RoPE旋转位置嵌入：现代LLM的位置编码标准

- **核心内容**：传统位置编码通过学习固定嵌入或正弦函数注入位置信息，但无法直接在注意力层体现相对位置关系。RoPE通过将查询和键向量按位置角度进行旋转，使得注意力权重直接成为相对距离的函数。数学上，将查询q_m旋转角度θm，键k_n旋转角度θn，其内积q_m·k_n自然包含相对位置信息(m-n)。这种方法在保持数学优雅性的同时，实现了位置感知的注意力机制。
- **关键概念**：旋转矩阵、相对位置编码、角度参数化、注意力权重衰减、长期依赖建模
- **实际意义**：成为GPT、LLaMA等主流大模型的标准配置；支持更长序列建模；数学上保证远距离token相似度衰减；简化了位置编码的实现复杂度。

#### 层归一化从Post-Norm到Pre-Norm的关键转变

- **核心内容**：原始Transformer采用Post-Norm（先子层计算再归一化），现代架构普遍采用Pre-Norm（先归一化再子层计算）。这一改变显著改善了训练稳定性和收敛速度。同时，RMSNorm替代LayerNorm，通过仅使用均方根归一化（去除均值中心化），减少了计算量和参数数量，同时保持相似的性能表现。RMSNorm公式简化为x/RMS(x)*γ，相比LayerNorm减少了β参数和均值计算。
- **关键概念**：内部协变量偏移、训练稳定性、Pre-Norm vs Post-Norm、RMSNorm、参数效率
- **实际意义**：提升大模型训练稳定性；加速收敛过程；减少训练时的梯度爆炸问题；降低内存和计算开销；成为现代LLM架构的标准配置。

#### 滑动窗口注意力：突破序列长度瓶颈

- **核心内容**：全注意力机制的O(n²)复杂度在长序列场景下成为计算瓶颈。滑动窗口注意力通过限制每个token只与其邻近窗口内的token交互，将复杂度降低到O(n·w)，其中w是窗口大小。现代架构采用局部-全局注意力交替的策略，类似计算机视觉中的感受野概念。多层滑动窗口的堆叠使得信息能够逐步传播到更远的位置，实现长距离依赖建模。
- **关键概念**：计算复杂度优化、窗口大小、感受野、局部-全局注意力、长序列建模
- **实际意义**：支持更长输入序列处理；显著降低内存占用；保持模型性能的同时提升效率；为长文档理解和生成提供技术基础；影响Mistral等模型的设计。

#### 分组查询注意力(GQA)：KV缓存优化的关键技术

- **核心内容**：传统多头注意力为每个头分别维护查询、键、值投影矩阵。GQA通过在多个注意力头间共享键和值的投影矩阵，显著减少了KV缓存的内存占用。极端情况下的多查询注意力(MQA)让所有头共享相同的键值投影，而GQA提供了灵活的中间方案。这种优化在解码阶段尤为重要，因为需要缓存所有历史token的键值对进行增量生成。
- **关键概念**：KV缓存、内存优化、多查询注意力、分组策略、解码效率
- **实际意义**：大幅降低推理时的内存需求；加速文本生成过程；支持更大模型或更长序列的部署；成为生产环境大模型的标准配置；平衡性能和效率的最佳实践。

#### BERT双向表示学习：从单向到双向的突破

- **核心内容**：BERT通过移除解码器部分，让编码器中的自注意力机制真正实现双向信息流动。每个token能同时关注到左侧和右侧的所有上下文信息，这与GPT等因果语言模型的单向注意力形成鲜明对比。掩码语言模型(MLM)任务通过随机掩盖15%的输入token（80%替换为[MASK]，10%保持原样，10%随机替换），强制模型学习完整的双向上下文表示。
- **关键概念**：双向编码、掩码语言模型、CLS token、segment embedding、上下文表示
- **实际意义**：开创了预训练-微调范式；在分类、命名实体识别等任务上实现突破性能；影响了后续大量BERT系列模型；为理解类任务提供强大的表示学习基础；推动了NLP任务的统一建模思路。

#### 预训练-微调两阶段训练范式

- **核心内容**：BERT建立了现代深度学习的标准训练流程：第一阶段在大规模无标注文本上进行自监督预训练，学习通用语言表示；第二阶段在特定任务的少量标注数据上进行有监督微调，适配具体应用需求。这种范式通过迁移学习大大减少了下游任务的数据需求，同时实现了更好的泛化性能。预训练使用MLM和NSP（下句预测）双重目标，微调阶段冻结或轻微调整预训练权重。
- **关键概念**：迁移学习、自监督学习、微调策略、任务适配、少样本学习
- **实际意义**：定义了现代NLP的标准训练流程；大幅降低了各种语言理解任务的数据需求；提升了模型在小数据集上的表现；为后续GPT、T5等模型提供了训练范式参考；推动了预训练模型的产业化应用。

#### 知识蒸馏在模型压缩中的应用

- **核心内容**：DistilBERT通过知识蒸馏技术实现了模型压缩的突破。核心思想是让小模型（学生）学习大模型（教师）的输出分布，而不仅仅是硬标签。使用KL散度损失函数衡量学生模型分布与教师模型分布的差异：KL(T||S) = Σp_T(x)log(p_T(x)/p_S(x))。这种"软目标"包含了比硬标签更丰富的知识信息，使得参数减半的DistilBERT仍能保持97%的性能。
- **关键概念**：知识蒸馏、软目标、KL散度、模型压缩、师生学习
- **实际意义**：为大模型部署提供了有效的压缩方案；启发了后续大量模型蒸馏研究；证明了分布学习比标签学习更有效；在资源受限环境中实现高性能模型部署；影响了移动端AI模型的优化策略。

#### Transformer架构分类与演进趋势

- **核心内容**：Transformer架构从最初的编码器-解码器结构发展出三大分支：编码器-解码器（T5系列）适合翻译等序列转换任务；仅编码器（BERT系列）专精于分类理解任务；仅解码器（GPT系列）主导生成任务并成为当前主流。这种演进反映了计算资源更好地投入到单一目标上的效率考量。现代LLM几乎全部采用仅解码器架构，因为下一词预测任务最容易扩展且与对话应用高度匹配。
- **关键概念**：架构演进、计算效率、任务专化、扩展性、应用匹配度
- **实际意义**：指导了后续模型架构选择；确立了生成式AI的技术路线；影响了计算资源的分配策略；为不同应用场景提供了架构参考；推动了专用模型向通用模型的转变。

#### RoBERTa的训练优化策略

- **核心内容**：RoBERTa通过系统性的训练优化证明了BERT存在训练不充分的问题。关键改进包括：移除次句预测(NSP)任务发现几乎无性能损失；采用动态掩码策略，每个epoch对相同文本使用不同掩码模式；大幅增加训练数据规模和多样性；延长训练时间。这些优化在相同模型架构下显著提升了性能，揭示了充分训练的重要性。
- **关键概念**：训练策略优化、动态掩码、数据规模、训练充分性、性能提升
- **实际意义**：建立了更有效的预训练范式；证明了数据和训练时间的重要性；简化了预训练目标设计；为后续模型提供了训练经验；推动了对模型训练深度的重新认识。

## 🔬 提及的技术/方法/论文

| 技术/论文 | 讨论语境 | 重要性 |
|----------|----------|--------|
| Attention Is All You Need (2017) | Transformer原始架构的奠基论文 | ⭐⭐⭐ |
| RoPE (Rotary Position Embedding) | 现代LLM位置编码的标准方法 | ⭐⭐⭐ |
| BERT (2018) | 双向编码器表示学习的里程碑 | ⭐⭐⭐ |
| T5 (Text-to-Text Transfer Transformer) | 编码器-解码器架构的代表 | ⭐⭐ |
| DistilBERT | 知识蒸馏在模型压缩中的应用 | ⭐⭐ |
| RoBERTa | BERT训练策略的系统性优化 | ⭐⭐ |
| Longformer (2020) | 局部注意力机制的早期探索 | ⭐⭐ |
| ALiBi (Attention with Linear Bias) | 相对位置偏置的确定性方法 | ⭐ |
| Group Query Attention (GQA) | 多查询注意力的改进版本 | ⭐⭐ |
| RMSNorm | LayerNorm的简化高效版本 | ⭐⭐ |
| ELMo | BERT同期的双向表示学习方法 | ⭐ |
| Multi-Query Attention (MQA) | 极端的键值共享策略 | ⭐ |

## 💬 经典金句（3-5 句）

> "现代模型几乎都基于原始的Transformer架构，只是在一些关键组件上有所变化。"
> — Afin（斯坦福教授）

> "我们想要远距离的token相似度比近距离的token更低，这是位置编码要解决的核心问题。"
> — Afin（斯坦福教授）

> "计算预算最好投入到仅解码器架构上，这就是为什么现代LLM都采用这种设计。"
> — Shervin（斯坦福教授）

> "软目标包含了几乎所有的知识，这比硬标签学习更有效。"
> — Hinton, Vinyals & Dean（引用）

## 👤 主要人物

#### Afin

**身份**：斯坦福大学CME295课程教授
**背景**：专注于Transformer架构和深度学习技术，在位置编码、注意力机制优化等领域有深入研究
**核心观点**：现代Transformer虽然在细节上有所改进，但核心架构仍然基于2017年的原始设计。强调数学直觉在理解复杂技术中的重要性，特别是在RoPE等高级概念的教学中。

#### Shervin

**身份**：斯坦福大学CME295课程教授
**背景**：专长于自然语言处理和预训练模型，对BERT系列模型的技术发展有深入了解
**核心观点**：BERT的双向编码能力是其成功的关键，预训练-微调范式定义了现代NLP的标准流程。认为编码器-解码器架构向仅解码器的转变反映了计算效率和任务匹配的重要性。

## 📺 视频类型判断

**教程示范**：学术课程、技术教学、架构解析

---

## 📝 完整翻译

### (0:00 - 10:37) 课程开场与上节回顾
> 回顾Transformer自注意力机制和多头注意力的基本概念

大家好，欢迎来到 CME295 的第二讲。

在开始之前，我想提醒大家两个课程安排上的事项。首先，我和 Shervin 回看了第一讲的录像，发现音频质量不太理想。所以这节课我们换了录音设备，但问题是我的声音可能无法在整个教室传播。请问坐在后排的同学能清楚听到我说话吗？好的，太好了。

第二个事项是关于期末考试。目前期末考试暂定在 12 月 10 日星期三，但我们正在尝试看能否提前到那一周的更早时间。具体安排确定后会通知大家，现在还待定。

好的，说完这些，让我们进入今天的主题。但在此之前，按照惯例，我们先快速回顾上节课的内容。

如果你记得的话，第一讲主要介绍了自注意力机制 (Self Attention) 的概念。自注意力机制就是每个 token 通过注意力机制关注序列中的所有其他 token。这里涉及查询 (queries)、键 (keys) 和值 (values) 的概念。基本思想是查询会询问哪些其他 token 与自己最相似，通过比较查询和键来实现，然后获取对应的值。

我们看到自注意力机制可以用这个公式表达：$\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$。希望大家对这个公式很熟悉。需要知道的是，这个公式是高度优化的，涉及大型矩阵乘法运算，我们的硬件在执行这类运算时非常高效。

我们还介绍了 Transformer 架构，如右图所示。Transformer 由两个主要组件构成：左侧的编码器 (encoder) 和右侧的解码器 (decoder)。Transformer 最初是在机器翻译的背景下提出的。你可以把左侧理解为处理源语言（比如英语）的输入文本，右侧负责解码目标语言（比如法语）的翻译。多头注意力层 (multi-head attention layer) 就是自注意力机制发生的地方。

我记得有很多关于多头注意力层的问题——既然叫多头，那么多个"头"对应什么？在 Transformer 论文《Attention Is All You Need》中有这样一个图，实际上展示了每个头的作用。你可以把每个头理解为模型学习将输入投影为查询、键或值的一种方式。

更具体地说，对于查询和键，每个头都对应这些小方框，方框的数量就是头的数量。

为了更好地可视化和理解这意味着什么，我想展示论文中的内容，即解释每个头作用的一种方式。我们有一个叫做注意力图 (attention map) 的概念，它试图表示每个查询点积的值。

在这个例子中，我们想看看哪个其他 token 与 token "its" 最相似。我们计算代表 "its" 的查询与所有其他键的点积，看看哪些键导致查询乘以键的高值。

当你这样做时，你会看到论文中显示的情况：有两个词 "application" 和 "law" 被突出显示为具有高注意力权重。这里的注意力权重就是查询 "its" 与每个 token 的键的点积。

这也有一种解释方式。你可以看到被突出的 token 是 "law" 和 "application"，这很合理，因为 token "its" 指的是法律 (law)。基本上，模型需要学会如何将这些词与之前发生的事情联系起来。它也指向 "application"，这也是另一种解释方式。

作者选择将这些值作为不同头的函数来展示。比如左侧显示的是第一个头的强度，第二个头显示法律 (law) 的强度非常高。长话短说，这些头可能学习不同的方式来弄清楚哪些词是重要的。

学生：当我们进行所有这些计算时，它们是通过不同的 MLP 吗？

很好的问题。答案是我们将为每个头使用不同的投影矩阵。你可以这样理解，我们有详细的例子，每个头都有自己的投影，并且这些计算会并行进行。

每个头都会有一个结果，然后这些结果会被连接起来，再用输出矩阵进行一次投影。

长话短说，这是高度并行化的，基本上就是投影操作，这里有矩阵乘法和 softmax。明白了吗？

还有其他关于这个的问题吗？

好的。我想这只是为了说明我们在第一讲中的讨论。我知道有一些关于注意力头及其作用的问题，我想查看注意力图是理解它们含义的一种方式。

我强烈推荐大家阅读 Transformer 论文《Attention Is All You Need》。这是一篇非常密集的论文，只有几页，但我希望通过第一讲的内容，你们能够以一种有意义的方式理解其中的内容。

好的，现在我们开始今天要讨论的核心内容。

令人惊讶的是，这个在 2017 年引入的 Transformer 架构，实际上是一个多年来仍然保持相关性的架构，只有少数组件有轻微变化，我们将看到这些变化是什么。有一些轻微的变体，但总体而言，今天的模型或多或少都基于最初的 Transformer 架构。

我们将把课程分为两部分。第一部分由我来讲，内容是 Transformer 的哪些部分是重要的，以及有哪些变化。第二部分，Shervin 将讨论当今模型的术语以及它们与原始 Transformer 的关系。

### (10:37 - 42:30) 位置编码的演进
> 从学习式位置嵌入到正弦位置编码，再到现代RoPE旋转位置嵌入

好的，让我们从这个架构中的第一个重要概念开始：位置嵌入 (position embedding)。

如果你记得的话，我们让 token 以直接的方式与所有其他 token 交互，它们有直接的连接。但与 RNN 等按顺序依次处理每个 token 的方法不同，这里你基本上失去了一个 token 在另一个之前被处理的概念。所以你失去了这种位置信息。

因此，我们需要以某种方式量化每个位置的 token，并在 Transformer 处理输入时注入该信息。

我们要怎么做呢？原始 Transformer 论文的作者选择使用专用嵌入。当我说专用时，意思是每个位置都有一个嵌入。

位置一有一个嵌入，位置二有一个嵌入，等等。

他们选择将该嵌入添加到输入 token 嵌入中。

例如，如果我说"a cute teddy bear is reading"，其中"a"是位置一，它将由表示 token "A" 的向量加上表示第一个位置的嵌入来表示。

学生：位置嵌入是学习的还是静态的？

很好的问题。答案是两者都有。作者尝试了两种方法，我们将看到第二种是什么。但这里假设它们是学习的。这意味着你需要为每个位置学习嵌入。

这种方法的问题是你非常依赖训练集中的内容。例如，如果你的文本在位置二总是发生某些事情，你学习的嵌入会带有这种偏见。这是一个限制。

第二个限制是你只能学习到训练集中最大位置数的位置嵌入。假设你在长度最多为 512 的序列上训练 Transformer，你只能学习到该位置的位置嵌入。

学生：你如何参数化它？

你设置一个可学习位置嵌入的占位符，比如位置一到 512 之间。在训练时，你只是让这些权重通过常规的梯度下降等方法学习。

这是第一种方法，但如我所说，它有局限性，因为你只能学习训练集中存在的最大位置的嵌入。例如，如果在推理时有超出训练集位置的位置，你没有学习过，需要找到推断值的方法。这是第二个限制。

### (15:00 - 30:00) Part 2

这就是第二个限制。但在积极的一面，你只是让模型学习，我们已经看到梯度下降在从数据中学习方面表现出色。基于这些原因，作者提到的第一种方法表现良好，同时还有第二种不同的方法，即为每个维度的位置嵌入使用任意公式。

第一种方法是每个位置有一个嵌入，然后让模型学习。第二种方法是每个位置也有一个嵌入，但你不会学习它，而是使用预先确定的公式。我们将看到作者选择的是使用正弦和余弦的公式。这可能感觉有点奇怪，为什么选择这个？但我们会理解其中的道理。

这里的想法是，对于给定的位置 m，有一个大小为 $d_{model}$ 的向量。d 需要匹配你的 token 嵌入的维度，因为你要将它们相加。对于每个索引，你将根据这些公式计算对应的值。

这些公式是什么？基本上是 $\sin(\omega \cdot m)$ 和 $\cos(\omega \cdot m)$，我们将看到这个 omega 的含义。

在深入之前，让我们简化符号。假设你看到的这个大量是 omega，设 $\omega_i = 10000^{-2i/d_{model}}$。假设你这样构造嵌入。

我想让我们思考为什么这样做有意义。如果你仔细想想，你想要以一种反映以下事实的方式来表示位置：相邻的词更可能相关，而相距较远的词则不那么相关。如果你有两个词，一个相距一个位置，另一个相距一万个位置，你希望相距一个位置的更相似。

让我们看看这个公式是否有意义。假设你有两个位置嵌入，一个在位置 m，另一个在位置 n。假设你从这个预定公式计算所有值。

如果你还记得三角恒等式：$\cos(a-b) = \cos a \cos b + \sin a \sin b$。

事实证明，如果你表达 $\cos(\omega_i \cdot (m-n))$，这就是你得到的结果，正如我提到的恒等式。这个量正是当你计算这两个位置嵌入的点积时出现的一个分量。

因为当你计算位置 m 和位置 n 的点积时，你取第一个位置相乘，然后加上第二个位置相乘，等等，最终得到 $\sin(\omega_i m) \sin(\omega_i n) + \cos(\omega_i m) \cos(\omega_i n)$，这正是这个量。

所以最终你意识到，当你计算这些嵌入的点积时，你得到的是一个余弦函数的和，它是 m 和 n 之间相对距离的函数。

学生：你说的"配对"是什么意思？

确切地说。基本上，距离越近，它们越相似。这就是这种嵌入公式试图近似或模仿的直觉。

这样你基本上得到一个只是 m 和 n 之间相对距离函数的点积。提醒一下，为什么我关心点积？因为在嵌入世界中，当我们试图量化两个嵌入之间的相似性时，通常会使用涉及这两者点积的方法。你通常有余弦相似度，但余弦相似度只是点积除以每个嵌入的范数，基本上就是点积。这就是为什么我们关心点积。在这里我们看到，很好，它是两者之间相对距离的函数。

特别是，如果你还记得三角学，$\cos(0) = 1$，这个数字越高，余弦值越低。当然，它是周期性的，所以我说的在超过 $\pi$ 后不一定成立。但我想说的是，对于 $m = n$，你基本上会有余弦零的和，这是这个量的最大值。

所以当 $m = n$ 时，这个量是最大的，这意味着如果你看位置本身，它是最相似的，这符合我们的直觉。

当你绘制嵌入的值时，你得到这样的结果。在这个图中，y 轴表示所有 50 个位置的嵌入，x 轴是沿给定向量在向量的多个维度上的值。

如果你取第一行，你看的是第一个或零号位置（取决于你如何索引向量），你看的是位置零嵌入的所有值。你会看到对于低维度，这个值经常上下变化，所以是更高频的；当维度高时，值上下变化需要很长时间，所以基本上是更低频的。

这与我提到的 $\omega_i$ 有关。$\omega_i$ 对于 i（维度）的低值非常高，对于 i 的高值非常低。这基本上只是决定了你的余弦和正弦变化的快慢。

这就是原作者尝试的方法，他们注意到使用这种方法能够获得与学习方法相当的结果。但这里我们有一个很大的优势，因为它可以扩展到任何序列长度，不仅仅是训练时看到的序列长度。这是为什么这可能是更优选择的原因之一。

现在快进到 2025 年，你可能会问我们是否还在使用它？答案是某种程度上是的。我们仍在使用这个想法，即希望距离较远的 token 比较近的 token 相似度更低。

但我们不像他们那样注入嵌入。我们将看到为什么。

因为如果你记得，你关心的是在自注意力计算中确定 token 的相似程度。自注意力计算在哪里发生？在注意力层。

但这里我说了什么？我说让我们计算这些嵌入并在这里添加它们。但实际上我们想要的是在注意力层中反映这种相似性。

学生：在第一种方法中，它是添加到输入特征吗？是的。

是的，只是相加。但问题是我们主要希望这种直觉在多头注意力层中成立。这是人们尝试不同变体的原因之一，特别是让这些位置嵌入直接干预注意力层而不是输入。

因为当你在输入处这样做时，它将大致进入注意力层，但这是间接的。我们想要的是直接对注意力公式做一些事情，以反映我们希望相近 token 比远离 token 更相似的事实。

我们这样做的方式是，如果你记得，自注意力层基本上是 $\text{softmax}(QK^T/\sqrt{d}) \cdot V$。我们想要在 softmax 内部添加一些东西，这基本上是你量化一个 token 与另一个 token 相似程度的地方。你想添加一些东西来反映某些 token 应该比其他 token 更相似的事实。

有一些方法尝试了这种变体。对于那些了解 T5 论文的人，我们稍后会看到，他们通过学习偏置项尝试了这种相对位置偏置。

他们的想法是，假设你有位置 m 和 n 之间的给定距离。他们的想法是让我们学习，基本上将所有 $m - n$ 分桶，让模型学习这些将被注入 softmax 的量。

学生：偏置在这里是否会对最终的概率造成问题，因为它必须和为一？

你可以在 softmax 内部做任何你想做的事情，因为 softmax 无论如何都会对其进行归一化。你可以将偏置视为对于距离较远的事物更负，相比于距离较近的事物。

所以这种方法通过在注意力机制内部直接建模位置关系，而不是在输入层添加位置嵌入，能够更直接地影响 token 之间的相似性计算。T5 的相对位置偏置方法通过学习不同距离桶的偏置参数，在保持 softmax 归一化特性的同时，实现了位置感知的注意力计算。

这种演进反映了对位置编码机制理解的深化：从最初的绝对位置嵌入，到正弦余弦公式，再到直接在注意力层建模相对位置关系。每种方法都在解决如何更好地让模型理解序列中元素之间的空间关系这一核心问题。

### (30:00 - 45:00) Part 3

所以 T5 说让我们学习这些偏置。我们还有另一种方法，来自一篇名为 Teslong 的论文，它引入了一种叫做 ALiBi 的方法。ALiBi 代表 Attention with Linear Bias（带线性偏置的注意力机制）。他们的做法是，不学习那些偏置，而是有一个确定性公式，作为两个位置之间相对差异的函数。他们得到了一些结果，这些论文总是相互比较，看哪一种性能更好。

但现实是，在今天的模型中，大多数模型实际上使用另一种位置嵌入方法，我们现在就来看看。

这种方法依赖于将 query 和 key 向量旋转某个角度。你可以这样想象：假设你有 query 和 key，假设在 2D 空间中，你要做的是将 query 旋转某个角度，这个角度是其位置的函数，然后将 key 向量旋转某个角度，这个角度是其位置 n 的函数。

那么你如何做到这一点呢？假设你有一个向量，你想旋转那个向量。谁在这里做过空间中的旋转？是的，这是正确的。通过矩阵乘法，你会使用一个叫做旋转矩阵的量。

旋转矩阵表达如下：基本上是一个 2x2 矩阵，在 2D 平面中包含 cosine θ、-sine θ、sine θ、cosine θ。我看了看时间，实际上只是简单地展示它如何工作是很简单的，但我们可能没时间了。你想让我快速展示一下这确实是旋转向量的方法吗？

好的。作为提醒，我们要展示的是可以使用矩阵乘法在 2D 空间中旋转向量。假设我们有以下向量，可以用两个维度 x 和 y 来量化。你可以在 2D 空间中用这个来表达你的向量，但是如果你注意到，这些是向量的范数和相对于 x 轴的角度 φ。

你也可以将 v 写成 r 乘以 cosine φ 和 sine φ 的向量形式。如果你用旋转矩阵乘以这个 V，你将得到 cosine、sine 和 cosine 的某种乘法。我将把这个练习留给你们，但你们可以证明旋转矩阵乘以这个 v 可以表达为 r 乘以 cosine(θ + φ) 和 sine(θ + φ)。

这是一个快速证明。我只是给你们留下旋转矩阵和 V 的乘法，但你们会得到这些三角恒等式，将导致这个公式。这基本上表明，如果你将这个矩阵和这个向量相乘，你基本上是将向量旋转了这个角度。

问题是为什么你想这样做？这是一个很好的问题，这是我的下一张幻灯片。这只是一个小介绍。回到这些方法，我们想要做的是量化 token 之间的相似性，让相近的 token 比距离较远的 token 更相似。

我们之前方法的问题是，在第一种方法中，这种学习到的嵌入总是有过拟合问题，因为当你学习这些偏置时，它总是依赖于你拥有的训练集。也许你的数据集中相近的 token 是相似的，但方式与你在推理时看到的不同。这种 ALiBi 方法没有可学习的组件，但这相当受限制，因为毕竟这是一个非常简单的公式，只是 n 和 m 之间的相对差异。

所以人们尝试了不同的方法来提出某种嵌入，反映你希望更远位置比更近位置相似性更低的事实。在这种方法中，我们回到作者提出的 sine 和 cosine 世界，从 cosine 和 sine 函数的角度思考相似性。

这是一个小介绍，他们的方法叫做 RoPE。我不确定你们是否听说过它。它代表 Rotary Position Embeddings（旋转位置嵌入）。我们将看到这种方法。

为什么我们关心这种方法？这种方法有两个很棒的特点。首先，如果你旋转 query 和 key，你最终会得到一个量，它将是两者之间相对距离的函数。这将非常好。这就是为什么我在黑板上写了这个。

特别是，如果你记住你的注意力公式，你有 query 乘以 key 的转置。如果你将 query 旋转角度 m，key 旋转角度 n，你最终会得到一个包含角度 θ 旋转矩阵的公式，我想是 n 减去 m。这很棒，因为这是这两个位置之间相对距离的函数。

为什么我详细讲这个？因为事实证明，现在大多数模型都使用 RoPE，这就是为什么它很重要。我想说另一件事，可能很难理解为什么这样有效的直觉，但希望我在开始时给出的关于 sine 和 cosine 的解释能帮助你建立这种直觉。

说到这一点，事实证明由 query 和 key 给出的注意力权重的上界是这样的，我们观察到长期衰减。这意味着当 m 减去 n 很大时，我们确实看到上界变得越来越小。你看到这些小振荡，它也不是完美的，但我们确实有一些数学结果，关于上界在长期内衰减。

对此有什么问题吗？是的，完全正确。问题是相对距离在旋转矩阵中被捕获，是的。

这是一个很好的问题。问题是 θ 是什么？实际上 θ 是固定的。你记得我在这里谈到的 ω 吗？它基本上是 i 和 d 的某种函数。我实际上很快就跳过了。我向你展示的是在 2D 空间中，但这里我们在大于二的维度空间中。我很快就掠过了，但你扩展这种方法的方式是通过按块拥有这个 2D 空间。但是 θ 是一个函数，通常是你固定的东西，但是 i 的函数，i 是维度，基本上在 1 和 d/2 之间，它是那个的函数，也是 d 的函数。

所以你通常会看到这个 θ 大致等于 ω_i，这个差不多。问题是它与潜在维度有相同的维度。这里你有矩阵的乘积，所以你需要维度匹配。

我花了很多时间在这上面，因为我认为这实际上相当重要。很多模型使用这个，直觉不是很明显。所以我希望这是有帮助的。这就是位置嵌入。

问题是关于如何获得这条曲线？这实际上是一条在数学上显示的曲线，因为基本上这很复杂。我们不会写下公式，但如果你感兴趣，在 RoPE 论文中有一个附录，他们在数学上证明了它被某个量上界，这就是这里显示的。很好的问题。

### (42:30 - 50:44) 层归一化的变革
> 从后归一化转向预归一化，从LayerNorm到RMSNorm的优化

位置嵌入是 Transformer 的一部分，它已经改变了一点，我们已经看到了它是如何改变的以及为什么。现在我们要看 Transformer 的另一个组件，它也稍微改变了一点，那个组件是层归一化。

如果你记得 Transformer 架构，它再次由编码器-解码器组成，然后你有内部的组件。你有这些写着"add and norm"的框。它们是什么意思？基本上这里我们做的是取这个子层的输入以及输出，将它们加在一起，然后归一化。这是作者们做的一个小技巧，在实践中被证明可以改善收敛并使收敛更快。

想法如下：如果你有一个向量，有时向量的组件可能非常大，有时可能非常小。这里的想法是将向量的组件归一化到某个范围内，某个归一化范围内。

### (45:00 - 1:00:00) Part 4

具体做法是取你的向量，然后减去计算出的均值（基本上就是其组件的总和），然后用标准差进行归一化。你需要学习两个量：一个是 gamma，它是重新缩放因子；另一个是 beta，这是你学习的另一个因子。你将让模型学习这两个量。

如我所提到的，在实践中，这样做有助于训练稳定性和收敛时间。这是原始 Transformer 论文中使用的技术。

我想指出，从那时起已经有了一些变化。我们从对子层的输入加输出进行归一化，改为对输入和归一化输入的子层求和。换句话说，我们改变了归一化的位置。在 Transformer 论文中，这被称为 post-norm 版本，而现在我们使用 pre-norm 版本，它基本上是在向量进入子层之前进行层归一化，这里的子层可以是注意力层或 FFN。

不仅如此，还有另一个变化。现在人们不使用 layer norm，他们使用一种叫做 RMS norm（均方根归一化）的东西，这基本上是你之前看到的一个变体。人们不再计算这个，而是仅通过 X 的组件的均方根来归一化 X，并且只学习 gamma。为什么这样做？基本上他们表明收敛特性是相当的，但这里你需要学习的参数更少，所以它基本上更快。

很好的问题。我想问题是归一化背后的直觉是什么？直觉是，如果你看你的模型，你有几层，在某些层中，你的模型，你的向量，更准确地说是你的激活。你看到从这里到这里的向量基本上被称为激活。有时激活在其组件的一部分中具有极值，有时在另一部分中。如果这些激活变化太大，模型通常在学习每一层的权重方面遇到困难。

所以想法是将激活组件的值带到某个范围内，不要在某个方向上偏离太远。如果你有兴趣，有一个关键词叫做"内部协变量偏移"（internal covariate shift），这基本上是对我在这里描述的现象的术语。这就是直觉。

很棒的问题。问题是这与批归一化（batch normalization）有什么区别？批归一化是跨另一个维度的归一化，即批次的维度。假设你有一堆向量，你做的是相对于其他向量的相同维度的所有其他组件来归一化每个组件。

### (50:44 - 1:02:27) 注意力机制优化
> 滑动窗口注意力和分组查询注意力(GQA)降低计算复杂度

你可以把它看作是另一种归一化方式。但话说回来，当涉及到这些基于 Transformer 的模型时，人们通常使用层归一化，可能是因为经验上它工作得更好，但也因为批归一化基本上也依赖于批次，它可能在训练和推理之间引入一些差异。这基本上就是原因。

很好。我们已经看过位置嵌入，我们已经看过层归一化。现在我们将看到 Transformer 的第三个重要组件，即注意力。

特别是，我想有一些我没有真正强调的，但当你做自注意力时，你基本上让每个 token 与所有其他 token 交互。当你看一个显示所有交互的矩阵时，你有 n（序列长度）乘以 n（序列长度）。你基本上有 $O(n^2)$ 的复杂度，这是很多的，特别是随着 n 变长。所以人们试图将这个 $O(n^2)$ 近似为更易处理但不会失去性能的东西。

2020 年有一篇论文叫做 Longformer。它所做的只是通过限制操作的窗口来尝试不同版本的注意力。在这里，不是让每个 token 与每个人交互，每个 token 只与其邻近区域交互。

又是一个很棒的问题。问题是，你是在注意力矩阵计算之后做这个吗？你是指 softmax，对吧？所以你提出了一个很好的观点，如果你对所有东西做 softmax，为什么要这样做？在实践中，有一堆实现做了一些巧妙的操作，叫做平铺，一些巧妙的操作，不涉及你在 softmax 中看到的巨大矩阵操作。

我想它就像 $Q K^T$ 的 softmax。你不会计算整个东西。在如何计算方面会有一些巧妙性。基本上是的。问题是，它能与卷积相比较吗？我们马上就会看到。但是是的，你与视觉世界有一些相似性，我们马上就会看到。

很好。现在当你有这样的局部注意力时，人们使用术语滑动窗口注意力。当他们使用这个术语时，他们指的是这个，基本上只是将注意力限制在邻近的 token 上。

现在人们做的是在某些层中他们会有局部注意力，在其他层中他们会有全局注意力，他们会交错这些层。根据模型，他们通常尝试不同的组合。所以没有固定的配方，但这是现在通常使用的东西。这里的窗口，我是说出于说明目的，窗口在我的插图中超小，但现在随着序列长度可能非常大，你可以把这个窗口想象成有几千个。所以它看起来相当大。

只是给你另一个例子。回到你提到的卷积比较。你有一些架构，这里我要以 Mistral 为例，它在每一层都有这种滑动窗口注意力。但是当你思考它时，这里的 token 可以关注到这里的 token，然后这里的 token 可以关注到这里的 token，等等。

如果你想一想，这有点类似于计算机视觉中感受野的想法。我不确定你是否熟悉计算机视觉世界，但如果你熟悉，这意味着只是取一个 token 并试图思考这个 token 实际上与哪些其他 token 交互过，这基本上是人们在做卷积时有时也会问的问题。

他们会说好吧，这个值实际上看到了哪些其他值，所以你也可以这样想。

第一个变化是不做完整的 n×n 注意力，人们有时做局部注意力。与所有这些正交的第二个变化是不为每个头有一个投影矩阵，而是跨头共享投影矩阵。

这里的想法是你有 h 个头。想法是你将为查询有一些数量的投影矩阵，但然后你要做的是将键和值的投影矩阵分组，你将跨几个头共享。

现在，你可能会问，为什么你要为键和值共享投影矩阵但不为查询共享？这是你想知道的问题吗？

### (1:00:00 - 1:15:00) Part 5

所以问题是我们是否将这个应用到自注意力和交叉注意力？我不想剧透后面的内容。我稍后会讲到其他东西。但是如果你看看 Transformer 架构，我们很快就会看到我们最关心的是解码器中的内容，因为在现代模型中我们实际上放弃了编码器。我们还没有看到这个，但我只是告诉你们，这通常在解码器的掩码自注意力中发挥作用。

但是这种技术可以应用到所有的注意力层。但我想告诉你们的是，现代 LLM 是仅解码器模型，基本上只有 Transformer 的解码器部分。我们马上就会看到这个，也就是掩码自注意力。我不会说太多，因为 Shervin 会讲解这个。

很好的问题。所以问题是你什么时候知道应该使用哪一种？我会说选择总是由几个因素驱动的。一是它的表现如何。二是你多在意延迟成本这样的东西？这真的取决于你的模型有多大，你想在计算上节省多少，比如你的输入长度如何，例如如果你有较短的输入长度。所以这里我们想要做的是避免必须为整个 O(n²) 的东西做所有这些事情。所以我猜所有这些都会发挥作用。我会说答案应该是什么并不直接。但我会说很多最近的模型都倾向于共享投影矩阵。所以通常我会说 GQA 是你会看到的，但这并不一定适用于所有模型。

### (1:02:27 - 1:09:32) Transformer架构分类
> 编码器-解码器、仅编码器、仅解码器三大架构类别及应用

好的，我们时间不多了，所以我要让 Shervin 来讲第二部分。谢谢。

谢谢。我们将继续这次讲座，更深入地探讨 Transformer 领域中的各种模型。然后我们将深入研究一个对分类设置非常有用的特定架构。

首先我们要回到上次一起看到的架构。这个传统的编码器-解码器架构，你有两个组件。你有 2017 年的原始 Transformer 论文，有这种架构，但后来你也看到更多建立在它之上的架构。所以这里我们谈论 T5 模型家族。T5 是一篇论文的缩写，包含多个 T。第一个是 transfer，然后是 text-to-text transformers。这就是 T5 命名的来源，然后它派生出多个版本。

T5 是基础论文，然后有 mT5。M 代表多语言，其中在训练数据以及计算词汇表方面做了更多工作。然后你有 byT5，这是一种无 tokenizer 的方法，你放弃了 tokenization 的事实，而是在字节级别操作。所以 byte 是字节，基本上你的词汇表大小要小得多。所以不是有大约 30k，你有 2^8。所以一个字节是 8 位，然后你可以用两个字节表示每个字符。这就是他们所做的。

关于 T5 家族我想提到的一件事是，目标函数与原始 Transformer 做的有点不同。原始 Transformer 为训练任务做下一个 token 预测，但 T5 家族所做的是，他们操作所谓的 span corruption 任务。

基本上你会有你的句子作为编码器的输入，而不是把所有东西都放到编码器中，你会留下空白，这就是我们所说的 span corruption，然后 span corruption 可能是一个或多个缺失的 token。

如果我想举个例子，比如 "my teddy bear is cute and reading"。你可以有 "my teddy bear" span corrupted，然后是 "is reading"。这可能是一个潜在的编码器输入，然后你可以有多达 n 个这样的 token，n 是被破坏的 span 数量的参数化。T5 家族称它们为 sentinel token。所以如果你看到 sentinel token，它们代表一系列被破坏的 token。

解码器的工作是依次找到每个这些 span。所以你从表示第一个被破坏 span 的 token 开始，然后开始解码过程，直到它预测到下一个 sentinel token，直到第 N+1 个，其中在两个连续 sentinel token 之间解码的 token 对应于被恢复的被破坏 span。

这基本上是从下一个 token 预测目标函数的转变。

问题是你能详细说明解码过程吗？所以你关于重建说的完全正确。所以你有表示一些缺失文本的 sentinel token。所以你想要解码的是那个缺失的文本。所以解码器输出将完全像每个重建的 span。然后如果你想知道训练如何工作，你做一个 teacher forcing 机制，你在解码器中输入所有东西，然后尝试一次重建所有东西。

现在我们要讨论另一类 Transformer，基本上你有这种编码器-解码器结构，你只是忘记解码器，然后只处理编码器。

所以你可能会告诉我，好吧，嘿，你不能用这个做生成，然后我会回应你，是的，这正是重点。所以这个编码器有编码器表示，可以用于可能更面向分类的任务，比如情感提取、token 分类，像所有这些过去用特定语言模型完成的事情，它们可以用 Transformer 的编码器部分完成。我们稍后会更深入地研究三个关键的仅编码器模型。BERT 是我认为这个领域的核心模型，然后是两个其他架构 DistilBERT 和 RoBERTa，它们在改进轴上进行研究。

最后有一个类别。正如我刚才提到的，今天的 LLM 完全移除了编码器部分，当你没有编码器时，你就没有在堆叠编码器末尾的编码嵌入，这些可以提供给交叉注意力。所以这个模块完全消失了。所以每个堆叠的解码器只有掩码自注意力和一个 FFN 作为它的一部分。

这基本上是从那时以来流行起来的东西，因为当你看每个这些模型的流行度时，你过去有这种类似 Transformer 的架构，在开始时很流行，主要假设是编码器部分对于获得解码表示非常有用，但随着时间推移，人们意识到你的计算预算最好投资在仅解码器上。然后有更多投资投入到我认为最容易扩展和泛化的任务，下一个词预测，而不是我刚才为 T5 提到的任务，这可能更加定制化，所以你需要破坏东西，所以它更复杂，而下一个词预测是我认为你能做的最简单的事情。

### (1:09:32 - 1:47:15) BERT深度解析
> BERT的双向编码、掩码语言模型、分类微调全流程剖析

它证明了奇迹，并且与成为有帮助的聊天机器人的任务很好地对齐，这主要是今天的应用。

仅解码器架构，我今天不会谈论它们，但当我们越来越多地谈论 LLM 时，它将成为下次讲座的核心部分。

好的，太棒了。现在我们可以深入研究如承诺的仅编码器架构和 BERT。

首先我们要看 BERT 意味着什么。BERT 是一个首字母缩略词，表示来自 Transformer 的双向编码器表示。我们将一起看这个缩略词的每个部分对应什么。让我们从编码器部分开始，这是最容易理解的。正如我们所说，我们只是丢弃解码器。所以来自 Transformer 的编码器基本上就是它的意思。

现在另一部分，为什么我们谈论双向性？

这是一种方式，论文的结果有点显著，因为我们能够从给定输入获得输出表示，这些表示对每个 token 都关注了所有东西。

这是因为我们只有编码器，我们有这个真正关注每个其他 token 的自注意力层。

这与你拥有的掩码自注意力形成对比，我们说掩码使注意力机制具有因果性。所以每个 token 可以关注自己和它之前的 token。顺便说一下，这是作者在论文中大量讨论的，说 GPT 出现了，GPT 不是真正双向的，然后这些可以用于分类任务的编码真的是双向的。

有问题吗？

是的，这是对的。所以问题是当你没有掩码时，每个 token 可以关注彼此。你知道，这完全正确。然后掩码恰好在那里防止从 token 到那些在它们之后出现的 token 的链接。

我只想把这篇论文放在背景中，NLP 领域那时正在蓬勃发展。所以同年你有另一篇里程碑论文叫 ELMo。来自语言模型的嵌入，我会说那篇论文的时机有点不幸，因为它真的有构建双向表示的新见解，但结果是它与 Transformer 同年出现，作为一种基于 Transformer 的同类工作。所以它被掩盖了一点。

ELMo 主要思路是，它基于双向 LSTM，你有多层堆叠在彼此之上，基本上你能够通过这种架构为每个词构建双向表示。那么为什么它没有像 BERT 一样受欢迎？这是因为你有与以前模型相同的缺点，基本上由于这种递归性很难扩展。

### (1:15:00 - 1:30:00) Part 6

你能够通过这种架构为每个词构建双向表示。那么为什么它没有像 BERT 一样受欢迎？这是因为你有与以前模型相同的缺点，基本上由于这种递归性很难扩展。

当你想到 ELMo 和 BERT 时，你一开始并不会想到这些论文，因为它们都是《芝麻街》里的角色名字。我成长的地方并不了解《芝麻街》，所以对我来说 ELMo 和 BERT 就只是论文名称，但你们可能一开始会有不同的联想，我觉得这挺有趣的。研究者们通常很有趣，他们会试图将缩写词融入主题。所以如果你看论文名字，会觉得很有意思。

让我们深入探讨我刚才提到的纯编码器模型的目标。你有一组 Token 作为输入，目标是执行一些可能专注于将某种表示投影到某处的任务。通常是分类任务，BERT 的工作方式非常具体。

你会看到两种在结构上比较特殊的 Token。首先是这个 CLS Token，代表分类，它基本上是一个占位符 Token，放在序列的开头，然后在整个注意力机制、投影以及整个编码器机制的末尾，被投影到一个嵌入中，我们可以用它进行分类。

CLS 只是某种占位符，它将承载整个输入的双向信息。

另一个有用的 Token 是 SEP Token，代表分隔符。我们很快就会看到它所操作的目标函数，它旨在分隔两个句子。

这个模型有一个非常有趣的概念，就是多阶段训练。你不是一次性训练模型，而是分多个阶段。第一阶段旨在与感兴趣的任务对齐。这就是我们所说的预训练。

我们会详细了解，这种预训练通过两个目标函数完成，分别是 MLM 和 NSP。MLM 代表掩码语言模型，NSP 是下一句预测。

掩码语言模型是让模型学习输入内部结构的一种方法，而 NSP 可以看作是判断句子顺序是否合理的方法。我们会更深入地研究每一个，但这是作者假设的目标函数组合，有助于学习高质量的通用嵌入。

第二点我想提到的是，一旦你有了所有这些，你还有一个进一步的阶段，保留你学到的嵌入，然后在其上附加其他网络，通常是线性投影，然后微调你学到的嵌入以适应某些目标任务。这就是我们所说的微调。

问题是我们这里仍然只有编码器吗？因为下一句预测可以看作是解码器任务。实际上，下一句预测任务是我们将两个句子一个接一个地放在一起，预测它们是否真的连续。这是一个分类任务。

在详细了解之前，我想讨论一下这种方法通常的优缺点。

在优点方面，这种预训练机制可以在相当无标签的数据上完成。你仍然有下一句预测任务，但这是你可以控制的，因为你知道哪些句子彼此跟随。所以这是某种自监督的方式。

掩码语言模型任务也是无监督的。这是从无标签数据中学习有趣嵌入的非常有趣的方法。在实践中，我们看到这种无标签数据导致学习到有用的表示，然后你只需要很少的数据就可以在此基础上构建。

在微调阶段，你从这些学到的非常好的嵌入开始，只需要调整少量权重，这通常会导致超越当时最先进水平的性能。

在缺点方面，所有文本生成任务都无法触及，因为我们没有解码器。我们也可以看到这种两阶段过程和需要进一步调整嵌入的事实可能是障碍。

当你将这种方法与更传统的一次性方法比较时，这可能被视为缺点。

我想让我们专注于原始的 Transformer，逐步了解它发生了什么，以及我们如何得到 BERT 架构。

这是我们上周看到的原始 Transformer 论文中的内容。BERT 所做的是提取其中的编码器部分，并基本上附加我刚才提到的新目标函数，以及一些在表示 Token 方面的新技巧。

首先，一个有趣的注意事项是它使用了一个叫做 WordPiece 的特定分词器。你可以将其视为在训练集上学习的分词器，基于最大化似然的合并规则。基本上你有一个巨大的训练集，你训练一个分词器，将原子 Token 合并在一起构建你的目标词汇表。

我提到了大约 30k 的数量级，这通常是他们在论文中选择的大小。一般来说，这类论文中的词汇表大小都在 10^4 的数量级。

除了我们看到的 T5 的无 Token 方法，它有非常受限的 Token 集合，如 2^8 = 256。除了这种特殊情况，你总是有这种数量级。

我们将使用我在开始时提到的 Token。你有 CLS Token，它将承载整个序列的双向表示，然后你有 SEP Token 来分隔两个句子以进行下一句预测任务。

还有一些我还没有谈到的内容。为了有这个掩码语言模型任务，你当然需要掩码一些 Token。我们将看到基本上应用这种掩码的技术、在哪里以及以什么频率，以及基于结果表示的输出任务。

关于输入嵌入的一大新闻。保持不变的是，你仍然有这个为每个 Token 的巨大字典查找嵌入，你要学习并加法添加我们在上次讲座中看到的位置编码，这可以是硬编码或学习的。

但有新的内容。引入了一种新的编码叫做分段编码，它仍然会加法添加到 Token 上，除了我们只有两种可能的编码。你有表示第一个句子的分段 A，然后有表示第二个句子的分段 B。它应该有助于 NSP 任务，表示可以有助于表示一个句子在另一个句子之前的特征。至少这是作者提出的假设，我们会看到这后来受到了挑战，但这是这里引入的关键概念之一。

问题：分段编码到底做什么？这基本上是学到的东西。你有两个索引，基本上是两个可以学习的嵌入。你只是加法添加它们，第一句中的 Token 用分段 A，第二部分用分段 B，你用梯度下降学习它。

同一句子中的每个 Token 是否会有相同的分段编码？是的。

我想强调的第二点是，我们采用 Transformer 的编码器部分，这里没什么新的。我们有自注意力，然后是 FFN。这就是你得到编码双向性质的地方。然后我们假设在其上训练 MLM 和 NSP 将帮助我们学习这个编码器中有用的投影矩阵，可以适用于任何分类任务。

正如我承诺的，我谈到了更详细地说明这个 MLM 任务要做什么。当你查看输入时，你基本上会有你的输入句子并随机替换一些 Token。

它将被替换，80% 的时间用掩码 Token，10% 的时间为这个 MLM 目标函数选择的 Token 根本不会被替换，我们只是说是同一个 Token，预测同一个 Token，然后 10% 的时间它将被更改为其他随机词。最后，你有这个 Token 子集，你将在其上执行 MLM 任务。

这里的直觉是，当你想预测一个 Token 是什么时，你需要了解它的上下文。所以你要强制模型学习它左边和右边的内容。这种双向属性就是这种架构付诸实践的体现。

### (1:30:00 - 1:45:00) Part 7

接下来我们要讨论下一句预测任务，基本上这个过程是从给定语料库中选择两个句子，然后将它们并排呈现，50% 的时间按正确顺序排列，其余时间按随机顺序排列。目标是让建立在 CLS token 上的分类头来判断 A 和 B 是否是连续的。这就是它所做的全部工作。这个假设是它也有助于学习一些有用的嵌入。

现在我要介绍论文中使用的表示法。我推荐阅读这篇论文，它读起来非常不错。与"Attention is all you need"一起，我认为这是一篇里程碑式的论文，大概有 17 万次引用，非常令人印象深刻。在原始 Transformer 论文中我们称为 N 的现在叫做 L，之前称为 D_model 的是我们嵌入的维度 H，A 是注意力头的数量，之前称为小 h。这里我只是展示这些新符号以供参考，给你们一个映射关系，当然在 Affin 中我们会与原始符号保持一致。

一个有趣的点是，当你查看一些模型仓库（如 Hugging Face）时，你会看到 BERT 模型通常有几个版本，有时你会看到 cased 和 uncased。这表示对数据进行了什么样的预处理，是否只有小写单词，还是大小写有区别。根据你感兴趣的任务，这可能是你需要选择的。

我从论文中给出一些数量级，关于每个参数选择的值。如果我没记错的话，原始 Transformer 有 12 个堆叠的编码器和解码器。这里的一些数字是从那里获取的，数量级是 1 亿参数。

到目前为止有任何问题吗？

现在我们要讨论 Fine-tuning 阶段，目标是基本上采用我们在预训练阶段学到的内容，然后冻结那些权重，而不是在相同权重上重新训练，你有一些分类线性层放在 CLS token 上或者放在感兴趣的 token 上。你将学习那里的线性嵌入。你将有一些分类任务。你可以冻结所有这些预训练权重，只训练这些小权重，或者你可以重新训练整个网络。我认为有多种分类方案，其中一些会受到你重新训练大部分网络的意愿以及分类任务与原始预训练任务的不同程度的影响。

为了给出一些可能是 Fine-tuning 任务的例子，你可以进行情感提取，在 CLS token 上构建分类层，我们还给出其他例子，比如问答，基本上给你一些输入，模型的目标是检测响应的开始和结束范围。这基本上是一个在 token 级别的目标函数。

现在我建议我们深入研究一个具体例子，我们最喜欢的例子："这只泰迪熊太可爱了。"让我们看看 BERT 在实践中是如何工作的。

基本上在 uncased 设置中你要做的是取句子，进行预处理，把所有内容都变成小写，然后对你的 tokenizer 学到的任何 tokenization 机制应用 word piece 算法。例如这里显然它有所有这些合并规则。这是出现在词汇表中的 token。

然后正如我承诺的，我们在开头添加 CLS token，然后添加 SEP token。你还有这些 pad token，基本上用于填充序列直到结尾，因为当你训练时，你按批次训练，批次由具有固定长度的矩阵组成。

让我们深入研究一下，就像我们上次在 Transformer 深入研究中所做的那样。你有学习到的嵌入，就像一个巨大的查找表，在每个 token 的索引和学习表示之间。你向其中添加 token 的位置嵌入，以及这里新的东西——段落嵌入。正如我们所说，这是添加到每个 token 的嵌入，相同的嵌入被添加到相同的 token。所以同一段落 A 的所有 token 都添加相同的嵌入，然后其他嵌入 B 被添加到段落 B 的所有 token。

现在你有了类似于 Transformer 的东西，它是位置感知和上下文感知的，嗯不是上下文感知还没有，但是这里是段落感知的。

它经过这个编码器架构，然后在情感提取的情况下，我们不关心对应于每个 token 的嵌入，那不是 CLS token，因为我们关心的是 CLS token 的输出嵌入，我们在那里插入一个线性层，它将学习一些分类任务。

这里有什么问题吗？

我们丢弃除了 CLS token 之外的所有输出嵌入，这有意义吗？

是的。问题是这个 FFN 对应什么？基本上你有一个从输出嵌入维度到你感兴趣任务的映射。所以它可能是正面或负面的分类。你有一些具有某种长度的隐藏层。你通常有两个矩阵来学习从一个到隐藏层的投影，然后从隐藏层到输出，然后你学习这些权重以便基于这里学到的嵌入来执行分类任务。

很好的问题。这是我一直在等待的问题。问题是为什么我们要扔掉所有这些其他输出嵌入，因为在分类任务中我们不需要它们。我们按照惯例在某个 token（如 CLS token）上操作，这个 token 的魔力在于编码器中的所有这些自注意力机制已经将每个其他 token 的表示混合到该表示中，使得输出嵌入是上下文感知的。

所以基本上这是一个面向分类的嵌入，这就是我们说要插入到线性层中的内容。当我说我们扔掉所有其他嵌入时，这实际上是我们在分类情况下所做的，但如果我们在 token 级别进行分类，那么每个都可能被使用。例如当我说问答时，如果你想检测给定 token 是否是答案的开始或结束，你通常会有两个 FFN，分别预测答案的开始和结束，你会将它们应用于每个这些嵌入。

还有其他问题吗？

问题是 CLS 的 query、key 和 value 是什么？它将与所有其他 token 经历相同的过程。所以你在这里为 CLS token 的嵌入学习一个表示。它被投影到 query，被投影到 key，被投影到 value。它进行所有注意力计算，最后你得到这个基本上关注到所有其他 token 的嵌入。所以答案是它与其他 token 相同。只需将其视为一个 token，可以是任何 token，都是相同的。

我看到我们还有五分钟，我要快速浏览结尾部分。BERT 的一个伟大之处，ELMo 也是如此，就是你有上下文化的嵌入。在这里你可以看到，基于这些学习到的嵌入学习你想要的任何分类任务是非常容易的。这种灵活性在这里受到了极大赞赏，在工业界被广泛使用。

任何涉及情感检测或其他分类相关任务的东西，现在使用 BERT 类模型都很常见。现在我要谈论它的局限性。正如你在原论文中看到的，我认为上下文长度是 512 大小。在这类早期论文中通常受到限制，提到了一些技术来进一步增长这个上下文大小，而不会让复杂度完全失控。

然后你有一些近似方法，你计算局部注意力等等，使用这些技巧帮助你在保持合理计算要求范围内的同时增长上下文大小。我要谈论另外两个局限性，我们将看到其他模型试图解决的问题。

一个是延迟可能被视为很高。对于 BERT base 你仍然有 1.1 亿参数。这是相当多的，有没有办法让这个更小更快？第二个基本上是你有 MLM 和 NSP 这两个目标函数。这两个真的有用吗？有没有办法简化预训练过程？

首先关于第二个我们有的限制，基本上对成本的敏感性。这里有人听说过蒸馏吗？很好，有几个人。我想引用 Hinton、Vinyals 和 Jeff Dean 的这句话，我认为非常有信息量，是看待给定模型输出分布实际上对了解它学到了什么非常有帮助的良好心态。软目标包含几乎所有知识。这是他们包含这些引用的一些讲座，我觉得基本上他们是蒸馏概念的起源，你在实践中很快看到学习模型的分布比直接学习硬标签更有帮助。

所以你有教师和学生模型的概念，基本上蒸馏的目标是将较小模型的输出分布直接映射到你更复杂的模型，而不是硬标签。

### (1:45:00 - End) Part 8

你用来最小化这个的目标函数是KL散度，它基本上说在由教师T描述的世界中，用学生S来建模有多糟糕。所以它基本上试图评估学生分布与教师分布T的接近程度。一个有趣的注意点是，如果你的yt分布只是一个硬标签，你就会发现交叉熵损失。如果你在一个位置只有一个1，其他地方都是0，你就有负对数ys，这是非常有趣的。

然后当蒸馏后的BERT变体完成时，顺便说一下，这是一篇非常简洁的论文，只有四页但影响巨大。它表明如果你将层数减少两层，你会获得很大的收益并几乎保持相同的性能，这是非常显著的，他们基本上使用蒸馏作为保持这种性能的方式。所以这一直是减少层数的关键方法。


---

*生成时间: 2026-01-03 16:30:27*
*由 YouTube Monitor & Translator (Claude CLI) 生成*