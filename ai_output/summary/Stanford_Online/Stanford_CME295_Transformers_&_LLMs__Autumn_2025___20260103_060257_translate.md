# Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 1 - Transformer

## 📹 视频信息

- **频道**: Stanford Online
- **发布日期**: 2025-10-17
- **时长**: 1:41:56
- **原始链接**: [https://www.youtube.com/watch?v=Ub3GoFaUcds](https://www.youtube.com/watch?v=Ub3GoFaUcds)

---

> 本文内容整理自斯坦福大学教授阿费因（Afin）和舍尔文（Shervin）在 Stanford Online 频道的《Transformers & LLMs》课程第一讲。

## TL;DR

斯坦福 CME295 课程深度讲解 Transformer 架构：从 NLP 基础到注意力机制的完整演进，展示了如何解决 RNN 长程依赖问题，最终构建现代 LLM 的基石。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-09:42 | 课程介绍与背景 | 介绍授课团队、课程目标和NLP发展历程 |
| 09:42-22:35 | NLP任务分类与历史演进 | 详解分类、多分类、生成三大NLP任务类型及评价指标 |
| 22:35-30:21 | 文本分词技术详解 | 对比词级、子词级、字符级分词的优缺点与适用场景 |
| 30:21-53:19 | 词向量表示学习 | 从one-hot到Word2Vec的词嵌入演进过程 |
| 53:19-59:29 | RNN序列建模机制 | 介绍循环神经网络处理序列的核心思想与应用 |
| 59:29-67:08 | RNN的局限性分析 | 深入分析梯度消失问题与长程依赖挑战 |
| 67:08-79:41 | 注意力机制原理 | 详解注意力机制如何解决长程依赖问题 |
| 79:41-89:34 | Transformer整体架构 | 完整介绍编码器-解码器架构与核心组件 |
| 89:34-101:46 | 端到端实例演示 | 完整演示Transformer处理文本的每个步骤 |

## 📊 核心论点

#### 1. NLP任务的三大范式定义了模型设计需求

- **核心内容**：NLP任务可归纳为分类（文本→标签）、多分类（文本→多标签）、生成（文本→文本）三大类。分类任务如情感分析使用准确率、精确率、召回率、F1分数评估；多分类任务如命名实体识别在token或实体级别评估；生成任务如机器翻译使用BLEU、ROUGE等基于参考的指标，困惑度衡量模型对输出的"惊讶度"。随着LLM发展，评估正从基于参考转向无参考方法。
- **关键概念**：分类范式、命名实体识别、机器翻译、BLEU/ROUGE指标、困惑度、无参考评估
- **实际意义**：为模型选择和评估提供框架；指导架构设计从单一输出到序列生成；推动评估方法从传统指标向LLM时代的智能评估演进。

#### 2. 分词策略直接影响模型性能与计算效率

- **核心内容**：词级分词简单但面临OOV（词汇外）问题和形态变化挑战；子词级分词如BPE利用词根共享减少OOV，但增加序列长度；字符级分词完全避免OOV且对拼写错误鲁棒，但序列过长影响计算效率。现代模型多采用子词级，词汇量从英语1万到多语言数十万不等。序列长度直接影响注意力机制的O(n²)复杂度。
- **关键概念**：OOV问题、BPE算法、词根共享、序列长度复杂度、多语言词汇量、计算权衡
- **实际意义**：影响模型训练效率和推理速度；决定多语言支持能力；直接关联现代LLM的上下文长度限制和计算成本。

#### 3. 词向量学习开启了语义表示的新时代

- **核心内容**：传统one-hot编码使所有词向量正交，无法捕获语义相似性。Word2Vec通过CBOW和Skip-gram代理任务学习密集向量表示，揭示了"king-queen = man-woman"等语义关系。核心思想是通过预测上下文或中心词来学习表示，使语义相近的词在向量空间中距离更近。但这种方法忽略了词序和上下文依赖。
- **关键概念**：one-hot编码、余弦相似度、Word2Vec、CBOW/Skip-gram、代理任务、语义关系、上下文无关
- **实际意义**：奠定了现代NLP的表示学习基础；启发了后续上下文相关表示的发展；为预训练语言模型提供了理论先导。

#### 4. RNN引入了序列建模但存在根本性缺陷

- **核心内容**：RNN通过隐藏状态递归处理序列，相比简单平均能保持词序信息。但面临梯度消失问题：反向传播时梯度需要连乘多个小于1的值趋于零，或大于1的值导致梯度爆炸。LSTM通过细胞状态和门控机制部分缓解但未根本解决。RNN的顺序计算特性也限制了并行化效率。长序列处理时隐藏状态成为信息瓶颈。
- **关键概念**：隐藏状态、梯度消失/爆炸、LSTM细胞状态、门控机制、顺序计算瓶颈、长程依赖
- **实际意义**：揭示了序列建模的根本挑战；推动了注意力机制的发明；为Transformer架构的革命性改进提供了动机。

#### 5. 注意力机制通过直连解决长程依赖问题

- **核心内容**：注意力机制允许模型在预测时直接访问输入序列的任意位置，绕过RNN的顺序传播限制。核心是Query-Key-Value机制：查询Q与所有键K计算相似度权重，加权求和对应值V。这创造了从任意位置到任意位置的直接连接路径，彻底解决了梯度消失问题。2014年首次用于机器翻译，为2017年Transformer论文"Attention Is All You Need"奠定基础。
- **关键概念**：直接连接、Query-Key-Value、注意力权重、加权求和、梯度直达、并行计算
- **实际意义**：成为现代LLM的核心机制；实现了高效并行训练；开启了预训练大模型时代。

#### 6. Self-Attention实现了真正的上下文相关表示

- **核心内容**：Self-Attention让每个token同时关注序列中的所有其他token，学习上下文相关的动态表示。不同于Word2Vec的静态嵌入，同一词在不同上下文中有不同表示（如"bank"在"river bank"vs"rob a bank"中）。通过矩阵运算实现高效并行，复杂度为O(n²·d)，其中n是序列长度，d是隐藏维度。现代GPU针对矩阵运算优化使其特别高效。
- **关键概念**：上下文相关表示、动态嵌入、词义消歧、矩阵并行运算、GPU优化、O(n²)复杂度
- **实际意义**：解决了词汇歧义问题；提供了可扩展的并行训练方案；成为所有Transformer变体的核心组件。

#### 7. Multi-Head Attention增强了模型的表示学习能力

- **核心内容**：多头注意力并行运行h个注意力头，每个头使用不同的Q、K、V投影矩阵学习不同的表示子空间。类似CNN中的多个滤波器，允许模型同时捕获多种语义关系和句法模式。不同头可能专注于不同的语言现象：语法关系、语义相似性、长短程依赖等。最后通过连接和线性投影整合多头输出，保持维度不变。
- **关键概念**：多表示子空间、并行注意力头、投影矩阵、连接操作、维度保持、表示多样性
- **实际意义**：显著提升模型表达能力；提供可解释性分析入口；成为所有大型Transformer模型的标准配置。

#### 8. Transformer编码器-解码器架构统一了NLP任务

- **核心内容**：编码器通过多层Self-Attention和前馈网络将输入序列编码为上下文相关表示。解码器包含三种注意力：Masked Self-Attention仅关注已生成token，Cross-Attention连接编码器输出，普通Self-Attention整合当前状态。位置编码通过正弦/余弦函数注入序列位置信息。最终通过线性投影和softmax输出词汇表上的概率分布。
- **关键概念**：编码器-解码器、Masked Attention、Cross-Attention、位置编码、概率分布输出、多层架构
- **实际意义**：提供了统一的序列到序列框架；支持从翻译到对话的广泛应用；为现代生成式AI奠定架构基础。

#### 9. 标签平滑技术提升了生成任务的性能

- **核心内容**：传统训练使用one-hot目标（概率为1的正确token，其他为0），但NLP生成任务通常有多个合理答案。标签平滑将目标概率修改为1-ε给正确答案，ε/(V-1)给其他词汇，其中ε是平滑参数，V是词汇量。这使模型对预测更不确定，避免过度自信，在BLEU等指标上表现更好。反映了语言生成的内在不确定性。
- **关键概念**：标签平滑、预测不确定性、多答案合理性、过度自信问题、BLEU改善、生成不确定性
- **实际意义**：提升翻译质量评估分数；减少模型校准问题；为现代生成模型的训练技术奠定基础。

#### 10. 矩阵化计算实现了Transformer的高效训练

- **核心内容**：Transformer的所有计算都可表达为矩阵运算：Q=XW_Q, K=XW_K, V=XW_V，注意力计算为softmax(QK^T/√d_k)V。输入矩阵X维度为[seq_len, d_model]，投影后Q、K、V维度为[seq_len, d_k/d_v]。QK^T产生[seq_len, seq_len]的注意力分数矩阵，每行是一个query对所有key的权重分布。缩放因子√d_k防止内积过大导致softmax饱和。
- **关键概念**：矩阵化运算、维度变换、注意力分数矩阵、缩放因子、softmax饱和、GPU加速
- **实际意义**：实现大规模并行训练；充分利用现代硬件加速；为大模型扩展奠定计算基础。

## 🔬 提及的技术/方法/论文

| 技术/论文 | 讨论语境 | 重要性 |
|----------|----------|--------|
| Attention Is All You Need (2017) | Transformer架构的开创性论文 | ⭐⭐⭐ |
| Word2Vec (2013) | 词向量表示学习的里程碑 | ⭐⭐⭐ |
| LSTM长短期记忆网络 | 解决RNN梯度消失的早期尝试 | ⭐⭐ |
| BLEU/ROUGE评估指标 | 机器翻译质量评估标准 | ⭐⭐ |
| BPE/子词分词 | 现代分词技术的主流方案 | ⭐⭐ |
| 标签平滑技术 | 提升生成任务性能的训练技巧 | ⭐ |
| 位置编码 | 为Transformer注入序列位置信息 | ⭐ |

## 💬 经典金句

> "注意力机制的idea是在你试图预测什么和过去的事物之间建立直接联系。"
> — Afin

> "Transformer作者试图摆脱这种序列化的文本处理方式，让模型与文本的所有部分同时建立直接连接。"
> — Afin

> "如果你有一个知道如何预测下一个词的模型，那么它对语言运作方式有了一定理解，这正是你想要的。"
> — Afin

> "梯度需要通过时间反向传播...如果你将许多小于1的量相乘，结果就趋向于零。"
> — Afin

## 👤 主要人物

#### Afin（阿费因）

**身份**：Netflix高级机器学习工程师；CME295课程联合授课教师
**背景**：法国École Polytechnique (X) 学士，MIT研究生，曾在Uber、Google工作，专注NLP和LLM领域
**核心观点**：强调从基础概念到实际应用的系统性学习路径，认为理解底层机制对掌握LLM至关重要。注重将复杂理论用直观例子解释清楚。

#### Shervin（舍尔文）

**身份**：Netflix高级机器学习工程师；CME295课程联合授课教师  
**背景**：法国École Polytechnique (X) 学士，Stanford ICME硕士，与Afin有相似的工业经历（Uber→Google→Netflix），专注大语言模型
**核心观点**：强调矩阵化计算和工程实现的重要性，善于将抽象的数学概念转化为可执行的代码逻辑。注重Transformer架构每个组件的具体实现细节。

## 📺 视频类型判断

**教程示范**：系统性的技术教学，从基础概念逐步构建到复杂架构的完整课程

---

## 📝 完整翻译

### (0:00 - 9:42) 课程介绍与背景
> 介绍授课团队、课程目标和NLP发展历程

大家好，欢迎来到 CME 295 课程——Transformers 与大语言模型。我叫 Afin，将和后面的 Shervin 一起教授这门课。在开始之前，让我简单介绍一下我们自己。

我们是双胞胎兄弟，背景很相似。我们都曾在法国的 École Normale Supérieure 就读，之后各自发展。我去了 MIT，Shervin 去了 Stanford 读 ICME 硕士项目。

毕业后，我们的行业背景也很相似。我先加入了 Uber，然后 Shervin 也来到 Uber。接着 Shervin 离开去了 Google，我也去了 Google。最近我加入了 Netflix，Shervin 也加入了 Netflix，我们一直在从事大语言模型方面的工作。所以我们都有技术背景，主要专注于 LLM 领域。

为什么要开设这门课呢？自 2020 年以来，Shervin 和我一直专注于 NLP 领域。我们每年以研讨会的形式开设这门课程，从 2021、2022、2023 到 2024 年。ChatGPT 在 2022 年发布后，突然引起了人们对 LLM 的巨大兴趣。

去年春天，我们开始将这门课程作为 Stanford 正式课程提供，现在叫做 CME 295，这是第二次开设。

那么这门课你能学到什么呢？首先，LLM 现在无处不在，我们的目标有两方面。第一是学习让这一切工作的底层机制，我们将学习 Transformer——这是让所有这些工作的基础架构。第二是了解这些 LLM 是如何训练的以及它们的应用领域。

如果你还在考虑这门课是否适合你，我想说这门课非常适合对这个领域感兴趣的人，无论是因为你想将其作为职业目标——比如想成为研究科学家或 ML 科学家，或者你想开发依赖 LLM 的个人项目，了解其优缺点，什么可行什么不可行。

或者即使你来自其他领域，只是想了解整个 AI 生成式 LLM 的工作原理以及如何将其应用到你的领域。

关于先修要求，最基本的是你应该有一些 ML 基础，比如知道模型是如何训练的，什么是神经网络，以及一些线性代数基础——比如矩阵乘法。但即使你在这些领域的能力还在发展中也没关系，我们会帮助你的。

关于课程安排：这门课将在每周五下午 3:30 到 5:20 在这里举行。这门课是两个学分，你可以选择计分或不计分。

如你所见，我们正在录制这门课。如果你因某种原因无法参加这个时间段，我们会确保在每周五晚上或周六提供录像。

关于成绩，这学期我们设置了两次考试。一次是期中考试，将在第五次课时举行，也就是 10 月 24 日。第二次是期末考试，将在 12 月 8 日那一周举行，具体日期待定。

每次课后，我们会在网站上发布幻灯片和录像。如果你感兴趣，我们也有教学大纲，你可以了解我们将要讨论的主题。

课程教材是这本《Transformers and LLMs Study Guide》，这里有一本，你可以看看。这门课的很多概念实际上都在这本书里，所以这是很好的学习资料。

我们还制作了这门课的简化版本，叫做"VIP 速查表"，在 GitHub 上可以找到。我们还将其翻译成了多种语言。如果你的语言不在其中，请告诉我们，我们很乐意一起翻译。

关于公告，我们会在 Canvas 上发布信息。如果你有任何问题，当然可以联系我们。Canvas 上也有一个叫 ED 的标签页，我相信你们都熟悉。只需点击那里发布问题，Shervin 和我会回复。

要联系我们，可以使用邮件列表或直接联系我们——我们就两个人。

关于后勤安排，有什么问题吗？我忘记提到的一点是，考虑到我们在录制这门课，如果你提问，观众可能不太清楚你的问题是什么。所以我会努力重复你的问题，这听起来可能很奇怪，但我会尽量不忘记。

**学生：** 考试中有编程部分吗？

**Afin：** 问题是考试中是否有编程部分。答案是没有。考试将纯粹专注于我们在课堂上学到的概念，实际上不是为了难倒你们。如果你跟上课程，看幻灯片和我们讲到的概念，应该没问题。

### (9:42 - 22:35) NLP任务分类与历史演进
> 详解分类、多分类、生成三大NLP任务类型及评价指标

**学生：** 如果在候补名单上怎么办？

**Afin：** 根据经验，很多人会最终确定他们的课程安排，有些人会退课。如果你仍在候补名单上，来找我们谈谈，但我很有信心会没问题的，因为现在候补名单大约只有六个人。

**学生：** 幻灯片在哪里？

**Afin：** 它们会在网站上，我们也会确保在 Canvas 上发布链接。

**学生：** 考试的权重分配？

**Afin：** 没有作业。期中考试 50%，期末考试 50%。如果这个时间段与其他安排冲突，请记住我们在录制，所以如果你无法参加某次课程也没关系。

**学生：** 期末考试是否只涉及下半学期的内容？

**Afin：** 我们还没有写考试，但我们正在考虑这个问题。期末考试可能主要关于下半学期的主题。

总之，期中考试 50%，期末考试 50%，这是一门有趣的课程。

现在让我开始正式上课。我想提到的另一件事是，每当我们谈论某个内容时，你会看到幻灯片底部有一个来源。这主要是为了注明我们引用的内容，也为了让你在感兴趣时能更深入地研究这些材料。因为我们每周只有两小时，总共只有九到十周，根本没有足够的时间覆盖所有内容。

第二个免责声明是，你会发现这个领域充满了缩写。当我刚开始时，我完全被这些缩写吓到了。但希望到课程结束时，你会对这些缩写的含义有一个心理映射。如果你在课程结束时有了这种心理映射，那我们就做得不错了。

现在让我们开始，我们将从非常高的层次开始，因为我假设我们从零开始，我们将讨论一般的 NLP。NLP 将是我们的第一个缩写。NLP 代表自然语言处理（Natural Language Processing），这是一个围绕操作文本——用文本进行计算的领域。

在很高的层次上，我们基本上可以将 NLP 任务分为三个类别。第一个类别是我们称之为分类的任务。我们有一个输入文本作为输入，然后我们想要预测某些东西。例如，你有一个电影评论，你想预测情感是积极的、消极的还是中性的。这是一个例子。你也可以进行意图检测——知道比如这个人想做什么。假设你说"我想为明天创建一个闹钟"，那么这里的意图就是创建闹钟。

### (15:00 - 30:00) Part 2

评估分类任务时，通常使用传统的分类指标。你有准确率（accuracy），即你正确预测的观察值的百分比。但还有两个关键指标我想提醒一下，我不确定是否每个人都了解。第一个是精确率（precision），即在你做出的所有正面预测中，有多少是正确的。第二个是召回率（recall），即在所有真实标签中，你正确预测为正面的有多少？还有一个叫做 F1 分数的指标，它基本上取精确率和召回率的调和平均数，给你一个单一的数字。

你可能会想，为什么需要所有这些指标？简短的答案是，有时你的任务和数据集中的类别非常不平衡。比如你的数据集中 99% 都是正面标签，只有 1% 是负面标签。在这种情况下，如果你采用准确率这样的指标，结果会非常误导，因为如果你有一个模型总是预测多数类别，那么你会有一个看起来很棒的分类器，但实际上并非如此。这就是为什么精确率和召回率真正发挥作用的原因。

现在让我们转到 NLP 任务的第二个类别，这是多分类类别。你有一个输入文本，需要预测多个东西，我们用命名实体识别（NER）任务来说明，正如我提到的，这是关于识别给定单词的类别。比如在这里，我们想要识别 "teddy bear" 作为一个实体。我想对于这种情况，你会使用分类指标，但不是在句子级别，而是在 token 级别或实体类型级别。我的意思是，假设你有一个类别，比如位置，你想知道你在预测该类别中的单词方面表现如何。你通常会根据这个来聚合这些指标。

让我们来看最后一个类别，正如我提到的，这是最受欢迎的一个：文本输入，文本输出。我用机器翻译任务来说明这一点，机器翻译是将文本从源语言翻译到目标语言。这里有英语到法语的例子，"cute teddy bear is reading"。对于这类任务，获取数据集比较困难，因为你需要成对的文本。有一个非常流行的数据集叫做 WMT，代表机器翻译研讨会。它包含不同语言的大量配对序列，比如来自欧洲议会数据集的英语-法语、英语-德语。

评估模型性能实际上要复杂得多，因为你可以想象，翻译某样东西有很多不同的方式。我相信我们在座的很多人都会说双语或三语。这就是使它变得如此困难的原因。过去，人们使用了几个基于规则的指标来做到这一点。你可能听说过的一个是 BLEU，BLEU 代表双语评估研究，它衡量你的翻译相对于参考文本的表现如何。

ROUGE 也是如此，它实际上是一套指标，但以不同的方式捕捉这一点。你会发现机器学习社区很有趣，因为 BLEU 我不确定你是否知道法语中的意思是蓝色，而 ROUGE 意思是红色。我想他们试图在其中添加一些乐趣。但这些指标的问题是你总是需要参考文本，你基本上需要标签。而在实践中，获得标签成本很高，需要大量时间和金钱来获得标签。我们将在课程后面看到，随着我们在语言模型空间取得的进展，实际上可以放弃这些基于参考的指标，转向更多无参考的指标。

### (22:35 - 30:21) 文本分词技术详解
> 对比词级、子词级、字符级分词的优缺点与适用场景

我要说的最后一个指标叫做困惑度（perplexity）。困惑度只看模型输出的概率，它基本上量化模型对其输出的惊讶程度。对于 BLEU 和 ROUGE，越高越好；对于困惑度，越低越好。

我想 LLM 自 2022 年以来一直是热门话题，但实际上这个领域可以追溯到那一年之前很久。在 80 年代，我们马上会看到，有一类模型实际上在那时就已经被构想出来了。90 年代我们有了 LSTM，我们也会很快看到。但那个时候的问题是我们没有互联网，没有大量的计算资源，我想这是阻止这些模型——今天的模型——被训练的限制因素之一。

最近，我们有了几个进展。Word2vec 真的是在计算有意义的嵌入设置方面的开创性工作之一，我们马上会看到。当然，我们有了 Transformer，它们是 2017 年发表的一篇论文的一部分，基本上是你今天看到的模型的基础。然后这些模型在计算和训练数据方面都得到了扩展。这就是 LLM 被称为这个名字的原因，我想这些更像是 2020 年代的事情。

我想我要问自己的第一个问题是：我们想要的是有一个处理文本的模型。但模型理解数字，它们并不真正理解文本。所以我们需要以某种方式对文本做些什么，使其更可量化，成为模型能理解的东西。如果你看一个句子，比如 "a cute teddy bear is reading"，你首先需要问自己如何切分这个句子以将其传递给模型。

这部分叫做分词（tokenization），它涉及的基本是根据一些任意的文本单位来切分文本。有几种方法可以做到这一点。我想第一种方法是完全任意地做。所以在这里，比如你会有 "a" 作为一个文本单位，"cute" 会是另一个文本单位，"bear" 会是另一个，等等。顺便说一下，这个文本单位叫做 token（词元），这就是为什么这个方法叫做分词。

另一种方法是简单地按单词分隔。但我想我们总是有利有弊。我想我们想要实现的目标之一是能够以有意义的方式表示这些 token。所以，在单词级别进行分词的一个缺点是，你最终会得到看起来相似但实际上被认为是不同 token 的单词。我想这里的限制是你需要为这些相似但不同的 token 计算嵌入，并以某种方式使它们的嵌入相似。我给你一个例子，假设我有单词 "bear"，然后你有另一个单词复数形式 "bears"。这两个单词非常相似，只是一个是单数，另一个是复数。如果我们采用单词级分词，那么我们最终会得到两个不同的实体，它们基本上只是被认为是不同的。"run" 和 "runs" 也是如此，你知道动词的变化。

### (30:00 - 45:00) Part 3

这听起来不错吗？我想这确实是处理文本的基础。但是，这整体上说得通吗？

好的。现在，我们所做的是获取一个输入文本，将其切分成基本上是 token 的部分。为了让我们的模型理解这些 token，我们需要为每一个找到一种表示方式。这就是所谓的词表示，或者更准确地说，应该叫做 token 表示。我们想找到一种表示这些 token 的方法。

### (30:21 - 53:19) 词向量表示学习
> 从one-hot到Word2Vec的词嵌入演进过程

最简单粗暴的方法就是为每个单词或每个 token 分配一个独热向量。比如假设我们有一个包含三个 token 的词汇表：book、soft 和 teddy bears。我们会让 soft 是一个 [1,0,0] 向量，teddy bear 是一个 [0,1,0] 向量，book 是一个 [0,0,1] 向量。

这叫做独热编码（one hot encoding），我们通常会看到这种方法。这确实是表示 token 的一种方式，但人们想要做的基本上是比较这些 token，看哪些与其他的更相似。

人们使用的一个常见相似性度量叫做余弦相似性（cosine similarity）。不知道你们是否听说过。你可以把它想象成看这些向量在 n 维空间中形成什么角度。如果它们指向相同方向，那么它们可能是相似的；如果它们正交，可能是独立的；如果它们完全相反，那么它们可能是对立的。这基本上就是我们想要的思维模型。

问题是，如果你用独热方式表示 token，你最终会得到所有向量都彼此正交。这就是问题所在。

理想情况下，我们希望意思相同或相似的 token 基本上具有高相似性，而不相似的 token——比如关于不同事物的——更加正交或者相似度低。

这里我只是为了说明目的——teddy bears 是 soft 的。所以你希望 teddy bear 和 soft 具有高相似性，而 teddy bear 和 book 相对独立，你希望它们的相似度接近零。这就是你想要的。这就是独热编码的问题，以及你想要的效果。

抱歉。哦，我明白了。问题是为什么你关心范数？余弦相似性实际上是通过范数标准化的。所以它是点积...哦，你的意思是为什么我在这里只放点积而不是...

我明白了。你的问题是为什么我们不关心范数？我想观众知道这个问题。这些度量都是试图捕捉相似性的方法。为什么你不关心范数？这就是人们试图量化这种相似性的方式。我想你需要看你的向量是如何训练的，以及范数是否表明了什么。

我能给你的最好答案是，这是一种度量方式，不是完美的度量。人们也可能使用点积作为度量，但我没有很好的答案给你。

只要你能捕捉到这些向量如何指向，通常你关心的是它们之间的角度。但是，通常你不会真正考虑范数。

有问题吗？还有其他问题吗？

这是个很好的问题。问题是关于词汇表的大小，以及这如何影响在词级、子词级之间的选择，以及这在不同语言之间如何变化。这是个很好的问题。

我想说这首先真的取决于你想要完成的任务。如果你的任务只涉及一种语言，你通常会选择子词分词器（subword tokenizer），因为我们在这里提到的原因。子词是一个很好的折中方案，既能够通过词根识别单词，利用这一点，又能减少遇到未登录词（OOV）风险的情况。

在大小方面，我知道人们尝试了不同的方法。我认为对于英语，你通常会目标设定在数万级别的词汇表大小。但如今的模型是多语言的，也涉及代码，所以你会看到词汇表大小现在有时达到数十万的数量级。

对于中文，你有使用字符的差异，对于拉丁文，是我们都习惯的字母表，但对于其他语言，你有类似的东西，但是用目标语言的字符。所以我会说，对于一种语言，数量级是数万；如果是多语言的，是数十万。这些是你想要瞄准的数量级。

很好的问题。问题是如何获得这些嵌入？这实际上是下一张幻灯片要讲的内容。

很好。既然我们知道独热编码不是表示 token 的好方法，我们想要做的是从数据中学习这些嵌入。

我提到在 2010 年代有一篇论文，我想是 2013 年的，叫做 Word2vec。它如此受欢迎的原因是，他们展示了一种非常直观和可解释的方式来看待这些嵌入。因为他们在说类似这样的话：king 之于 queen，就像某个词之于另一个词；Paris 之于 France，就像 Berlin 之于 Germany。所以基本上有一种方式来理解嵌入的意义。

现在问题是他们是如何做到的？

他们有两种计算这些嵌入的方法。一种叫做连续词袋（continuous bag of words），另一种叫做 skip-gram。但它们都依赖于同一个想法：利用我们拥有的文本，然后尝试基于上下文预测文本中的某些部分。

例如，连续词袋的目标是考虑给定目标词周围的词，你的目标是预测那个目标词。Skip-gram 有点相反，你从一个目标词开始，想要预测它周围的词。

这个任务通常称为代理任务（proxy task）。因为在这个练习中，我们关心的不一定是预测下一个词，至少现在还不是。我们的目标是学习这些词的有意义的表示。

这里的想法是，如果你有一个模型能够以某种方式预测下一个词，那就意味着你的模型对语言的工作原理有一些理解，这基本上就是你想要的。你基本上想要一个能够反映语言特征的嵌入——king 和 queen，或者 Paris 和 France，这种首都关系——你希望这些关联嵌入到表示中。

让我们通过一个非常简单的例子来看看这是什么样的。

在我们的例子中，假设我们的代理任务是预测下一个词。

这里我们使用一个非常基础的神经网络模型，它基本上接收一个大小为 V 的向量，经过一些矩阵乘法和偏置项得到隐藏状态，然后通过另一组乘法得到最终向量。

这基本上是一个非常简单的神经网络。输入大小是 V，隐藏层大小是 D，通常比词汇表小得多。词汇表通常是数万或数十万，而 D 通常是数百，比如 768 就是一个维度的例子。它要小得多。

我们试图通过这个代理任务学习词表示。我们要做的是考虑一个词作为输入，并预测下一个词。

### (45:00 - 1:00:00) Part 4

我们要做的是预测下一个词元。这里有一只泰迪熊。

现在你可以看到，在这个例子中，你的模型预测下一个词是均匀分布的，但你希望以某种方式最大化"泰迪熊"的概率。所以你要对所有词反复进行这个过程。最终，你得到一个学会如何预测下一个词的模型，这基本上就是代理任务。你要做的是提取模型学到的表示，也就是绿色单元。

现在发生的是，每当你有一个词时，你就将其表示为 one-hot 编码表示，然后将其与这些权重相乘，然后得到绿色表示，那就是你的词表示。

这样说得通吗？

很好的问题。问题是关于 V 对应什么，以及为什么只有六个。是的，在这个例子中，我们只有六个可能的词，这基本上就是词汇表大小，只是一个非常玩具化的例子，因为在实践中会有更多词。

我想这也是语言面临的挑战之一。技术上你可以有很多词的变体，这就是为什么如果你采用词级别的方式将文本分割成词元，你最终会得到非常大的词汇表，因为你需要考虑给定词的所有变体。

我想指出的另一件事是，假设你有一个大小为六的词汇表，这是你在训练时看到的六个词。但如果在推理时你遇到一个在训练时没有见过的词会发生什么？答案通常是人们会为所谓的未知词元或词汇表外词元预留一个位置，基本上可以把它想象成一个桶，用来装我们无法识别的所有东西。

所以如果假设在推理时你有一个无法识别的词元，它们都会采用那个表示，也就是未知词元表示。

顺便说一下，这是词级别分词器遇到困难的地方，因为你有更大的机会遇到词汇表外词元。子词级别的机会会更低，而字符级别我想你就没有这个问题了。这回答了你的问题吗？

很好的问题。第一个问题是你怎么知道何时完成？

代理任务的问题在于，当你训练模型时，你的真正目标实际上不是学习——我的意思是在这种情况下学习如何预测下一个词——你的目标是获得有意义的表示。但你可以做的是跟踪你正在追求的代理任务的损失函数，同时考虑到这不一定是你的最终目标。

我想一个非常合理的方法就是等到模型收敛。在这里你要做的是跟踪损失作为——有一个术语叫 epoch，就是模型看到训练集的次数——的函数，你比较这些不同的曲线，当它收敛时，这通常是停止训练过程的好时机。当然这取决于你的下游任务，但这是一种方法。

你的第二个问题，抱歉，你能重复一下第二个问题吗？

很好的问题。问题是你怎么知道生成何时停止？否则它永远不会停止。是的，完全正确。所以你有一些特殊词元。通常你有序列结束词元。所以通常当生成了序列结束词元时，就是它停止的时候。

第二个问题是什么决定了隐藏层的大小？我会说这是一个权衡，因为你希望嵌入足够丰富，能够为你的下游任务提供信息。例如，如果你想获得句子的嵌入，如果你想做一个非常专门化的任务，有很多不同的结果，也许你想要一个真正能捕捉到这一点的向量。所以也许你想要一个更大的向量。

但如果你有一个非常简单的任务，也许较小的向量可能更合理。所以我想隐藏维度的大小也会影响你之后运行的任何东西的复杂性，因为当然如果你有更长的向量，你会有更多的计算。所以你的推理可能会更昂贵等等。

我想有很多因素。总结一下，第一个是你的任务有多复杂。第二个是你对延迟成本等这些东西有多敏感。所以这真的是一个权衡，但你通常会看到数百或数千的嵌入。当然这些模型一直在增长，所以数字可能会改变，但这是你要考虑的数量级。

这确实是经验性的。我想你也可以依赖其他人发现的东西。但是像 768 这样的数字是人们通常采用的。

很好的问题。问题是你如何区分拼写相同但在不同语境中的词？你想得比我快。这基本上是基础知识，我们将要讨论能够解决在句子中将词语境化这些问题的方法。我们一会儿就会看到。

我时间不够了，所以我会尝试加快进度。好的，现在我们看到了如何学习词元的表示，但我想你可能也想得到句子或文本片段的表示。

用我们之前看到的内容来做这件事的一种非常简单的方法是取词表示的平均值，但问题是你会丢失很多意义，你丢失了顺序，我想这里你很好地指出了，你学到的表示是特定于词元的，无论它们在哪里。

### (53:19 - 59:29) RNN序列建模机制
> 介绍循环神经网络处理序列的核心思想与应用

这就是为什么我们有一类旨在捕捉文本出现的序列性质的模型。我们要谈论 RNN，代表循环神经网络。

RNN 所做的不是一次处理一个词，而是保持句子到目前为止的隐藏表示，并一次考虑一个词元。

如我之前提到的，这项技术实际上是相当早就引入的，在 80 年代。这个模型所做的是考虑词或词元出现的顺序。

在这个例子中，你从句子的最开始开始处理。你有一些虚拟的隐藏状态，通常记作 a 或 h，称为隐藏状态激活，有时甚至称为上下文向量。你有某种模块，它考虑到目前为止的隐藏状态和时间步 t 的词。这里是时间步一。

它在这里做的是接收到目前为止句子的含义，并考虑现在正在发生的词，它产生一个输出向量，这里可以用来尝试预测下一个词。例如，这里我们有这个隐藏状态和这个词的表示，然后你知道在这个蓝色框中你有一些矩阵乘法，你有一个输出向量，你尝试训练来预测下一个词。

然后你通过保持跟踪这些隐藏状态来继续这样做，所以你重复这个过程。我想你解释这些隐藏状态的方式是，它是到目前为止处理的序列的表示。

RNN 的好处是现在词序很重要，你也能够以更自然的方式编码句子。

让我们粗略地看看它是如何工作的。我们有同样的最喜欢的例子："可爱的泰迪熊正在读书。"所以你会有词元 "A"。你找到 one-hot 编码向量。你将它传递给你的网络。你计算隐藏状态。你尝试预测 "cute"。

但然后你跟踪隐藏状态，然后你将其输入另一个模块，然后你还考虑下一个词。所以你不仅考虑词本身，还考虑到目前为止句子的隐藏状态，你再次尝试预测下一个词，一次又一次。

这就是 RNN。

RNN 被用于很多任务，就像直接映射到我们之前看到的类别一样。

对于分类目的，你基本上可以使用句子中最后一个词的隐藏状态。例如，如果你想预测评论的情感，你基本上会取这里的最后一个向量，并尝试将其投影到你想预测的预测或标签空间中。所以例如，如果你想要正面或负面，我们基本上将该向量投影到该空间中。你可以在这里进行多分类。所以你基本上会有感兴趣词元的表示，你会投影它，或者对于生成，你基本上会处理整个源文本，然后在处理结束时有上下文向量，也就是激活向量，也就是隐藏状态，然后将用于解码输出预测。

这就是你如何为这些任务中的每一个使用 RNN。你这些天没有真正听说过 RNN 的原因是因为它们有一些优点但也有很多缺点。其中一个缺点是句子的含义基本上完全封装在这个隐藏状态中。

所以你有长程依赖的问题，这基本上影响了你引用"记住"模型在过去看到的东西的能力，这就是为什么你有另一类试图建立在 RNN 基础上的模型。这个叫做 LSTM，长短期记忆。

### (1:00:00 - 1:15:00) Part 5

### (59:29 - 1:07:08) RNN的局限性分析
> 深入分析梯度消失问题与长程依赖挑战

这个叫做 LSTM，长短期记忆。这个扩展的目标是有一种方法能够跟踪那些引用"重要记住"的事情，在我们之前讨论的隐藏状态之上。所以这里你有 a_t，你知道这是你的激活，基本上是到目前为止编码在其中的序列，然后你有另一个你跟踪的量叫做细胞状态，这里记作 C。

这种架构旨在改进那个部分，但我想它也不完美。

是的，这就是基于 RNN 方法的主要问题，即它们有这种忘记过去内容的问题。在文献中你会看到这种现象被称为梯度消失，之所以这样称呼。我知道我们的时间有点紧，但我要解释这一部分。

为了预测比如最后的单词，你基本上依赖于之前出现的每个隐藏状态。到目前为止还不错。

所以每当你想更新模型的权重来匹配这里的预测和实际预测时，当你做反向传播时，你需要考虑到这里的损失值基本上不仅仅是这个计算的问题，也是这个计算或这个计算的问题，它们基本上以顺序方式发生。

所以你有这种现象，试图通过时间反向传播。但问题是，在实践中当你把它写下来时。这是一个非常丑陋的公式。但当你写下来时，它最终是一堆量的乘积，如果它大于1，那么它就爆炸。如果它小于1，它就消失，因为如果你乘以很多小于1的东西，它就趋向于零。

我想如果你有一些你试图更新的东西趋向于零，你基本上就很难做更新了。这是一个高层直觉。这不是这门课的重点，这就是为什么我不会深入这些丑陋公式的细节。但我希望你能理解，为了记住过去的事情，它做得不是很好，因为这种顺序特性。

这有道理吗？

好的。我希望下一个东西会更有意义一些。但在此之前我只是回顾一下我们看到的内容。

我们的目标是表示文本。我们首先开始表示单词或 token，这是我们试图用 Word2Vec 做的，我们看到这是一个利用代理测试来学习这种表示的好方法，但我们有一些限制，你提到的其中一个是它不知道上下文，而且单词顺序不重要。

所以你有另一类能够考虑单词的方法，但当序列变得很长时，它们在跟踪事情方面有一些困难，你就有了梯度消失或长程依赖的问题。每当你看到这个术语时，它基本上指的就是那个。

另一个我没有提到但计算非常缓慢的事情。当你想训练这些模型时，在训练时为了预测这个单词，你基本上需要计算之前所有的隐藏状态。所以当你的序列变得很长时就变得很耗时。

出于所有这些原因，因为模型在记住过去事情方面有困难，人们尝试在某个东西和过去的东西之间建立更直接的连接。这就是注意力背后的想法。

### (1:07:08 - 1:19:41) 注意力机制原理
> 详解注意力机制如何解决长程依赖问题

注意力做的是试图在我们试图预测的东西和过去的某些东西之间建立直接链接。

在这个例子中，假设我试图将英语句子翻译成法语句子。这里我想输入句子是给定的。我在计算隐藏状态。我一次处理一个单词。这是我的传统 RNN。"一只可爱的泰迪熊在阅读"。这里我有一个隐藏状态，然后我在解码。

你可以想象，当想要生成我的翻译的下一个单词时，如果我知道我试图预测的单词会很好。换句话说，如果我能瞥一眼输入文本的某个区域会很好。

所以注意力背后的想法是在你试图预测的东西和之前的事情之间建立直接链接。这就是注意力背后的想法。它在2014年被引入，是的，这再次试图解决这些长程依赖问题。

这个例子想要做到这一点，这个概念实际上将是这门课的关键，因为我们将看到注意力机制是让一切，我是说大部分事情工作的东西。

这实际上是 Transformer 论文依赖的主要原理。Transformer 是我们将在这门课中看到的核心架构，它在2017年在这篇名为"Attention Is All You Need"的论文中被引入。即使从标题你就能看出作者想要只依赖那个部分。

作者试图做的是摆脱这些顺序处理文本的方式，而是让模型只是与文本的所有部分一次性建立直接连接。这被称为自注意力。他们在翻译任务上尝试了这个，他们意识到这给出了很好的结果。

回到我们仍在使用的例子"一只可爱的泰迪熊在阅读"。我们要说的是，为了计算 token "teddy bear"的表示，我们要一次性查看序列中所有其他 token，并且是直接连接。

我想回到你的问题，这里我们会有一个"teddy bear"的表示，它对于它所在的上下文是唯一的。所以回到你关于"river bank"和"robbing a bank"的问题，这里"bank"会有不同的表示。

这就是想法。这个想法大致有意义吗？这再次被称为自注意力机制。

我的时间怎么样？晚了。晚了。好的很好。所以这就是想法。现在我要介绍另一组想法，这更多是术语，但这将非常重要。

当你想要用别的东西来表达某些东西时，我们使用单词 query（查询）、key（键）和 value（值），Q、K 和 V。在这个例子中，我们的目标是找出查询"teddy bear"与哪些其他 token 更相似。

所以这里的问题是，好的，你有一个查询，你想看看哪些其他 token 最相似，所以你要做的是查看所有其他 token，它们基本上由键和值组成。

你要比较查询和键来量化你的查询与给定键的相似程度，并取对应的值。

我们将在这个例子中看到。假设你想要用其他一切来表达"teddy bear"。你要做的是取查询"teddy bear"，你要将该查询与所有其他键比较，看哪个元素最相似，然后权重更相似的那些并取它们相关的值。

这是一个非常高层的想法，我想这些东西是如何工作的。当然，我们将确切地看到它们如何工作，但这是一般想法。

### (1:15:00 - 1:30:00) Part 6

I'll help you translate this academic video transcript following the specified format and rules. Let me translate this section covering the Transformer architecture and attention mechanisms.

意思是你想要计算每个 token 的表示，作为其他 token 的函数。你通过使用称为注意力层的层来做到这一点。所以是多头注意力层，但多头只是以不同的方式进行这种计算，让模型学习不同的表示或不同的投影。

这里的想法是你将输入你的输入文本，输入文本中的所有 token 都将相互关注。比如，"a cute teddy bear is reading"。你将计算这个文本中所有 token 的表示，基本上作为其他 token 的函数。你将通过编码器来做到这一点。

所以这里通过多头注意力，然后你有一个前馈层，它只是让模型学习另一种投影。你在编码过程结束时将获得的是输入句子中 token 的丰富表示。

到目前为止都很好。但现在你的目标是真正翻译输入句子。所以你要做的是用句子开始 token 开始你的翻译。这是你的第一个 token。你要做的是使用输入句子中的所有表示来弄清楚接下来要预测什么。

所以我刚才说的是交叉注意力层，它是第二个...这个，你知道我不确定你是否看到箭头，但有两个箭头来自编码器，一个箭头来自解码器。有人能告诉我解码器的箭头代表什么吗？它是查询、键还是...我想有三分之一的概率。谁想试试？

学生：是键？

不对。

学生：查询？

对。思考方式是你试图问自己，输入中哪些词是重要的，对吧？所以基本上你想知道，给定你的查询，输入中哪些元素是重要的。所以这里这个箭头确实是查询，因为这是你想要弄清楚的东西，而键和值实际上来自编码器，它们基本上来自输入序列。

然后你有另一个注意力层，就是这个。那个试图弄清楚你正在解码的输出句子的其他哪些 token 对预测下一个 token 有用。

### (1:19:41 - 1:29:34) Transformer整体架构
> 完整介绍编码器-解码器架构与核心组件

假设你开始解码并说"un"，这是法语中的词，要预测下一个词，你基本上想弄清楚到目前为止翻译的哪些 token 对预测下一个词有用。这就是这个注意力层的作用，它被称为掩码注意力，因为它只查看到目前为止翻译的 token。

它不查看未被翻译的 token，因为当然它们没有被翻译。所以在你试图预测的 token 的右侧没有办法看到。

好。所以在很高的层面上，你有这个注意力层，它存在于编码器中，也存在于解码器中，但它有几个用例。所以这里的注意力层旨在从输入句子中计算嵌入，作为它们自己的函数。然后是解码器中的那些。

第一个，掩码自注意力层，旨在将某些东西表达为到目前为止已解码的所有东西的函数。第二个，交叉注意力层，试图将事物表达为在输入中看到的东西的函数。

鉴于你与不同 token 有直接链接，你没有这种顺序感，对吧？因为在 RNN 中，你基本上是一次表达一个东西，所以你对词序有某种感知，但在这里你没有，因为这就像一个直接链接。这就是为什么你有位置编码，它们用来告知词在序列中的位置。我们今天不会深入讨论，但我只是想指出这一点。

所以在非常高的层面上，我们将在详细示例中看到这一点。为了将句子从源语言翻译到目标语言，我们首先要对文本进行 tokenization。所以将其分解为任意单元。我们将为这些 token 学习嵌入。这就是输入嵌入的作用。

然后我们必须添加关于位置的一些编码。我们今天不会讨论，但值得注意。然后我们通过编码器。

编码器试图弄清楚如何将事物表达为输入中其他事物的函数。它在多头注意力层中做到这一点，然后它通过前馈神经网络，这只是一种投影向量的方式，给学习事物更多的自由度。

一旦你有了输入的这些表示，你就要开始翻译。所以你从 BOS token 开始。你试图做的是弄清楚下一个词是什么。所以你将看到，好的，到目前为止翻译的哪些词对翻译有用。这就是掩码多头注意力层的作用。

然后你有另一个注意力层，它是关于将事物表达为输入中内容的函数，这就是那里的交叉注意力层。然后你有一个前馈神经网络，再次给出更多的自由度。

最终，你有一个向量，然后通过 softmax，这只是让你猜测下一个词是什么的方式。所以你将有一个词汇表大小的向量，你将使用这些值来确定你的下一个词是什么。

简单，对吧？对此有任何问题吗？

学生：什么是头（head）的含义？

这是个很好的问题。问题是头意味着什么？

我想我说得太快了。我有点忽略了那部分。但当你进行注意力自注意力计算时，你基本上让查询与键相互作用，然后取相应的值。但没有什么阻止你多次这样做。

头这个术语给出了你用来获得查询、键和值的投影矩阵。当你有多个头时，你所做的是允许你的模型学习不同的投影。

所以这基本上是你的模型学习向量之间不同关联的额外自由度。这是个很好的问题。通常它会被记为小写 h 头数，这就是对应的含义。

这回答了你的问题吗？

很好。我们有很多要讨论的，但我实际上有一张幻灯片。所以，这是你提到的多头。我们基本上并行运行自注意力计算多次，再次使用模型学习的不同投影矩阵。

如果你有计算机视觉背景，这类似于在卷积中有多个滤波器。这是相同的想法，但在这里有所不同。

学生：投影是不同的吗？

很好的问题。问题是投影是否不同？我们通常不约束事物。我们只是让模型学习，但在实践中它只是倾向于学习看同一事物的不同方式。所以是的，通常没有约束。当然你有论文深入研究如果你改变这个会怎样，但通常你没有任何约束。

### (1:30:00 - End) Part 7


---

*生成时间: 2026-01-03 06:00:57*
*由 YouTube Monitor & Translator (Claude CLI) 生成*