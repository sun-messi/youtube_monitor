# Stanford CME295 Transformers & LLMs | Autumn 2025 | Lecture 5 - LLM tuning

## 📹 视频信息

- **频道**: Stanford Online
- **发布日期**: 2025-11-14
- **时长**: 1:47:38
- **原始链接**: [https://www.youtube.com/watch?v=PmW_TMQ3l0I](https://www.youtube.com/watch?v=PmW_TMQ3l0I)

---

> 本文内容整理自斯坦福大学计算机科学教授在 Stanford Online 频道的《Transformers 与大语言模型》课程第 5 讲，主题为 LLM 调优（LLM Tuning）。

## TL;DR

从预训练模型到用户友好的 AI 助手，需要经历监督微调（SFT）和偏好调优两个关键阶段。传统的 RLHF 方法通过奖励模型和强化学习实现人类偏好对齐，但计算复杂度高；新兴的 DPO 方法直接优化偏好数据，提供了更简洁的监督学习替代方案。

## 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-10:20 | 课程回顾与偏好调优概述 | 回顾预训练和SFT步骤，引入第三阶段偏好调优的必要性 |
| 10:20-18:08 | 偏好数据收集方法 | 介绍点式、配对式、列表式三种偏好数据收集策略 |
| 18:08-28:18 | 强化学习基础与RLHF框架 | 将RL概念映射到LLM场景，建立RLHF的理论基础 |
| 28:18-46:00 | 奖励模型训练与Bradley-Terry公式 | 详解如何用配对偏好数据训练奖励模型 |
| 46:00-1:15:40 | PPO算法详解 | 深入分析PPO的clip和KL penalty两种变体 |
| 1:15:40-1:27:04 | Best-of-N推理策略 | 介绍无需RL训练的推理时优化方法 |
| 1:27:04-1:42:58 | DPO：直接偏好优化 | 展示如何绕过奖励模型，直接监督学习偏好对齐 |
| 1:42:58-1:47:38 | 实践对比与课程总结 | 对比RLHF与DPO的优劣，展示泰迪熊示例的改进效果 |

## 📊 核心论点

#### 三阶段训练范式：预训练→监督微调→偏好调优

- **核心内容**：现代LLM训练遵循三阶段流程。预训练阶段在大规模文本数据上学习语言结构（昂贵、计算密集）；监督微调（SFT）阶段用高质量小数据集教模型特定任务行为；偏好调优阶段对齐人类偏好，调整模型的语调、安全性等特征。每个阶段解决不同问题：预训练获得语言能力，SFT获得任务能力，偏好调优获得人类对齐。
- **关键概念**：预训练（Pre-training）、监督微调（SFT）、偏好调优（Preference Tuning）、人类对齐、下一词预测
- **实际意义**：为ChatGPT等商业AI助手提供标准训练流程；确保AI输出既准确又符合人类价值观；推动AI安全研究发展。

#### Bradley-Terry模型：配对偏好的数学基础

- **核心内容**：Bradley-Terry公式将配对比较转换为概率形式：P(yi > yj) = σ(R(i) - R(j))，其中σ是sigmoid函数，R是奖励分数。这个公式的美妙之处在于用配对数据训练点式模型——虽然训练时需要成对样本，但推理时只需单个输入就能输出奖励分数。损失函数采用负对数似然：-E[log σ(R(x,yw) - R(x,yl))]。
- **关键概念**：Bradley-Terry公式、Sigmoid函数、配对学习、点式推理、最大似然估计
- **实际意义**：为所有现代偏好学习方法提供理论基础；简化人工标注工作（比较比评分容易）；被广泛应用于推荐系统、搜索排序等领域。

#### PPO算法的双重约束机制

- **核心内容**：PPO通过clip和KL penalty两种机制防止策略更新过大。PPO-clip使用 min(r(θ)·A, clip(r(θ), 1-ε, 1+ε)·A) 限制策略比率在[1-ε, 1+ε]范围内；KL penalty直接惩罚与参考策略的KL散度。这种设计平衡了两个目标：最大化奖励信号和保持训练稳定性。优势函数（Advantage）比原始奖励更稳定，因为它测量"比期望好多少"而非绝对奖励。
- **关键概念**：策略梯度、PPO-clip、KL散度惩罚、优势函数、价值函数、近端约束
- **实际意义**：成为RLHF的标准算法，被OpenAI、Anthropic等公司广泛采用；为复杂环境下的策略学习提供稳定训练方法；推动强化学习在NLP的应用。

#### 奖励破解（Reward Hacking）现象

- **核心内容**：当模型过度优化不完美的奖励信号时，会出现奖励破解现象——奖励分数提高但实际目标未达成。例如，如果用"掌声音量"衡量讲座质量，过度优化可能导致演讲者只讲笑话而非传授知识。在LLM中，这表现为模型学会"欺骗"奖励模型获得高分，但输出质量实际下降。解决方案包括：限制策略偏离程度、提高奖励模型质量、使用多个评估维度。
- **关键概念**：奖励破解、过拟合、代理目标、古德哈特定律（Goodhart's Law）、评估指标失真
- **实际意义**：提醒AI研究者关注评估指标的局限性；推动更robust的奖励模型设计；影响AI安全和对齐研究方向。

#### Best-of-N：推理时优化策略

- **核心内容**：Best-of-N方法在推理时生成N个候选响应，用奖励模型评分后选择最高分输出。这种方法无需额外训练，直接利用现有SFT模型和奖励模型。优点是实现简单、无训练复杂性；缺点是推理成本呈线性增长（N倍计算量和延迟）。当N增大时，最大延迟趋向分布的右尾，即使并行计算也无法解决延迟问题。适用于对质量要求极高但对成本不敏感的场景。
- **关键概念**：推理时优化、候选采样、奖励评分、延迟分布、计算成本权衡
- **实际意义**：为无法进行RLHF训练的场景提供替代方案；在某些高价值应用中被广泛使用；启发后续推理时优化研究。

#### DPO：直接偏好优化的理论突破

- **核心内容**：DPO通过数学推导将RLHF的两阶段训练转换为单阶段监督学习。关键洞察是：语言模型本身就是隐含的奖励模型。从PPO目标函数出发，推导最优策略π*，再反向求解隐含奖励R = β log(π*/πref) + Z。将此奖励代入Bradley-Terry公式，得到只依赖策略概率的损失函数：-E[log σ(β log(πθ(yw|x)/πref(yw|x)) - β log(πθ(yl|x)/πref(yl|x)))]。
- **关键概念**：直接优化、隐含奖励模型、监督学习、分配函数、策略概率比
- **实际意义**：大幅简化偏好调优流程，降低计算需求；被Llama、Mistral等开源模型广泛采用；启发更多直接优化方法研究。

#### 在线策略vs离线策略训练

- **核心内容**：RLHF采用在线策略（on-policy）训练，模型在每次迭代中生成新样本并基于这些样本更新参数。这与SFT的离线策略（off-policy）不同——SFT使用预先收集的固定数据集。在线策略的优势是能适应模型能力变化，但缺点是训练不稳定、需要探索-利用权衡。探索不足会导致模式崩塌，探索过度会影响训练效率。这种差异解释了为什么RLHF比SFT更复杂、更难调优。
- **关键概念**：在线策略、离线策略、数据分布匹配、探索-利用权衡、训练稳定性
- **实际意义**：解释RLHF训练复杂性的根本原因；指导算法设计和超参数调优；影响分布式训练策略选择。

#### 负信号注入与SFT的局限性

- **核心内容**：传统SFT只能教模型"应该生成什么"，无法教"不应该生成什么"。偏好调优通过配对数据引入负信号——明确告诉模型某些输出是不好的。这种负信号对于安全对齐至关重要。例如，仅用正面示例很难教模型避免有害内容，但配对数据能明确对比好坏响应。DPO的监督学习本质并不意味着失去负信号，因为配对对比仍包含"避免坏响应"的信息。
- **关键概念**：负信号、配对对比、安全对齐、有害内容检测、监督信号密度
- **实际意义**：为AI安全提供重要训练机制；解释为什么仅用SFT难以实现完美对齐；推动对抗性训练和红队测试发展。

#### 超参数敏感性与训练稳定性

- **核心内容**：RLHF涉及众多超参数：PPO的ε（clip范围）、β（KL惩罚系数）、GAE的λ和γ、学习率调度等。这些超参数相互影响，调优困难。相比之下，DPO主要只有一个关键超参数β（通常取0.1左右）。RLHF的训练监控也更复杂——平均奖励并非可靠指标，需要关注策略KL散度、价值函数误差、优势估计方差等多个指标。这种复杂性是RLHF被DPO等方法挑战的重要原因。
- **关键概念**：超参数调优、训练监控、KL散度跟踪、广义优势估计（GAE）、训练稳定性
- **实际意义**：影响实际部署中的算法选择；推动自动化超参数优化研究；为简化训练流程提供动机。

#### 分布偏移：DPO的隐藏挑战

- **核心内容**：DPO的监督学习本质带来分布偏移问题。模型在训练时拟合的是偏好数据集的分布，但推理时面对的是用户查询分布，两者可能存在差异。如果偏好数据不是由当前模型生成，这种偏移更加严重。解决方案包括：在自己的模型输出上收集偏好数据、定期更新偏好数据集、结合SFT损失进行联合训练。尽管有此局限，DPO的简洁性使其在很多场景下仍是优选。
- **关键概念**：分布偏移、域适应、数据分布匹配、联合训练、生成-评估循环
- **实际意义**：提醒在实际应用中注意数据收集策略；推动在线偏好学习研究；影响商业模型的迭代更新策略。

## 🔬 提及的技术/方法/论文

| 技术/论文 | 讨论语境 | 重要性 |
|----------|----------|--------|
| Bradley-Terry Model | 配对偏好数据的概率建模基础 | ⭐⭐⭐ |
| PPO (Proximal Policy Optimization) | RLHF的核心RL算法 | ⭐⭐⭐ |
| DPO (Direct Preference Optimization) | 绕过奖励模型的直接偏好优化 | ⭐⭐⭐ |
| Generalized Advantage Estimation (GAE) | PPO中的优势函数估计方法 | ⭐⭐ |
| LoRA (Low-Rank Adaptation) | 参数高效的微调方法 | ⭐⭐ |
| RewardBench | 奖励模型评估基准 | ⭐ |
| GRPO (Group Relative Policy Optimization) | 推理模型中的新RL算法 | ⭐ |
| DeepSeek Math | GRPO算法的应用案例 | ⭐ |

## 💬 经典金句（3-5 句）

> "Your language model is secretly a reward model."
> — DPO论文标题

> "We want to maximize rewards but not deviate too much from the base model."
> — 关于PPO设计理念

> "If you optimize too much for the volume of clapping at the end, you may start making jokes instead of teaching."
> — 关于奖励破解现象的生动比喻

> "Preference tuning allows us to inject negative signal because SFT only teaches what to predict, not what NOT to predict."
> — 关于负信号的重要性

## 👤 主要人物

#### 主讲教师（斯坦福大学）

**身份**：斯坦福大学计算机科学系教授
**背景**：专注于机器学习和自然语言处理研究，在大语言模型训练和优化方面有深入研究
**核心观点**：强调现代LLM训练的三阶段范式，认为偏好调优是从技术能力向人类对齐转化的关键步骤。深入解析了RLHF和DPO两种主流方法的理论基础和实践权衡。

#### Shervin（客座讲师/助教）

**身份**：课程助教或客座讲师
**背景**：专注于偏好学习和直接优化方法研究
**核心观点**：详细讲解DPO算法的数学推导，强调其简化训练流程的优势。认为DPO虽然在性能上可能略逊于RLHF，但在实用性和易用性方面具有显著优势。

## 📺 视频类型判断

**教程示范**：学术课程讲座，系统性教学内容，包含理论讲解、数学推导和实践应用讨论。

---

## 📝 完整翻译

### (0:00 - 10:20) 课程回顾与偏好调优概述
> 回顾预训练和SFT步骤，引入第三阶段偏好调优的必要性

大家好，欢迎来到 CME 295 第五讲。首先，感谢大家上周抽时间参加期中考试，希望难度对大家来说是合理的。对于旁听的同学，如果有兴趣参加考试，要知道考试题目和答案都已经发布在网站上了，现在你们对考试形式有了一些了解。期末考试会采用相同格式，不过内容会涵盖第五讲到第九讲的内容。

很好。今天我们要讨论 LLM 调优。

像往常一样，我们先回顾一下上次课的内容。上次已经是两周前了，我们讨论了如何训练 LLM。具体来说，我们看了两个重要步骤。第一步叫做预训练（pre-training），基本上是拿一个已初始化的模型，教会模型语言和代码知识。这是一个非常耗时、昂贵、计算密集的步骤，需要在大量数据上进行。我们看到了训练优化技术来实现这一点，包括跨 GPU 并行化技术。我们学习了数据并行方法，特别是 ZeRO 的变体（0、1、2、3），也快速了解了模型并行。

在这一步结束时，你得到的模型了解语言结构、代码以及所有喂给它的文本。但这个模型只能预测下一个 token。它是一个很好的自动补全工具，但还不是一个有用的模型，这就是为什么我们需要第二步。这里通常称为微调（fine-tuning）或 SFT（监督微调）。

在这里，我们拿预训练的模型，针对特定任务进行训练。现在你有 ChatGPT 和所有这些聊天助手，这可以是一个应用场景——将你的模型转换为助手。这里的目标是教会模型如何表现。

模型已经知道什么是语言、什么是代码等等，你只是试图让它按照你想要调优的用例来表现。通常这里的数据集规模小得多，但质量高得多，你基本上是拿预训练模型，通过下一个 token 预测任务教它准确预测哪些 token。

我们学过 LoRA，这是一种参数高效的方法，不调优所有权重，而是巧妙地引入低秩矩阵进行调优。我们就停在那里了。今天要看的是如何将模型与我们称之的人类偏好对齐。

这里我们拿已经为特定任务微调过的模型，试图对齐模型，使其更符合人类喜好或我们定义的某些指标。

举个例子，假设在第二步结束后你有一个助手。很可能你的助手表现符合你的要求，但语调不对。比如不够友好或不够安全。你想在第三步调优这些方面。

这一步叫做偏好调优（preference tuning），我们马上就会看到它是什么。

作为背景，假设我们有一个 SFT 模型，即经过预训练阶段和微调阶段的模型。比如我们可能问模型建议我们和泰迪熊可以做什么新活动。模型回答说："我建议你根本不要花太多时间和泰迪熊在一起。"这是助手的回应，但不一定符合我们的要求。

这里的想法是拿这些所谓的"坏"输出，找到或重写一个我们想要的输出。这一对就是我们所说的偏好对（preference pair）。

换句话说，给定这个 prompt，我们有两个回应。一个是我们想看到的，另一个是我们不想看到的。比如在这个例子中，我们想看到的答案是："当然，泰迪熊不仅是美妙睡眠的绝佳伴侣，也可以是有趣活动的好伙伴"，然后建议一些活动。

设置听起来不错吗？

简而言之，我们想让模型与人类偏好对齐。你可能会问，我们已经有微调步骤了，为什么还要第三步？

### (10:20 - 18:08) 偏好数据收集方法
> 介绍点式、配对式、列表式三种偏好数据收集策略

在第二步微调阶段，如果你记得，我们构建了非常高质量的数据集，包含我们希望模型以某种方式表现的各种 prompt。构建这样的数据集实际上非常耗时且困难，因为数据集必须非常高质量。在偏好调优中，我们不是真正教模型应该生成什么，而是告诉模型应该偏好什么样的输出。

我们不再是"请生成那种东西"，而更像"我偏好这个选项"。

通常，如果我们让你从头写一首好诗，会比给你看两首诗（一首差的一首好的）然后让你说哪首更好要困难得多。所以获取数据集已经容易得多了。

第二个原因是，在 SFT 阶段构建高质量数据集时，我们真正努力做对的一个方面是 prompt 的分布。我的意思是，如果我们有太多某种特定 prompt，我们的模型会更偏向于以那种特定方式回应。

人们试图小心处理 SFT 数据中 prompt 的分布。如果我们的模型表现不当，如果我们考虑在 SFT 数据集中只添加一个例子，我们必须非常小心添加哪个 prompt，以及它是否会让模型过度偏向那个方向。

第三个原因是我提到的——SFT 数据通常质量很高。如果你查看模型的所有错误步骤并试图放入 SFT 数据中，这会很困难，需要很多时间。但要注意的是，如果你的 SFT 数据表现很差，可能是因为 SFT 数据集本身有问题。

偏好调优不是万能的解决方案。有时候最好检查一下 SFT 数据集是否有问题。

### (15:00 - 30:00) Part 2

所以我们可以通过人类评分来比较它们，但我们也可以通过其他一些指标来比较。

我来列举几个。首先是 LLM as a judge（LLM 作为评判者），你们可能听说过，虽然我们还没有见过，但几节课后会讲到。这通常也用来比较一个观察结果相比另一个有多好。

我们也可以使用其他一些指标，基于规则的指标，比如 BLEU、ROUGE 等。尽管现在用得不多，比较这两个观察结果最简单的方法是采用二元设置，你可以说好的，回应一是比回应二更好还是更差。但你也可以考虑更细致的量表，意思是你也可以说好的，回应一是好得多、好一些、稍微好一些、稍微差一些、差一些或者差得多。这也是你可以做的。

但这种方法存在一些挑战，比如如果你考虑人类评分，很多任务都有点主观。所以在很多情况下，人们实际做的是在二元量表上建立成对偏好数据集。只考虑它是更好还是更差。

这听起来合理吗？

好的。获取这种数据的另一种方式是在你的日志中找到一个你不喜欢的回应，拿那个回应重写它，这基本上就是我们这里做的。所以当我们有这里的回应时，我们做的是拿这个回应重写一个好的。

这也是人们做的，但当然这更复杂一些，因为你需要生成，我告诉过你生成是有点昂贵和困难的，但这也是可能的。

数据收集有意义吗？

好的。酷。现在我们有了偏好数据，我们想要的是让我们的模型与评分偏好的回应对齐，并且降低那些不被偏好的回应的权重。

为了做到这一点，我们将看到一个叫做 RLHF 的方法。我们稍后会更详细地看到它。但正如名字所示，RLHF 依赖于 RL（强化学习）。

我先讲一些 RL 基础知识。我们这里有 RL 专家吗？

好的。没有。好的，不需要成为 RL 专家。所以别担心，我们会非常慢地进行。

在 RL 世界中，人们有一个 agent（智能体），这里我指的是 RL 术语中的 agent，它与环境交互。

它做什么呢？它在给定状态下，比如在时间 t。它可以采取行动，在时间 t 采取行动，它根据某些策略采取那个行动。策略通常记为 π，θ 的 π，给定状态的行动。所以这个策略的含义就是给出你在给定状态下采取行动的概率。

容易，对吧？所以给定 agent 采取某些行动，它也会收到一些奖励。有时是好奖励，有时是坏奖励。

我们要做的是利用这种思维方式进行我们的偏好调优练习。

### (18:08 - 28:18) 强化学习基础与RLHF框架
> 将RL概念映射到LLM场景，建立RLHF的理论基础

让我们一起看看如何将这些量转置到 LLM 世界中。

agent 是什么？agent 是 LLM。

就状态而言，它所处的状态就是它到目前为止的输入。

它想要做的行动是预测下一个 token。所以它基本上总是想知道好的，给定这个输入，下一个 token 是什么，这个行动或下一个 token 是在那里的 token 集合中。如果你想，环境可以是你词汇表的 token 集合。

为了决定哪个 token 应该是下一个，LLM 使用下一个 token 的概率来确定，这是当你做前向传播时获得的，当你查看概率分布作为输出时，这基本上是我们的策略。

所以我们这里的策略简单地等于 LLM 的输出，给定输入，以确定下一个 token。

到目前为止都好。现在我们要添加一个额外的东西。我告诉过你，我们正在构建我们的偏好数据集，我们想知道哪个输出比另一个输出更好。我们将把它用作奖励。我们将以某种方式把它用作奖励，我们会看到我们如何使用它。

我要总结一下这部分。我们有 LLM 接受一些输入，然后它想预测下一个 token。它想采取这个行动。为了做到这一点，它使用它输出的概率分布，然后它预测的 token 或它生成的输出然后接收一些奖励，这些奖励将反馈到调优 agent 中。这里是 LLM。

学生：问题是如果我们对所有配对都这样做，这不会很昂贵吗？为什么会很昂贵？

嗯。

是的，问题是在昂贵方面你做什么？所以是的，通常人们采用批次。但我不会...这确实很昂贵，但我们会看到一些数量级以及这如何工作。但你可以把这看作是一种训练程序，可以被视为与其他一些训练程序一样昂贵。没有什么让这更昂贵。

好的，但我们会看到确切的情况。所以我认为你提到的一些点是正确的。与常规的监督微调设置相比，有些部分是添加的，我们马上会看到。

学生：问题是你从这里获得的奖励对 LM 的改变是否足够强大？

你会在网络上看到一些表达，将这种训练程序描述为你知道的，它没有像 SFT 那样多的信号，因为对于 SFT，你实际上总是接受部分输入并让 LLM 学习如何生成下一个 token。但这里你每次完成只得到大约一个信号。所以是的，它绝对更稀疏，这就是为什么你会看到 RLHF 被视为更多的是具有稀疏信号的方法。

学生：很好的问题。问题是，你为每个 token 还是为整个东西应用奖励？我们会更详细地看到这一点，但它是为了整个东西。为整个东西，但我们马上会看到。

学生：那么 AT 和 ST 是什么？

作为提醒，ST 是你所处的状态，AT 是你想要采取的行动。所以在 LLM 的上下文中，LLM 的状态是你目前拥有的输入，然后行动是给定那个输入你想生成哪个 token。

酷。到目前为止都好。

好的，完美。现在我们对基于 LLM 的 RL 的心理模型有了一点了解，我想再次强调我们试图实现的是学习如何将这个策略与奖励对齐。

我们想学习 θ，θ 是我们 LLM 的参数，使得 π of θ 与偏好对齐，这就是 RLHF 发挥作用的地方。

RLHF 代表基于人类反馈的强化学习，它通常由两个阶段组成。第一个阶段是你搞清楚如何区分好输出和坏输出。

这里你收集的所有偏好配对实际上用于你学习什么是好什么是坏。所以这里的输入是 prompt 和回应的连接，输出是一个分数。

你想知道给定一个 prompt 和回应，这有多好。

然后第二步是 RL 步骤，强化学习步骤。这是你使用奖励将你的模型与偏好对齐的地方。所以这里作为输入你有 prompt，你以某种方式想要做的是能够生成更与奖励对齐的 ŷ。

顺便说一下，我想指出一点。RLHF 是基于人类反馈的强化学习。人类反馈部分指的是训练奖励模型的标签。

如果偏好配对基于人类评分，那么我们依赖人类偏好，那么我们就在 RLHF 中，因为你会看到那里也有 RL，比如 RLAIF（基于 AI 反馈的强化学习），那个依赖非人类偏好。

酷。好的，现在我们知道 RLHF 是什么，我们知道有两个步骤，我们自然会经历第一步。这里的想法是我们想构建一个知道哪个输出是好的，哪个输出是坏的模型。当然这里我们想要的是不仅考虑输出，还考虑输入，因为你需要以某种方式将那个答案置于上下文中。

让我们用我们最喜欢的例子。假设你有以下 prompt："建议一个我可以和我的泰迪熊一起做的新活动。"

你想要的是有一个奖励模型，这里我们记为 RM，它告诉你我们重写成好答案的答案是好的，我们想以某种方式有那个模型告诉我们我们构建的输出或我们没有构建的输出，我们看到的输出是坏的。

所以我们想以某种方式有一个模型，它接受 prompt 和好回应并说它是好的，我们想以某种方式有一个模型，它接受 prompt 和坏回应说它是坏的。

现在问题是你如何构建这样一个模型？

为了做到这一点，我们使用一个叫做 Bradley-Terry 公式的表述。

这是一个重要公式。我们在这里停留一会儿。

Bradley-Terry 模型假设每个项目都有一个隐藏的"质量"分数，当两个项目比较时，质量更高的项目获胜的概率遵循逻辑函数。具体来说，如果我们有两个回应 y₁ 和 y₂，它们的质量分别是 r(x, y₁) 和 r(x, y₂)，那么 y₁ 被偏好于 y₂ 的概率是：

$$P(y_1 \succ y_2 | x) = \frac{\exp(r(x, y_1))}{\exp(r(x, y_1)) + \exp(r(x, y_2))}$$

这实际上是一个 sigmoid 函数的形式。我们可以重写为：

$$P(y_1 \succ y_2 | x) = \sigma(r(x, y_1) - r(x, y_2))$$

其中 σ 是 sigmoid 函数。这个公式让我们能够将成对偏好转换为可训练的损失函数。给定我们收集的偏好数据，我们可以训练一个奖励模型 r(x, y) 来最大化观察到的偏好的似然性。

这就是我们如何从人类偏好数据中学习奖励函数的方法，它将成为 RLHF 第二阶段强化学习的基础。

### (30:00 - 45:00) Part 3

### (28:18 - 46:00) 奖励模型训练与Bradley-Terry公式
> 详解如何用配对偏好数据训练奖励模型

我们继续看这个公式的具体含义。它表示输出 yi 比输出 yj 更好的概率等于某个分数的指数函数（关于 i 的分数）除以该分数的指数与另一个分数指数的和（关于 j 的分数）。

这就是所谓的 Bradley Terry 公式，这是我们构建模型时将使用的公式。这里，它也等于 σ(R_A,i - R_j)。谁知道 σ 是什么？

学生：是 sigmoid 函数。

正确！它是 sigmoid 函数。提醒一下，sigmoid 是 1/(1 + exp(-x))。图形看起来是这样的：当 x 趋向负无穷时，函数趋向于 0；当 x 趋向正无穷时，函数趋向于 1。

换句话说，如果输出 i 比 j 更好，我们希望 sigma 函数的输入尽可能高，因为我们希望概率尽可能接近 1。所以你希望当输出 i 好的时候，R_i 要高；当输出 j 不好的时候，R_j 要低。

到目前为止还好理解吧。

在这里，我们想要做的是以某种方式训练一个模型，能够使用这个公式输出分数 R_A,i 和 R_j。这个公式涉及两个量，因为我们有成对的数据集。

训练的思想是这样的：你有一个初始化的模型，一方面输入你的提示词 X 和获胜输出（我称之为获胜输出，即 ŷ_W），将其输入模型，它产生分数 R(X, Y_W)。然后你有第二个输出，即 X 和 Y_L，Y_L 是失败的输出。你将其输入模型，得到第二个分数。

现在你需要某种损失函数来考虑这两个分数。基于这个公式，你们有什么损失函数的建议吗？

学生：成对的...

学生：二元交叉熵。

能详细说明一下吗？

学生：负对数似然...

是的，正确！很好的回答。我想你的答案是对这个量使用某种负对数似然，交叉熵可以看作是这种方法的特例。为了确保我们理解这一部分，让我重新推导一下你提到的公式。

一个想法是，给定我们的数据和我们看到的 Bradley Terry 公式，找到能够最大化该数据发生概率的参数 θ，这基本上就导致了你刚才提到的结果。我这里写出来，确保我们都理解。

假设你有一个偏好数据集，比如获胜样本与失败样本配对。你有一堆这样的配对。假设这些配对彼此独立发生。你想要找到某种方式最大化看到这些样本的概率。

这意味着我们要最大化这 n 个偏好对的乘积，即某个输出 w 比某个输出 l 更好的概率。通过 Bradley Terry 公式，我们得到这个公式，基本上是这个奖励的 σ 函数的乘积，奖励基本上是输入提示和输出的函数。

我在这里只是从第一原理重构损失函数。

每当你看到概率的乘积时，首先要想到的是取对数，因为这可能变得非常小，会导致不稳定。如果你取对数，最大化乘积等同于最大化该乘积的对数。

如果我取对数，它基本上等于获胜奖励减去失败奖励的 σ 函数的对数的和。我们想要最大化这个。

但机器学习中的人们喜欢最小化东西，所以我们在前面加一个负号。最大化对数和等同于最小化负对数和。这就是我们的损失函数。

这有意义吗？这正是你提到的。

通常我们写成期望形式，这就是我们的损失函数。我们的损失函数是负期望的对数 σ(r(x,y_w) - r(x,y_l))。

听起来不错。这里损失函数是成对的，但奖励模型是成对的还是点式的？我想，你需要一对来做预测，还是只需要一个？

学生：你需要一对吗？

这就是美妙之处。你不需要一对。你是以成对方式训练的，但实际上是点式的。

我觉得这是我意识到的一点。看这个损失函数，这很美妙。你以成对方式训练，但最终你有一个奖励模型，它接受一个提示和其输出，只输出一个数字。

我们看到它会尝试为获胜样本输出高分，为失败样本输出低分。这就是我们的奖励模型。看一下时间，我实际上没有按时完成，所以我继续。

通常这里你需要一组数据，通常是数万个甚至更多。这里的标签是偏好评级。如果我们谈论 RLHF，偏好应该来自人类。

在模型方面，你有很多不同的选择。正如我们现在知道的，仅解码器预测下一个 token 的模型非常流行。你完全可以采用这样的 LLM，只需在句子末尾添加一些分类头，然后用它来预测奖励。或者如果你记得，我们在课程开始时也看到了仅编码器模型，比如 BERT，你通常也可以投影 CLS token 的嵌入。这也是一个很好的选择。

通常现在人们采用 LLM 路线，因为现在一切都是语言模型。所以是仅解码器，你只需添加一个分类头。

人们也提出了基准来评估你的表现。这里放一个参考，如果你感兴趣的话，Reward Bench 是一个相当流行的基准。

最终你得到一个奖励模型，它接受一个提示和给定的响应，给你一个分数。

学生：在某些任务中，人类偏好可能是某种东西，但在其他任务中可能是别的？

正确。问题是在某些任务中人类偏好会是某种东西，但在其他任务中可能是其他东西。通常这些奖励是相对于给定维度的。

它们可以是，比如输出是否有用，或者输出是否友好，输出是否安全。这些都是不同的维度，可以有不同的奖励模型。你也可以有一些整体分数。

但是的，你需要定义一个维度来实际量化你的响应有多好。我提到的是你会发现的常见维度。我还要说的另一件事是，人类评级对你暴露的指导方针非常敏感。

我们不会在这里详细讨论，但人类偏好数据的一个重要方面是确保你告诉评估者的指导方针尽可能客观。我的意思是，有时你不能让它们超级客观，但你有责任确保它们足够清晰，这样你的人类偏好就不会太嘈杂，因为它们可能很嘈杂，但这也是一个挑战。

学生：这里的奖励是回归还是分类？

问题是这里的奖励是回归还是分类。让我们看看损失函数。我想这有点难说，因为奖励可以被解释为某种分数，但我们有一个概率公式。

我可能会将其框架为分类任务，因为你有偏好数据——是更好还是更差，所以它像 1 或 0。但最终你使用奖励，所以这不是纯粹的分类任务，因为最终你有这个... 但要注意的一点是，这些奖励所处的尺度通常本身就是缩放的。

在推理时，你有一些归一化程序，在你的批次中归一化，但是的，我可能会更多地将这个公式描述为概率性的。

学生：我们归一化分数吗？

问题是我们是否归一化分数？有很多不同的方法来归一化。我们通常会归一化。如果你在回归设置中，你试图在给定尺度上预测某个分数，但这里你没有这样做。所以这里是自由形式的。

### (45:00 - 1:00:00) Part 4

这里确实会有一些重新缩放。你有问题吗？

学生：能否详细介绍一下奖励模型的输出是什么？

我们稍后会看到，但你可以将好的输出想象为得分为1，坏的输出为-2或-3。基本上是在连续尺度上的分数。我们马上会看到一个例子，希望这样会更清楚。

### (46:00 - 1:15:40) PPO算法详解
> 深入分析PPO的clip和KL penalty两种变体

我们训练了一个奖励模型。现在第二步是使用该奖励模型来对齐我们的模型，这里的模型指的是LLM。经过预训练阶段和SFT阶段的LLM就是我们想要与人类偏好对齐的模型。

我们将使用强化学习来做到这一点，并且将使用刚才构建的奖励模型。这里的奖励模型是我们在第一步中获得的模型，它让我们能够区分好的输出和坏的输出。

这是对齐模型的一般方法。首先，你将提示作为输入。你的LLM将生成一个完成（completion）。这里完成意味着模型的完整响应。顺便说一下，人们也用rollout这个术语来称呼完成。

所以它生成一个完成、一个rollout、一个完整响应，这个完整响应连同提示一起进入奖励模型。正如我们所看到的，奖励模型现在知道这是好还是坏。假设现在它是坏的。在实践中，它不会生成拇指向下的符号，而是生成某种分数，比如-2。

我们考虑这个奖励，然后用这个信息来调整LLM。这比听起来要复杂，我们会看到如何做到这一点，但这就是总体思路。

提醒一下，奖励模型是我们在第一步训练的模型，它是一个冻结的模型。我们不训练奖励模型，奖励模型已经训练过了。我们正在训练的模型是LLM。我认为这是需要注意的重要一点。

我们的目标是优化更高的奖励，但是不要偏离初始模型太远。我想大家都同意要优化更高的奖励，对吧？但我刚才说的第二句话是我们不想偏离初始模型太远。

为什么我们不想过度偏离基础模型呢？

学生：会灾难性地遗忘它所学到的东西。

这是个很好的回答。建议的答案是，你的初始模型中拥有所有这些知识，这些知识是经过预训练然后指令调整或调整的，你不想移动得太远。这正是原因之一。

还有什么其他原因吗？

学生：在不是超级干净的数据上过度拟合。

很好的观点。第二个建议的答案是你可能在不是超级干净的数据上过度拟合。

学生：奖励模型可能是有噪声的。

没错，你们的两个观点基本上是一致的，就是奖励模型可能是有噪声的。这正是重点。这是一个称为奖励黑客（reward hacking）的现象，如果你试图过度优化奖励，你基本上是在假设奖励确切地量化了你想要的东西，但情况可能并非如此，因为奖励模型本身是不完美的。

为了确保每个人都理解这一点，我将用一个例子来说明。假设我正在讲课，我的目标是让讲座尽可能有信息量。

我可以选择一些可以量化的东西来知道我说的是否有信息量。假设我的奖励是讲座结束时掌声有多响亮，鼓掌声音是否很大。

问题是，如果我过度优化讲座结束时的掌声音量，可能会发生这样的情况：如果我发现你们都喜欢笑话，我开始讲笑话，然后你们在最后鼓掌，我的奖励被最大化了，但我的目标没有实现，即让讲座有信息量。

在实践中，这种奖励黑客现象会导致你的模型过度优化一个不完美的指标或分数。你很可能最大化奖励但不能实现你想要的目标。这正是奖励黑客背后的理念。

另一个原因是，第一，你的基础模型已经很棒了，它知道很多东西。第二，因为奖励模型是不完美的，所以你有这种奖励黑客现象。第三个原因也是因为你可能有一些训练不稳定性。

总之，我们有一系列理由说明为什么我们不想偏离太多。

在这种设置中，你通常比奖励模型情况下有更多的观察值。你通常至少有10万个观察值，这里得到的标签来自奖励模型。你从SFT后获得的模型开始训练。

损失函数将试图让你最大化奖励，但也不让你偏离基础模型太多。我们会看到确切的做法，但这就是我们的损失函数的样子。它将有这两个部分。

正如我们看到的，我们不希望它偏离太多，因为奖励黑客和不稳定性等。为了做到这一点，你可能听说过一个非常流行的RL算法叫做PPO。

PPO代表近端策略优化（Proximal Policy Optimization）。它被称为近端的原因是我们不希望我们的模型偏离基础模型太远，我们会看到如何做到这一点。

这大致是我们的损失函数的样子。我们有一个要最大化的奖励模型部分，然后还有一些其他部分，用于测量我们正在训练的策略分布与参考模型或基础模型的距离。这里参考模型就是你的SFT模型。

损失的公式大致有意义吗？

关于KL散度，我不想假设你们知道这是什么。KL散度是某种度量方法。我不想说距离，因为距离有很多含义。它不是距离，但它是衡量两个概率分布有多远的某种度量方法。

你取一个概率分布P和另一个概率分布Q，计算这个公式：$\sum P_i \log \frac{P_i}{Q_i}$，这给你一个数字，表示这两个概率分布有多远。

你想要最小化损失，所以你想要最小化它们之间的距离。

### (1:00:00 - 1:15:00) Part 5

如果你遵循策略生成，它就能预测奖励。这个价值函数在论文中经常出现，人们在做 PPO 时需要训练它，通常是因为人们在计算我上面提到的优势时需要这个价值函数。这个价值函数通常与策略联合训练。

换句话说，你有一个 LLM，它被初始化为 SFT 模型，生成预测。你通常还会有一个价值头，这次不是分类问题，而是回归问题。你试图估计如果继续按照策略生成，最终的奖励是什么。

这通常是你需要与策略联合训练的东西。我故意不详细解释优势是如何确切表述的，这超出了本课程的范围。但如果你感兴趣，我强烈推荐阅读这篇关于广义优势估计方法的数学论文，这通常是人们用于此目的的方法。这是关于使用广义优势估计的高维连续控制的论文。如果你感兴趣，请随意看看。但这通常是人们用来估计这些优势的方法。

现在我们马上会看到这些优势在哪里使用。但从高层次来看，这对每个人都有意义吗？我们在这里试图实现什么？

换句话说，我们想要的是最大化奖励，但不要偏离基础模型太多。但我们实际上想要的是找到一种方法，让奖励相对于你实际期望的更加相对化。

你想要将你获得的奖励与某种平均响应进行比较，想法是如果你得到了很好的奖励，那么在平均情况下奖励会是什么，你仍然想要最大化那个。人们使用这种奖励减去基线的原因是因为它减少了这些估计的方差，通常使训练进行得更快。这就是人们使用这部分的原因。

但为了计算优势，人们使用一种叫做广义优势估计的方法，你可以把它想象成一个带有一些超参数的大公式。这在下面的论文中。只是一般概念。

好的，看看时间。我们将看看 PPO 损失的两种典型变体。第一种叫做 PPO clip。

这里的想法是防止我们模型的更新从一次迭代到下一次过于大。你有这个损失表述，它是比率的最小值（我们马上会看到那是什么）乘以优势，以及该比率在两个边界之间的一些裁剪乘以该优势。

在我们深入细节之前，我想指出一些可能看起来令人困惑的事情，因为它对我来说是令人困惑的。首先，那个公式表达为 $L =$，但实际上它不是我们想要最小化的东西，而是我们想要最大化的东西。

人们通常用 L 表示损失。所以要知道我们想要最大化它，我们不想最小化。第二件事是我们引入了一个叫做 $R(\theta)$ 的项。这不是奖励模型。

通常人们用 R 表示奖励，但这里不是奖励。它实际上是我们策略与前一步策略之间的概率比率，记作 $\pi_{\text{old}}$ 或 $\pi_{\theta_{\text{old}}}$。它量化了你当前阶段的策略与你旧策略的不同程度。

这里当我们说"旧"时，它不是 SFT 模型。它不是 SFT 模型。它是 RL 阶段前一次迭代的模型。

因为这里，我们想要策略不要有过于宽泛的更新。

到目前为止还好吗？

$a$ 是什么？$a$ 是我们的优势。你可以把它想象成这个复杂公式，它是奖励和价值函数的函数，基本上告诉你你的片段有多好。

这个复杂的公式我们马上会看到为什么它有意义。如果优势是正的，意味着我们想要强化生成的任何东西。所以这里我们的目标函数会看起来像这样。如果 $a$ 是正的，那么这个 $L_{\text{clip}}$ 会看起来像某个线性函数，作为 $R$ 的函数，因为它是 $R \times A$。

### (1:15:00 - 1:30:00) Part 6

所以现在你可能会问的问题是，你真的需要所有这些模型才能表现良好吗？

答案是也许不需要。也许需要，但也许不需要。现在有很多其他方法变得更加流行。所以你可能在推理模型的语境中听说过 GRPO。我们将在下周第六讲更详细地研究 GRPO。但只需要知道有许多变体。所以 PPO 只是一个非常流行的算法，即使在 PPO 内部也有许多变体，现在有许多其他 RL 算法正在被使用。

### (1:15:40 - 1:27:04) Best-of-N推理策略
> 介绍无需RL训练的推理时优化方法

如果你想提前了解下一讲，你也可以看看这篇来自 DeepSeek 团队的论文。DeepSeek-Math 介绍了开放语言模型数学推理的极限，该论文引入了 GRPO 算法。但如果你不看也没关系，我们下周会讨论。

我也想谈谈基于 RL 方法的一些挑战。

第一个是你需要有这个两阶段过程。你需要首先训练你的奖励模型，然后需要使用奖励模型来训练你的策略。

但问题是你做了步骤一，做了步骤二，然后你意识到，哦等等，步骤一有问题。那么你需要重新做所有事情。所以有很多依赖关系，这些依赖关系真的会让你的生活变得更困难，训练配方也不是最简单的。这是一个缺点。

第二个缺点是关于你需要调整的超参数数量。我们从 KL 散度公式中看到了 beta，这是一个。我们有来自 PPO 裁剪版本的 epsilon，这是另一个。广义优势估计，我们还没有看到公式，但我想至少有两个我能想到的超参数。

总之，你有很多超参数。

当然，有些超参数比其他超参数表现更好。如果你需要重新训练，那么你需要再次做所有事情。

然后你有一些不稳定性挑战。你知道你可以约束你的迭代不要太大，以某种方式控制这一点，但有时这还不够。

就你可以用来监控训练的指标而言，你会使用什么指标？

我们在某种意义上试图最大化奖励。通常你用来监控训练的指标是平均奖励。

但这不是一个好的...我的意思是这是一种监控你的训练的方法，但这不一定是最好的方法。

至少在预训练和 SFT 时，你有这个交叉熵损失，你在监控它，它真正告诉你你的模型如何能够重现你试图让它输出的行为。

但是，这是另一个挑战。

然后另一件事是，在 RL 世界中，为了让你知道哪些完成你不应该做，哪些完成你应该做更多，每当你生成时你需要有一些多样性。

所以如果在 RL 训练循环期间，每次你有一个 prompt，你生成一些东西，然后你重试，你生成的其他东西太相似了。

这不是很好，因为你没有探索可能完成的集合。所以这是你需要记住的另一个挑战，就是你需要强制你的模型做一些探索，我想看看什么可能是好的。

然后最后一点是，我的意思是你们很多人都不熟悉 RL。我也不熟悉，为什么我们一定需要 RL 来做这个阶段并不是很清楚。

我也想说一件事。所以这种基于 RL 的训练，你会经常听到在线训练这个术语。我只是想告诉你这意味着什么。这与 SFT 的区别是，在 SFT 期间，我们有一些 prompt 和一些我们希望我们的模型生成来模仿的响应。

但在这里我们所做的是在每次迭代时，我们要求我们的模型生成一个输出，然后我们使用该输出的好坏来更新我们的模型。所以两者之间有核心区别，因为在这里我们所做的是要求模型生成一些东西，而对于 SFT，我们只是使用一些不一定由我们的模型生成的数据。

所以在线训练指的是涉及模型从其当前策略生成并对此进行优化的训练。

这与离线训练形成对比，离线训练依赖于不是由你正在训练的模型完成的生成。

这有意义吗？

所以这是一个你会经常看到的术语，这就是为什么我告诉你。PPO 是一个在线算法。

**学生问题：**

问题是为什么我们不对这些偏好数据做 SFT？嗯，SFT 你告诉模型在给定这个输入的情况下你应该生成这个。然后在给定这个输入的情况下你应该生成这个。但你没有告诉它关于它不应该生成的东西。

比如假设你有一些你不希望发生的事情，除非你将其重写为一个完美的句子。在传统的 SFT 框架中，你没有办法告诉你的模型不要生成那个。

但话虽如此，这是一个很好的 prompt，因为稍后，比如说 10 分钟后，我们将看到一种监督的方法来做我们用 RL 做的事情。

但大概有意义吗？

完美。所以在接下来的几分钟里，我们将看到一种实际上比 RL 更常用的方法，这是为那些有奖励模型但不想做 RL 的人准备的。

那么为什么你不想做 RL？因为它很昂贵，你需要很多模型。所以就你的观点而言，它确实很昂贵，因为你需要所有这些模型，有时调整那个训练太困难了。所以你有这个叫做 best of n 或 BoN 的方法。

这里的想法是利用你获得的奖励模型来选择你将返回给用户的输出。

### (1:27:04 - 1:42:58) DPO：直接偏好优化
> 展示如何绕过奖励模型，直接监督学习偏好对齐

所以这里的想法是你有一个 prompt，你告诉你的模型好的，我实际上想要有几个生成。所以你生成，比如说我不知道四次五次，你所做的是给每个 prompt completion 一个分数，你只返回最好的那个，评分最高的那个。

这就是为什么它被称为 best of n。所以你有 n 个完成，你对所有这些进行评分，你只返回最好的那个。

所以你有你的 prompt。我们最喜欢的例子：建议一个我可以和我的泰迪熊做的新活动。你把它放入你的 SFT 模型。所以，你不是在训练你的 SFT 模型。它是现状，你只是生成，假设三个完成。

第一个是一个很好的答案。第二个是不，不要花时间和泰迪熊在一起，这不是一个好答案。然后第三个，我想也是一个好的，带你的泰迪熊去野餐。你把所有这些放入你的奖励模型，这就是我想回应你的地方，比如奖励模型的输出是什么。所以它会是这样的分数。例如，第一个会是相当好的完成。所以是 0.8，第二个根本不好，比如说负二，第三个比如说一般般。

所以 best of n 方法会选择评分最高的那个，也就是第一个，这就是你会返回的那个。

那么这种方法有什么问题吗？

答案是如果模型很差，那么它就会很差。这是公平的。但我想如果你用足够高的温度做足够多的完成，你会有一些多样性。

### (1:30:00 - 1:45:00) Part 7

好的。今天我要向 Shervin 交接。谢谢你们。

在最后这个部分，我们要讨论 DPO。

但我还不会告诉你们 DPO 的含义。我们首先要听听在强化学习世界中遇到的问题，然后看看我们能做些什么，以及研究人员是如何解决这些问题的。

首先我要重复 Afinen 提到的一点，在强化学习的 PO 表述中，在损失优化过程中你必须携带一堆模型权重。当你看这个损失函数时，你有当前模型的策略需要优化，还有旧模型或参考模型，然后你有优势函数，其中隐藏了奖励模型以及我们讨论过的价值函数。所以总共有四个模型，这是很多的。

第二个问题是，假设你采用 best of n 的方法，你不做强化学习，只是多次生成，然后像 Ein 提到的那样选择最佳完成。这样你就有了延迟和成本问题。为了思考实验，假设我们有无限的资金，这样还行吗？假设你并行生成所有这些。

你仍然需要等待生成答案的最大时间来给出最终答案。当你看延迟的分布时，通常是某种不以单点为中心的形状。当你看 n 个答案的最大值的概率分布时，它往往会向右偏移。所以即使你有无限的资金、无限的计算，你仍然需要比单次通过等待更长的时间。

这些问题有道理吗？

好的，这里就出现了有人提出的问题。为什么我们不一直使用监督学习呢？这就是 DPO 研究人员探索的路线。DPO 代表直接偏好优化（Direct Preference Optimization）。它不是先找到某个奖励模型，然后迭代模型权重这种昂贵的两步过程，而是优化一个直接优化模型权重的单一损失函数。如你所见，不再涉及奖励。所以没有 R。你有办法将你关心的内容表述为偏好对的函数。

在那个损失函数中，你只有一堆变量和 sigmoid 函数，但在括号内你有给定完成发生的概率，你有这个减法操作，比较获胜完成发生的概率与失败完成的概率。所以如你所见，这是一个关于这些偏好对的直接表达式，这很不错。

如果你仔细看，你会认出我刚才提到的关于 Bradley 公式的内容，因为你有这个 log sigmoid 的期望值，在那个 sigmoid 内你有项的差，你可以认出相当于你看到的奖励项的等价物。

我知道这一次看到很多内容。公式不是很漂亮。你们有什么问题吗？

然后我们稍后会讨论那个 beta 是什么，以及我们如何首先获得这个公式。

好的。在讲到那之前我要提到一件事，这篇论文的标题是"你的语言模型暗中是一个奖励模型"。原因是你可以从这个损失函数中识别的奖励占位符是作为你策略的函数表达的。所以那里没有任何 R。但当你表达奖励的等价物时，你直接找到了你模型的表达式。这是一个相当有趣的洞察。

好的。但现在让我们一起思考我们首先是如何到达那里的。

你们还记得几张幻灯片前我们的 PO 目标。我们想要最大化奖励，并最小化获得的策略与参考策略之间的距离，这是最大化奖励和 KL 散度项之间的权衡。然后你有这个 beta 系数出现在那里，控制你想要多大程度地惩罚偏离基础模型。这篇论文做的是写下了这个公式，然后求解最优解。它表达了最优策略 π 星，当你推导这个表达式时，你得到它作为 r 的函数。这是目标函数中的另一项。

所有这些步骤没有额外的假设。这只是一堆推导。你可以看到这个 Z 项代表分割函数，它只是归一化，但它不是新的东西。它就像其他已经在那里的项的函数。

当你重新排列这些项时，你可以等价地将 R 表达为这个 π 星的函数。这只是在这两个表达式之间重新排列项。没有什么太花哨的。

这篇论文做的关键部分是将这个项识别为某种奖励，然后将其表达为 Bradley 公式的一部分。你记得你有获胜完成比失败完成更大的概率，你有两个 R 之间差的 sigmoid。

基本上他们再次采用了这个公式，并插入了他们为最优奖励获得的表达式，我们看到这是策略的函数。所以最终你插入东西，你不再有奖励，只有其他参数的函数。所以你有这个 beta，只有策略在那里。

最后一步与 Afin 提到的相同。一旦你有了想要最大化的概率，你可以将其转换为损失函数。这个损失函数是这篇论文提出优化的，以便调整策略权重以使其与偏好数据对齐。

是的，我将这篇论文相当繁重的步骤大大简化为这几个步骤。它们有道理吗？

我们在这里看到的 beta 与我们在 PO 公式中看到的是同一个 beta。如果你想要某种数量级，通常取 0.1 左右。所以 beta 是一个超参数，你想要优化的只是策略，然后你将偏好对作为输入。

有道理吗？

好的，很好。现在让我们比较这种方法相对于 RLHF 给出了什么。

RLHF 是这种两阶段训练过程，你首先拟合奖励函数，然后用它来进行策略更新步骤。正如 Afin 提到的，这相当繁重。我们刚才提到你需要很多模型在内存中，然后训练稳定性是一个问题。

你没有直接监督。你依赖当前模型的策略来生成下一次更新。所以其中有很多复杂性，与此相反，这个 DPO 公式给出了一种监督的方式来思考偏好调整，你只需取这些偏好对，直接在它们上拟合损失函数。然后不是四个模型，你只需要两个，因为你有 π theta 和 π ref，这是你的基础 SFT 模型，你不需要任何其他模型副本。

现在你可能想问，如果它更容易，为什么不是每个人都简单地使用 DPO？所以它没有那么容易。在实践中，每种方法都有一些优缺点。下面列出的论文对基准差异和达到那里的步骤进行了非常有趣的研究。

只是为了得到主要线路，总体而言 PPO 比 DPO 表现更好。DPO 的一个很好的事情，就是这种监督、这种直接监督，代价是有时拟合的不是模型在训练时看到的确切分布。这篇论文对此有一些讨论。如果你在偏好相关数据上通过 SFT 训练，你可能会得到更好的性能。但这种分布偏移是 DPO 固有的挑战。

因为 DPO 方法使你能够做的一件事就是根本不用担心奖励建模，只需要获得一些偏好数据集。但这样做的问题正是那种分布偏移。所以你可以在其上训练 SFT 或自己生成然后评级，但那是你需要付出的另一个成本。

### (1:42:58 - 1:47:38) 实践对比与课程总结
> 对比RLHF与DPO的优劣，展示泰迪熊示例的改进效果

好的，很好。

有什么问题吗？

问题是你在相对于基础模型优化参数。是的，但它不是基础模型，是当前的模型。你有一个基础模型，在迭代中保持固定，出现在损失函数中，你确实在更新参数，那将是偏好调整的模型。是的。所以你有两个权重、两组权重，一个冻结的，一个你更新的。是的。好的，很好。

还有其他问题吗？

是的。问题是不用参考模型，你能使用另一个模型、另一个大型模型吗？我认为这是一个有趣的观点。我想那会是一个不同的算法。你可能会失去一点你试图做的事情的意义，因为这个偏好调整阶段试图将你的响应与你偏好的内容对齐，你希望它们相对于你到目前为止学到的内容对齐。所以如果你从不同的模型开始，然后偏好调整不同的概率分布，你可能在做从损失角度来看可能没有那么大意义的事情。但我想你知道方向，我的意思是我不能给出一个总体答案。你知道，也许它可能给出有趣的行为，但我的直觉告诉你，所有这些的要点是从你拥有的最后一个训练检查点开始，然后进一步偏好调整你的完成分布，对吧？所以建议是，你知道，如果你改变参考模型，那可以看作是你模型的另一个 SFT 版本。是的，我想是这样。

好的，很好。现在是今天我最喜欢的部分。你们都还记得我们在最喜欢的提示上获得的完成吗？"我能把我的泰迪熊放进洗衣机吗？"如果你们记得上集，我们说过指令调整后的回应是"不行，它可能会损坏，试试手洗吧。"

有人对这个答案有什么抱怨吗？

我可以给个提示，泰迪熊确实应该手洗。所以这在事实上是正确的。

### (1:45:00 - End) Part 8

确实应该手洗。所以这在事实上是正确的。

但假设这在事实上是正确的，你们还有什么不喜欢的地方？我是说，这个答案还有什么你们不满意的？

**学生：** 建议是要求"试试手洗吧"。我觉得更广泛的一点是，这个回答有点太粗糙了，听起来不够友好。而且你知道，问这个问题的人可能很喜欢他们的泰迪熊。所以你可能想要用更温和的方式告诉他们。这正是偏好调优阶段的核心所在。

我们不想学习新的事实，我们想要专注于现有的回复分布，并将模型应该返回的内容与人类的偏好对齐。

基于这种直觉，经过偏好调优的答案可能会更温和一些："最好不要这样做，你的泰迪熊可能会受到伤害。轻柔的手洗更安全。" 它传达的是同样的要点，但语调听起来对提问者来说要好得多。

好的，很棒。对 DPO 部分有什么问题吗？

**学生：** 权衡取舍是什么，以及实践中使用哪种方法？

这完全取决于你的计算预算，取决于你想在性能与训练过程管理之间如何平衡。像 RL 这样的 PPO 很难调优好，它会给出最高的结果，但你可能用更少的努力就能达到几乎同样的水平。


---

*生成时间: 2026-01-04 03:03:07*
*由 YouTube Monitor & Translator (Claude CLI) 生成*