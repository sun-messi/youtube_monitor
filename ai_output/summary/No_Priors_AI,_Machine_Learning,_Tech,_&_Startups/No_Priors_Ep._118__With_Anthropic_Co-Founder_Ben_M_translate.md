# No Priors Ep. 118 | With Anthropic Co-Founder Ben Mann

本文内容整理自 **No Priors: AI, Machine Learning, Tech, & Startups** 频道的视频。

原始链接：https://www.youtube.com/watch?v=aStf54Vxy24

---

我将按照要求为您生成视频摘要：

# 视频摘要：Anthropic 联合创始人 Ben Mann 解析 Claude 4 与 AI 发展

## 1. 视频开头信息

> 本文内容整理自 Anthropic 联合创始人本·曼（Ben Mann）在 No Priors 播客频道的深度访谈。

## 2. TL;DR

大语言模型正迈向更长视野的任务，Anthropic 通过Constitutional AI和负责任的模型扩展政策，探索AI安全与能力边界。

## 3. 📑 章节导航表

| 时间戳 | 章节标题 | 一句话概括 |
|--------|----------|-----------|
| 00:00-00:33 | Ben Mann 介绍 | 介绍 Ben Mann 的职业背景和加入 Anthropic 的初衷 |
| 00:33-02:05 | Claude 4 发布 | 解释 Claude 模型版本发布的决策过程 |
| 02:05-03:42 | Claude 4 亮点与改进 | 详细阐述 Claude 4 在代码生成等方面的显著进步 |
| 03:42-06:42 | 先进用例与能力 | 展示 Claude 4 在复杂任务中的自主执行能力 |
| 06:42-09:35 | 模型专业化与未来发展 | 探讨AI模型的专业化和多Agent协作趋势 |
| 09:35-18:08 | Anthropic 模型开发方法 | 阐述 Anthropic 在模型训练和开发中的独特方法 |
| 18:08-23:38 | 人类反馈与模型自我改进 | 讨论如何通过人类和AI反馈不断提升模型 |
| 23:38-27:02 | 经验主义与现实世界应用 | 分享模型能力验证和实际应用的方法 |
| 27:02-38:35 | AI 安全与伦理考量 | 深入探讨AI安全研究的边界和原则 |
| 38:35-41:00 | MCP 与行业标准 | 介绍模型上下文协议（MCP）及其意义 |

## 4. 📊 核心论点

#### Claude 4：更智能的代码生成

- **核心内容**：Claude 4 在代码生成中显著改进，克服了之前模型的"过度变更"问题。它能更精准地执行开发者指令，减少不必要的代码修改，提高专业软件工程的可维护性和可靠性。
- **关键概念**：代码重构、AI辅助编程、模型自我修正
- **实际意义**：显著提高软件开发效率，降低开发成本，推动AI在专业编程领域的广泛应用

#### Constitutional AI：原则驱动的模型训练

- **核心内容**：通过设定明确的语言原则，使用AI自我反馈和修正机制，在模型训练过程中内置伦理和行为准则。这种方法不仅确保模型输出符合预期标准，还能持续改进模型的一致性和可靠性。
- **关键概念**：强化学习、自我批评、原则嵌入
- **实际意义**：为大规模AI模型提供更可靠、更可控的训练方法，减少潜在的偏见和风险

#### 模型安全研究的边界

- **核心内容**：Anthropic 谨慎评估AI安全研究，例如研究模型可能的欺骗行为，同时设定明确的伦理边界。通过负责任的扩展政策（RSP），持续评估和控制模型潜在风险。
- **关键概念**：AI对齐、安全级别、双重用途技术
- **实际意义**：为AI研究建立负责任的框架，平衡技术创新和社会风险

## 5. 🏢 提及的公司/产品

| 公司名 | 讨论语境 | 重要性 |
|--------|----------|--------|
| Anthropic | AI模型开发和安全研究的核心 | ⭐⭐⭐ |
| OpenAI | 模型开发和MCP标准参与者 | ⭐⭐ |
| Google | MCP标准合作伙伴 | ⭐⭐ |
| Microsoft | MCP标准合作伙伴 | ⭐⭐ |
| Novo Nordisk | AI在医疗研究中的应用案例 | ⭐ |

## 6. 💬 经典金句

> "我们正在尝试展示，部署这些能力是可以负责任的，同时也极其有用。"
> — Ben Mann

> "模型的真正价值在于它能为人类解决复杂的、需要长时间专注的任务。"
> — Ben Mann

## 7. 👤 主要人物

#### Ben Mann（本·曼）

**身份**：Anthropic 联合创始人、前 OpenAI 工程师
**背景**：GPT-3 论文早期作者，2021 年离开 OpenAI 创立 Anthropic，致力于人工智能长期安全研究
**核心观点**：通过负责任的模型开发和Constitutional AI，探索AI的边界和潜力，同时始终将安全和伦理放在首位

## 8. 📺 视频类型判断

- **访谈对话**：多人深入探讨AI技术发展、伦理和未来

---

## 📝 完整翻译

### (33:00 - 2:05:00) Releasing Claude 4

我不认为你应该让某些病毒通过数百万个细胞来增加其感染性，或者进行获得功能性突变。如今，控制模型的难度可能比控制生物标本要容易得多。你随意提到了生物安全级别。这正是我们的人工智能安全级别所模仿的对象。

我认为，只要我们有适当的保障措施，比如训练模型具有欺骗性，这可能听起来很可怕，但我认为这对我们了解很重要。例如，如果我们的训练数据被污染，我们是否能在后期训练中纠正。在我们发表的一篇名为"对齐伪造"的论文中，我们发现那种行为即使经过对齐训练也会持续存在。因此，我认为测试这些事情非常重要。

不过，我敢肯定总有一个界限。我发现，早期建立的先例往往会长期持续，即使人们意识到环境或其他因素已经改变。顺便说一句，我基本上反对针对人工智能的大多数监管。我认为有些出口管制等是可以支持的，但总的来说，我倾向于让事情现在就发生。

但另一方面，我确实认为在某些情况下，如果早期进行某些研究，人们可能没有足够的背景来避免后来继续这种研究。训练人工智能或模型具有欺骗性就是一个很好的例子。多年后，即使环境已经足够改变，使其不再像从前那样安全，人们可能仍会继续这样做，因为之前曾经这样做过。

我发现这些事情往往会随着时间在组织或理念上持续存在。有趣的是，没有人明确说"我们绝对不应该做X类研究"。需要澄清的是，我已经不在安全团队了。我现在主要考虑如何让我们的模型有用，部署它们，并确保它们符合基本的安全部署标准。我们有很多专家一直在思考这类问题。

主持人： 很酷。感谢你分享这些见解。我想稍微转换一下话题。你能谈谈 Claude 4 之后会是什么吗？训练过程中有什么新的涌现行为会改变公司的运营方式，或者你想构建什么产品？你正在运营这个实验室，这基本上是 Anthropic 的尖端，或者安全团队是如何运作的？未来会如何改变你们的运营？

Claude工程师：让我讲个关于计算机使用的小故事。去年，我们发布了一个参考实现，一个可以点击、查看屏幕和阅读文本的代理。现在已经有几家公司在使用它。Manis 正在使用，许多公司内部用于软件质量保证，因为那是一个沙盒环境。

但我们没有部署面向消费者或终端用户的应用程序的主要原因是安全性。我们不确定如果给 Claude 访问包含你所有凭据的浏览器权限，它不会犯错并采取不可逆转的行动，比如发送你不想发送的邮件，或者在提示注入攻击中泄露更严重的凭据。

这有点可惜，因为在完全自主模式下，它可以为人们做很多事情。它是有能力的，但安全性还不足以让我们自己将其产品化。虽然这很雄心勃勃，但我们认为这也是必要的，因为世界其他地方也不会放慢脚步。如果我们能证明可以负责任地部署这些能力，同时又极其有用，那将提高标准。这是一个我们非常谨慎地推出的例子。但我们知道我们目前的水平还不够高。