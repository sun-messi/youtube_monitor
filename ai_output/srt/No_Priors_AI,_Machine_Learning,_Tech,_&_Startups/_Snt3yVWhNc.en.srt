1
00:00:05,839 --> 00:00:10,719
Hi listeners, welcome to No Priors. How

2
00:00:08,720 --> 00:00:12,559
can we even begin to wrap this year up?

3
00:00:10,719 --> 00:00:14,160
The AI field has grown, breaking out

4
00:00:12,559 --> 00:00:16,880
into the mainstream and taking center

5
00:00:14,160 --> 00:00:18,640
stage with policy makers. Chat GPT

6
00:00:16,880 --> 00:00:20,720
shipped massive numbers and asked for

7
00:00:18,640 --> 00:00:22,480
massive [music] dollars. Gemini and

8
00:00:20,720 --> 00:00:24,480
Google roared back strong. And on the

9
00:00:22,480 --> 00:00:26,800
application front, AI coding has shifted

10
00:00:24,480 --> 00:00:29,359
to agents and is eating up all of our

11
00:00:26,800 --> 00:00:31,359
inference capacity. Doctors are adopting

12
00:00:29,359 --> 00:00:32,880
clinical decision support on mass and in

13
00:00:31,359 --> 00:00:34,719
law and customer support. Enterprise

14
00:00:32,880 --> 00:00:37,440
adoption is accelerating. [music] What's

15
00:00:34,719 --> 00:00:39,280
next? On the research front, the race

16
00:00:37,440 --> 00:00:41,200
has multiple live players with open

17
00:00:39,280 --> 00:00:43,840
source closing the gap too. [music] A

18
00:00:41,200 --> 00:00:45,760
handful of Neolabs, new research labs

19
00:00:43,840 --> 00:00:48,320
got funded this year. And the narrative

20
00:00:45,760 --> 00:00:50,399
is changing. Ilia is calling it the age

21
00:00:48,320 --> 00:00:52,000
of research. People are trying different

22
00:00:50,399 --> 00:00:55,360
ideas around diffusion,

23
00:00:52,000 --> 00:00:57,120
self-improvement, data efficiency, EQ,

24
00:00:55,360 --> 00:01:00,160
large scale Asian collaboration,

25
00:00:57,120 --> 00:01:02,480
continual learning, energy transformers.

26
00:01:00,160 --> 00:01:04,080
It's more open than it's ever been.

27
00:01:02,480 --> 00:01:06,479
Finally, we had a lot of attempts to

28
00:01:04,080 --> 00:01:09,439
make AI reach into the real world with

29
00:01:06,479 --> 00:01:11,119
renewed optimism around robotics. Next

30
00:01:09,439 --> 00:01:13,760
year, those companies are going to start

31
00:01:11,119 --> 00:01:15,840
making contact with reality. From a

32
00:01:13,760 --> 00:01:17,119
prediction standpoint, personally, I

33
00:01:15,840 --> 00:01:18,560
think we're going to see somebody make a

34
00:01:17,119 --> 00:01:20,720
lot of money. Hundreds of millions of

35
00:01:18,560 --> 00:01:23,680
dollars trading markets with LLMs next

36
00:01:20,720 --> 00:01:25,520
year. It's inevitable. So, we're in the

37
00:01:23,680 --> 00:01:26,640
second or third inning. Markets are

38
00:01:25,520 --> 00:01:28,479
running a little hot and a little

39
00:01:26,640 --> 00:01:30,560
volatile. It's hot in the hot tub. So,

40
00:01:28,479 --> 00:01:31,840
get into it with me and Alad. Okay.

41
00:01:30,560 --> 00:01:34,400
Alad, it's been a year.

42
00:01:31,840 --> 00:01:36,560
>> I know. How's it going? 2026, baby.

43
00:01:34,400 --> 00:01:39,280
>> Are you feeling the AGI? Are you feeling

44
00:01:36,560 --> 00:01:40,960
AI AI winter in a good way?

45
00:01:39,280 --> 00:01:42,799
>> I think I'm actually just feeling

46
00:01:40,960 --> 00:01:44,720
microplastics. I think I'm now 80%

47
00:01:42,799 --> 00:01:46,560
microplastics and just increasing my

48
00:01:44,720 --> 00:01:48,320
microplastic consumptions. A friend of

49
00:01:46,560 --> 00:01:50,079
mine actually launched a new water brand

50
00:01:48,320 --> 00:01:51,280
that uh has no microplastics by the way.

51
00:01:50,079 --> 00:01:53,280
It's called Loop and they have like

52
00:01:51,280 --> 00:01:54,000
glass bottles and also the cap doesn't

53
00:01:53,280 --> 00:01:55,920
have plastic.

54
00:01:54,000 --> 00:01:58,079
>> Does it come with continual testing?

55
00:01:55,920 --> 00:01:59,360
>> Yeah, that's continual testing for you.

56
00:01:58,079 --> 00:02:01,759
>> They did actually try to take out all

57
00:01:59,360 --> 00:02:04,079
the microplastics and so they uh I guess

58
00:02:01,759 --> 00:02:05,600
bottled water in actual bottles has more

59
00:02:04,079 --> 00:02:06,719
microplastics than plastic bottles

60
00:02:05,600 --> 00:02:09,039
because of the cap.

61
00:02:06,719 --> 00:02:10,160
>> Okay, we'll check back in with you in 27

62
00:02:09,039 --> 00:02:12,239
to see if you feel

63
00:02:10,160 --> 00:02:14,640
>> Yeah. But I'm just completely oified out

64
00:02:12,239 --> 00:02:16,480
of plastic. I'm actually really worried

65
00:02:14,640 --> 00:02:17,920
about microlastics. What about all the

66
00:02:16,480 --> 00:02:19,120
little glass particles? Aren't you

67
00:02:17,920 --> 00:02:21,360
worried about that? People talk about

68
00:02:19,120 --> 00:02:22,720
microlastics, but not microlastics. I'm

69
00:02:21,360 --> 00:02:24,160
much more concerned about that.

70
00:02:22,720 --> 00:02:26,080
>> I don't think those particles end up

71
00:02:24,160 --> 00:02:28,080
embedded for you permanently.

72
00:02:26,080 --> 00:02:30,319
>> Silicon. [laughter]

73
00:02:28,080 --> 00:02:32,000
You're not worried about silicons. When

74
00:02:30,319 --> 00:02:34,400
I go to the beach, I'm like, "Oh no,

75
00:02:32,000 --> 00:02:36,480
microlastics everywhere." I'm actually

76
00:02:34,400 --> 00:02:39,680
very willing to insert silicon in my

77
00:02:36,480 --> 00:02:41,920
body eventually in my [laughter]

78
00:02:39,680 --> 00:02:43,280
>> Wow, that was Yeah, I'm not gonna say

79
00:02:41,920 --> 00:02:45,280
anything. We can keep going.

80
00:02:43,280 --> 00:02:46,800
>> What's What's happening in AI? Al, what

81
00:02:45,280 --> 00:02:47,920
are you where where are we and what are

82
00:02:46,800 --> 00:02:49,599
you most excited about?

83
00:02:47,920 --> 00:02:51,280
>> Yeah, I guess for 26 there's a bunch of

84
00:02:49,599 --> 00:02:54,239
stuff I think will be interesting that's

85
00:02:51,280 --> 00:02:55,200
coming. I think we will um I think

86
00:02:54,239 --> 00:02:56,800
there's probably four or five things.

87
00:02:55,200 --> 00:03:00,080
One is I think people will proclaim yet

88
00:02:56,800 --> 00:03:02,080
again that AI is not doing much and it's

89
00:03:00,080 --> 00:03:03,360
overhyped and like that MIT report that

90
00:03:02,080 --> 00:03:05,200
people are quoting that I thought really

91
00:03:03,360 --> 00:03:06,879
didn't matter and the reality of the

92
00:03:05,200 --> 00:03:08,560
technology ways to take like 10 years to

93
00:03:06,879 --> 00:03:10,000
propagate and people are getting

94
00:03:08,560 --> 00:03:10,959
enormous value out of AI already and

95
00:03:10,000 --> 00:03:12,480
they're going to get way more out of it

96
00:03:10,959 --> 00:03:14,720
in the future. You know, so there's

97
00:03:12,480 --> 00:03:16,800
these undoubtedly next year there'll be

98
00:03:14,720 --> 00:03:19,360
these overstated but bubble claims as

99
00:03:16,800 --> 00:03:20,400
well as um hey I actually isn't working

100
00:03:19,360 --> 00:03:21,680
that well kind of claims and that

101
00:03:20,400 --> 00:03:23,680
happens every technology cycle and we'll

102
00:03:21,680 --> 00:03:25,360
just hear it again next year and

103
00:03:23,680 --> 00:03:26,640
there'll be pundits and discussions and

104
00:03:25,360 --> 00:03:28,319
just a bunch of waste of time on it. So

105
00:03:26,640 --> 00:03:31,120
I think that'll happen. I think another

106
00:03:28,319 --> 00:03:32,879
prediction for 26 is the next set of

107
00:03:31,120 --> 00:03:34,720
verticals will hit massive scale. I

108
00:03:32,879 --> 00:03:36,959
think this year we saw consolidation of

109
00:03:34,720 --> 00:03:38,400
coding into a handful of players, of

110
00:03:36,959 --> 00:03:40,080
medical scribing into a handful of

111
00:03:38,400 --> 00:03:42,400
players, of legal into a handful of

112
00:03:40,080 --> 00:03:43,680
players like Harvey and others. And so I

113
00:03:42,400 --> 00:03:45,760
think we'll see that next set of

114
00:03:43,680 --> 00:03:47,280
consolidated verticals happening. So

115
00:03:45,760 --> 00:03:48,560
that'll be interesting. I can keep

116
00:03:47,280 --> 00:03:50,319
going. I have like a bunch of these. Do

117
00:03:48,560 --> 00:03:51,760
you want to go next? We can alternate. I

118
00:03:50,319 --> 00:03:52,959
just did two. Why don't you do two?

119
00:03:51,760 --> 00:03:53,599
>> Maybe I'll react.

120
00:03:52,959 --> 00:03:55,440
>> Or react.

121
00:03:53,599 --> 00:03:57,200
>> I'll react and then I'll and then I'll

122
00:03:55,440 --> 00:03:58,239
give you two predictions. Um I have to

123
00:03:57,200 --> 00:03:59,920
think of my predictions while I'm

124
00:03:58,239 --> 00:04:02,799
reacting. So I'm glad I have at least

125
00:03:59,920 --> 00:04:07,360
two threads. Yes. I I think that the

126
00:04:02,799 --> 00:04:10,720
overall sentiment on AI in the investing

127
00:04:07,360 --> 00:04:12,879
landscape is a lot of people getting

128
00:04:10,720 --> 00:04:15,920
stressed about the amount of capital

129
00:04:12,879 --> 00:04:19,280
they have at work and then just a level

130
00:04:15,920 --> 00:04:21,359
of uncertainty around uh the adoption

131
00:04:19,280 --> 00:04:23,360
cycle and technical bets that people are

132
00:04:21,359 --> 00:04:24,960
making that they don't have full first

133
00:04:23,360 --> 00:04:27,919
principles confidence on coming to

134
00:04:24,960 --> 00:04:32,000
roost. So, uh I I think like any number

135
00:04:27,919 --> 00:04:34,240
of exogenous factors plus noise about um

136
00:04:32,000 --> 00:04:36,400
the speed of adoption, which by the way

137
00:04:34,240 --> 00:04:37,759
seems like blinding overall and we can

138
00:04:36,400 --> 00:04:39,120
talk about what the constraints are.

139
00:04:37,759 --> 00:04:41,199
>> Yeah. So fast I don't even know what

140
00:04:39,120 --> 00:04:43,759
people are talking about. I I just saw a

141
00:04:41,199 --> 00:04:46,240
report um that talked about it's from

142
00:04:43,759 --> 00:04:48,800
this group called uh off call that

143
00:04:46,240 --> 00:04:52,000
talked about adoption of AI by doctors

144
00:04:48,800 --> 00:04:53,680
and look there is just amazing adoption

145
00:04:52,000 --> 00:04:56,080
of of course you know several different

146
00:04:53,680 --> 00:04:57,680
categories like documentation clinical

147
00:04:56,080 --> 00:04:59,280
decision support with things like a

148
00:04:57,680 --> 00:05:01,680
bridge and open evidence and obviously

149
00:04:59,280 --> 00:05:04,160
the general models but there's like

150
00:05:01,680 --> 00:05:06,479
massive enthusiasm from most of the

151
00:05:04,160 --> 00:05:08,800
physician profession here and I'm like

152
00:05:06,479 --> 00:05:10,080
okay of of all of the domains that were

153
00:05:08,800 --> 00:05:12,320
professional considered more

154
00:05:10,080 --> 00:05:15,039
conservative. The fact that there is

155
00:05:12,320 --> 00:05:16,720
this like you know desire to have things

156
00:05:15,039 --> 00:05:18,560
that make work better seems like

157
00:05:16,720 --> 00:05:21,039
obviously to continue in the other

158
00:05:18,560 --> 00:05:23,759
professions. I I think this is by the

159
00:05:21,039 --> 00:05:25,600
way super underd discussed that the

160
00:05:23,759 --> 00:05:28,960
people who tended to be the slowest

161
00:05:25,600 --> 00:05:30,720
adopters of technology love AI. That's

162
00:05:28,960 --> 00:05:32,560
physicians, that's lawyers, that's

163
00:05:30,720 --> 00:05:33,919
certain accounting types. It's, you

164
00:05:32,560 --> 00:05:36,320
know, it's it's actually kind of

165
00:05:33,919 --> 00:05:38,000
fascinating. It's compliance, you know,

166
00:05:36,320 --> 00:05:39,520
it's all the people who always never

167
00:05:38,000 --> 00:05:41,759
adopt technology are now adopting this

168
00:05:39,520 --> 00:05:43,440
stuff fast. So, I do think that's really

169
00:05:41,759 --> 00:05:44,720
notable and very under discussed.

170
00:05:43,440 --> 00:05:46,560
>> It will keep happening. There are

171
00:05:44,720 --> 00:05:48,479
actually lots of professions where like

172
00:05:46,560 --> 00:05:50,960
being able to reason and interact with

173
00:05:48,479 --> 00:05:52,639
unstructured data is very useful. Like I

174
00:05:50,960 --> 00:05:54,320
expect that there's going to be some

175
00:05:52,639 --> 00:05:56,880
like negative market current. Like you

176
00:05:54,320 --> 00:05:58,800
know if Nvidia doesn't overperform by

177
00:05:56,880 --> 00:06:01,280
some massive amount one quarter,

178
00:05:58,800 --> 00:06:02,400
everybody's going to freak out. But I I

179
00:06:01,280 --> 00:06:03,840
think that has very little to do with

180
00:06:02,400 --> 00:06:05,280
the fundamental secular change.

181
00:06:03,840 --> 00:06:06,720
>> Yeah. It has to do with microplastics

182
00:06:05,280 --> 00:06:08,240
and Nvidia. It's my two cents.

183
00:06:06,720 --> 00:06:09,840
>> It has to it has to do with um

184
00:06:08,240 --> 00:06:11,280
microlastics as you said.

185
00:06:09,840 --> 00:06:13,039
>> Yeah, it's true. Actually, the silicon

186
00:06:11,280 --> 00:06:14,880
there is in the air. I bet that they

187
00:06:13,039 --> 00:06:16,240
have microlastics all over the place.

188
00:06:14,880 --> 00:06:18,080
It's messed up. Sarah,

189
00:06:16,240 --> 00:06:20,880
>> it's part of the trade. If you make $20

190
00:06:18,080 --> 00:06:22,720
million as an average Nvidia employee,

191
00:06:20,880 --> 00:06:23,520
you also have to have microlastics in

192
00:06:22,720 --> 00:06:24,960
your blood.

193
00:06:23,520 --> 00:06:26,880
>> Don't listen to this, Jensen. Jensen's

194
00:06:24,960 --> 00:06:28,720
our next guest. You can't hear that.

195
00:06:26,880 --> 00:06:32,400
>> 1% microlastics in the blood.

196
00:06:28,720 --> 00:06:34,240
>> I think um you know, a third area is the

197
00:06:32,400 --> 00:06:35,680
next set of foundation models are going

198
00:06:34,240 --> 00:06:39,120
to come. And by that I don't mean the

199
00:06:35,680 --> 00:06:40,800
neolabs and the and the nextgen LLMs

200
00:06:39,120 --> 00:06:44,000
which of course will happen but I mean

201
00:06:40,800 --> 00:06:46,240
uh physics materials um science progress

202
00:06:44,000 --> 00:06:47,680
by models math progress and I think

203
00:06:46,240 --> 00:06:49,520
what'll happen is there'll be one or two

204
00:06:47,680 --> 00:06:50,800
usea one or two cases where it works

205
00:06:49,520 --> 00:06:52,800
really well for something they'll invent

206
00:06:50,800 --> 00:06:55,600
some new material or there'll be some

207
00:06:52,800 --> 00:06:57,680
conjecture proved or something

208
00:06:55,600 --> 00:06:59,199
and then it'll fall into this overstated

209
00:06:57,680 --> 00:07:00,639
hype cycle of it's going to change

210
00:06:59,199 --> 00:07:03,360
everything about physical sciences or

211
00:07:00,639 --> 00:07:04,960
whatever and that oneoff will be

212
00:07:03,360 --> 00:07:06,319
overstated And in the long run, the

213
00:07:04,960 --> 00:07:07,759
trend will be understated and will be

214
00:07:06,319 --> 00:07:08,960
incredibly important. So that's what

215
00:07:07,759 --> 00:07:10,560
that's another prediction for next year

216
00:07:08,960 --> 00:07:13,280
is there'll be a couple anecdotal

217
00:07:10,560 --> 00:07:14,639
one-offs in science that will make

218
00:07:13,280 --> 00:07:15,759
people say, "Look, science is solved."

219
00:07:14,639 --> 00:07:17,039
And they'll realize science has been

220
00:07:15,759 --> 00:07:17,680
solved and then later science will be

221
00:07:17,039 --> 00:07:20,400
solved.

222
00:07:17,680 --> 00:07:22,880
>> I have uh Okay, fine. Three three quick

223
00:07:20,400 --> 00:07:25,120
predictions for you. One is there's

224
00:07:22,880 --> 00:07:28,000
going to be like some collapse of

225
00:07:25,120 --> 00:07:30,560
sentiment around a set of robotics

226
00:07:28,000 --> 00:07:32,319
companies next year. Not because it like

227
00:07:30,560 --> 00:07:35,199
actually isn't as a field going to

228
00:07:32,319 --> 00:07:38,240
progress but because you know people are

229
00:07:35,199 --> 00:07:40,720
beginning to project timelines

230
00:07:38,240 --> 00:07:42,080
>> and uh you know not everybody is going

231
00:07:40,720 --> 00:07:44,080
to deliver on those timelines.

232
00:07:42,080 --> 00:07:47,919
>> What's your timeline? I think that we

233
00:07:44,080 --> 00:07:50,400
will see um humanoid and semihumanoid

234
00:07:47,919 --> 00:07:52,479
robots get deployed at small scale in

235
00:07:50,400 --> 00:07:54,479
environments be the consumer or

236
00:07:52,479 --> 00:07:57,520
industrial next year and not everything

237
00:07:54,479 --> 00:07:59,840
will work and that like the because

238
00:07:57,520 --> 00:08:01,599
there's this you know hype cycle around

239
00:07:59,840 --> 00:08:03,120
human rights overall as soon as

240
00:08:01,599 --> 00:08:04,639
something doesn't perfectly work which

241
00:08:03,120 --> 00:08:06,560
it will not people are going to freak

242
00:08:04,639 --> 00:08:09,039
out right and then there's going to be

243
00:08:06,560 --> 00:08:12,400
some bifurcation about people investing

244
00:08:09,039 --> 00:08:14,400
>> yeah I mean we're in year 15 17 whatever

245
00:08:12,400 --> 00:08:17,440
self-driving something around there and

246
00:08:14,400 --> 00:08:18,960
it's really working now but it

247
00:08:17,440 --> 00:08:20,400
seems like robotics should have maybe a

248
00:08:18,960 --> 00:08:21,440
faster curve but a similar curve right

249
00:08:20,400 --> 00:08:23,280
it's going to take some time to figure

250
00:08:21,440 --> 00:08:24,000
all this stuff out and then once it's

251
00:08:23,280 --> 00:08:25,360
figured out it's going to be really

252
00:08:24,000 --> 00:08:26,720
valuable and the big question for me on

253
00:08:25,360 --> 00:08:28,080
robotics you know it's interesting if

254
00:08:26,720 --> 00:08:30,080
you look at self-driving there's like

255
00:08:28,080 --> 00:08:31,680
two dozen three dozen whatever

256
00:08:30,080 --> 00:08:33,039
legitimate self-driving companies really

257
00:08:31,680 --> 00:08:35,519
good teams and good approaches and all

258
00:08:33,039 --> 00:08:38,159
the rest and then arguably the two

259
00:08:35,519 --> 00:08:40,240
biggest winners at least now are Whimo

260
00:08:38,159 --> 00:08:42,800
and Tesla which were two incumbents

261
00:08:40,240 --> 00:08:44,080
right Whimo's Google Tesla is Tesla So I

262
00:08:42,800 --> 00:08:46,080
wonder what will happen to robotics. It

263
00:08:44,080 --> 00:08:47,360
feels to me like Optimus or some form of

264
00:08:46,080 --> 00:08:49,760
like Tesla robot will be one of the

265
00:08:47,360 --> 00:08:51,760
winners most likely, right? High

266
00:08:49,760 --> 00:08:53,360
probability. And then the question is

267
00:08:51,760 --> 00:08:55,839
does Whimo just adopt what it's doing

268
00:08:53,360 --> 00:08:57,440
for cars to robots as well? Because

269
00:08:55,839 --> 00:08:59,279
there's some similar problems there. Is

270
00:08:57,440 --> 00:09:01,120
it some other big industrial company? Is

271
00:08:59,279 --> 00:09:04,000
it startups? Like who are who are the

272
00:09:01,120 --> 00:09:05,920
winners and why? And structurally when

273
00:09:04,000 --> 00:09:07,360
you have a lot of capital needs but also

274
00:09:05,920 --> 00:09:10,240
a lot of hardware and manufacturing

275
00:09:07,360 --> 00:09:12,560
needs that's going to favor incumbents

276
00:09:10,240 --> 00:09:13,920
which is self-driving right um I guess

277
00:09:12,560 --> 00:09:16,480
arguably the other winners in

278
00:09:13,920 --> 00:09:18,080
self-driving are Chinese companies right

279
00:09:16,480 --> 00:09:20,320
Chinese car companies which are banned

280
00:09:18,080 --> 00:09:21,600
from coming into the US market and those

281
00:09:20,320 --> 00:09:23,519
will probably also be winners in

282
00:09:21,600 --> 00:09:25,839
robotics right the most likely global

283
00:09:23,519 --> 00:09:28,080
winners in robotics will be some subset

284
00:09:25,839 --> 00:09:30,399
of China plus Tesla plus something else

285
00:09:28,080 --> 00:09:32,480
right maybe maybe one of the startups

286
00:09:30,399 --> 00:09:34,800
>> I think that's right but that's like I I

287
00:09:32,480 --> 00:09:36,640
think in most industries like

288
00:09:34,800 --> 00:09:38,560
>> you know the incumbents are more likely

289
00:09:36,640 --> 00:09:41,360
to win than the startups if you're just

290
00:09:38,560 --> 00:09:42,880
looking at it like as as a numbers game.

291
00:09:41,360 --> 00:09:44,640
>> I don't know. Yeah, I don't know. I

292
00:09:42,880 --> 00:09:45,920
don't think so. I think um I think

293
00:09:44,640 --> 00:09:46,880
there's startup industries where

294
00:09:45,920 --> 00:09:48,160
startups should win and there's

295
00:09:46,880 --> 00:09:49,440
incumbent industries where incumbents

296
00:09:48,160 --> 00:09:50,880
should win and they have different

297
00:09:49,440 --> 00:09:52,320
characteristics in terms of market

298
00:09:50,880 --> 00:09:53,839
structure, in terms of capital needs, in

299
00:09:52,320 --> 00:09:55,760
terms of certain of expertise and supply

300
00:09:53,839 --> 00:09:57,920
chain, you know. So I do think there are

301
00:09:55,760 --> 00:09:59,600
markets where incumbents should

302
00:09:57,920 --> 00:10:00,959
definitionally do better. They don't

303
00:09:59,600 --> 00:10:02,160
always but they typically do. And then I

304
00:10:00,959 --> 00:10:03,120
think there are markets where startups

305
00:10:02,160 --> 00:10:05,360
will do better.

306
00:10:03,120 --> 00:10:07,519
>> Sure. But I I don't I don't argue that

307
00:10:05,360 --> 00:10:09,519
like some markets are like the modes are

308
00:10:07,519 --> 00:10:11,519
structurally deeper, right?

309
00:10:09,519 --> 00:10:14,160
>> But one way that you might look at

310
00:10:11,519 --> 00:10:17,200
autonomous vehicles is it's one very

311
00:10:14,160 --> 00:10:19,279
complex single use case robot.

312
00:10:17,200 --> 00:10:21,360
>> And it mostly does locomotion. It does

313
00:10:19,279 --> 00:10:22,800
lots of other necessary types of

314
00:10:21,360 --> 00:10:25,120
prediction, defensive drive, whatever

315
00:10:22,800 --> 00:10:25,920
else. But it's it's it's a single use

316
00:10:25,120 --> 00:10:27,600
case robot.

317
00:10:25,920 --> 00:10:29,680
>> Yeah. And we and we forget there's a lot

318
00:10:27,600 --> 00:10:31,519
of good ones like that. Dishwashers are

319
00:10:29,680 --> 00:10:33,040
great single-use robots. Vacuum cleaners

320
00:10:31,519 --> 00:10:34,320
are great. You know, like there's all

321
00:10:33,040 --> 00:10:35,680
these things that we actually have that

322
00:10:34,320 --> 00:10:36,959
are robots in the home that we pretend

323
00:10:35,680 --> 00:10:38,880
aren't, right? We forgot that they're

324
00:10:36,959 --> 00:10:40,880
robots. Elevators are robots.

325
00:10:38,880 --> 00:10:42,720
>> No, seriously. Escalators are robots.

326
00:10:40,880 --> 00:10:44,240
>> I'm going to use the language of like

327
00:10:42,720 --> 00:10:46,560
for a robot [laughter] to be a robot, it

328
00:10:44,240 --> 00:10:48,399
has to be somewhat intelligent, right?

329
00:10:46,560 --> 00:10:50,959
Um and so dishwasher doesn't count as an

330
00:10:48,399 --> 00:10:53,360
appliance. Um a self-driving car does

331
00:10:50,959 --> 00:10:54,959
count as a robot, not just

332
00:10:53,360 --> 00:10:57,279
>> where's the border of intelligence for

333
00:10:54,959 --> 00:10:59,279
you? I I think like it's probably some

334
00:10:57,279 --> 00:11:00,560
level of generalization, right? It can

335
00:10:59,279 --> 00:11:01,920
work in different environments. It can

336
00:11:00,560 --> 00:11:04,079
work on different tasks. It can work on

337
00:11:01,920 --> 00:11:06,560
different objects. Otherwise,

338
00:11:04,079 --> 00:11:08,079
>> self-driving car is okay. Yeah, I don't

339
00:11:06,560 --> 00:11:09,440
know. I didn't have that complex of a

340
00:11:08,079 --> 00:11:11,360
definition. I just had it as like

341
00:11:09,440 --> 00:11:13,279
something that will do

342
00:11:11,360 --> 00:11:14,880
>> certain pre-programmed types of labor

343
00:11:13,279 --> 00:11:17,680
for you. But maybe that's maybe I have a

344
00:11:14,880 --> 00:11:19,680
better definition. Let me look up what

345
00:11:17,680 --> 00:11:21,839
definition of robot is. a machine

346
00:11:19,680 --> 00:11:24,160
capable of carrying out a complex series

347
00:11:21,839 --> 00:11:25,839
of actions automatically,

348
00:11:24,160 --> 00:11:27,279
especially when programmable by a

349
00:11:25,839 --> 00:11:28,560
computer. But you know, all these things

350
00:11:27,279 --> 00:11:30,160
have chips in them now. Your dishwasher

351
00:11:28,560 --> 00:11:30,880
has a chip in it, right? Or the computer

352
00:11:30,160 --> 00:11:33,680
in it.

353
00:11:30,880 --> 00:11:35,440
>> Okay. Yes. But like uh I would argue

354
00:11:33,680 --> 00:11:37,440
that robotics has not been an

355
00:11:35,440 --> 00:11:39,120
interesting area of innovation without

356
00:11:37,440 --> 00:11:41,600
intelligence. And so that's the relevant

357
00:11:39,120 --> 00:11:43,360
set for maybe you and me and many people

358
00:11:41,600 --> 00:11:44,800
that are looking for something that

359
00:11:43,360 --> 00:11:46,880
changes quickly.

360
00:11:44,800 --> 00:11:49,200
>> Yeah, that's cool. I mean, I do think

361
00:11:46,880 --> 00:11:51,680
that um on the on the on the topic of

362
00:11:49,200 --> 00:11:54,720
robots, the biggest trend perhaps or one

363
00:11:51,680 --> 00:11:56,160
of the biggest trends of 2026 100% will

364
00:11:54,720 --> 00:11:57,760
be that self-driving will really begin

365
00:11:56,160 --> 00:11:59,200
to matter and that'll be both in terms

366
00:11:57,760 --> 00:12:02,640
of your own car, it'll be in terms of

367
00:11:59,200 --> 00:12:03,839
Whimo and Tesla caps. It's going to be I

368
00:12:02,640 --> 00:12:05,519
think one of the big things that's

369
00:12:03,839 --> 00:12:06,800
talked about next year. So, I think I

370
00:12:05,519 --> 00:12:07,279
think on the robotics team that's the

371
00:12:06,800 --> 00:12:10,160
big E.

372
00:12:07,279 --> 00:12:12,399
>> I think if you um look at all of the

373
00:12:10,160 --> 00:12:15,040
potential use cases for robots besides

374
00:12:12,399 --> 00:12:16,480
self-driving and say like self-driving

375
00:12:15,040 --> 00:12:18,720
>> I mean the Optimus team actually proves

376
00:12:16,480 --> 00:12:20,800
this like if you take if you take a

377
00:12:18,720 --> 00:12:22,560
model that is powering Tesla

378
00:12:20,800 --> 00:12:24,160
self-driving and you put it in Optimus

379
00:12:22,560 --> 00:12:25,279
it can do locomotion but it can't do

380
00:12:24,160 --> 00:12:27,600
many other things and you still have to

381
00:12:25,279 --> 00:12:30,240
do the hardware right like manipulation.

382
00:12:27,600 --> 00:12:31,839
And so I think that the advantages here

383
00:12:30,240 --> 00:12:33,680
are not as strong as you believe they

384
00:12:31,839 --> 00:12:34,800
are. And like startups, some set of

385
00:12:33,680 --> 00:12:36,639
startups,

386
00:12:34,800 --> 00:12:37,839
>> the scariest competition is the Chinese,

387
00:12:36,639 --> 00:12:38,959
but I I do think that there is

388
00:12:37,839 --> 00:12:40,399
opportunity here.

389
00:12:38,959 --> 00:12:42,079
>> Oh, I totally think there's opportunity

390
00:12:40,399 --> 00:12:44,160
for startups. And don't misinterpret me.

391
00:12:42,079 --> 00:12:46,000
I just think that it's not just the fact

392
00:12:44,160 --> 00:12:47,279
that you have a model or a base model.

393
00:12:46,000 --> 00:12:48,720
You have the expertise to build the

394
00:12:47,279 --> 00:12:50,880
model, but then you also have all the

395
00:12:48,720 --> 00:12:52,320
supply chain. And I think that's really

396
00:12:50,880 --> 00:12:54,000
important because a lot of the same

397
00:12:52,320 --> 00:12:55,279
sensors that you need to use are there.

398
00:12:54,000 --> 00:12:57,120
and you know how you think about

399
00:12:55,279 --> 00:12:58,720
actually procuring and scaling things is

400
00:12:57,120 --> 00:13:00,240
there you know there's there's good

401
00:12:58,720 --> 00:13:01,440
overlap actually in terms of some of the

402
00:13:00,240 --> 00:13:02,880
other skill sets that are needed that

403
00:13:01,440 --> 00:13:04,320
take a long time to build usually at a

404
00:13:02,880 --> 00:13:06,000
startup or that are a little bit painful

405
00:13:04,320 --> 00:13:08,560
to build and people do it it's fine it's

406
00:13:06,000 --> 00:13:09,680
not I mean did it and SpaceX did it and

407
00:13:08,560 --> 00:13:12,320
you know all these companies have done

408
00:13:09,680 --> 00:13:13,920
it it's extra stuff so that makes sense

409
00:13:12,320 --> 00:13:15,600
I I do think I do think some startups

410
00:13:13,920 --> 00:13:18,320
will succeed here I just trying to think

411
00:13:15,600 --> 00:13:19,839
through you know besides the startups

412
00:13:18,320 --> 00:13:20,880
who's going to be big and then also I

413
00:13:19,839 --> 00:13:22,480
think there are one or two like

414
00:13:20,880 --> 00:13:24,240
incumbent slots that will just default

415
00:13:22,480 --> 00:13:26,160
happen unless something very strange

416
00:13:24,240 --> 00:13:27,279
happens and you know one could have

417
00:13:26,160 --> 00:13:28,800
argued that should have happened in

418
00:13:27,279 --> 00:13:30,320
foundation models where Google should

419
00:13:28,800 --> 00:13:33,440
have had a default slot in the end it

420
00:13:30,320 --> 00:13:35,040
did right it got there and I think that

421
00:13:33,440 --> 00:13:37,200
was very predictable that the Google

422
00:13:35,040 --> 00:13:38,480
models will get good I think I even may

423
00:13:37,200 --> 00:13:39,760
have wrote a post about this like two

424
00:13:38,480 --> 00:13:42,399
three years ago that Google will be

425
00:13:39,760 --> 00:13:44,880
relevant right because they just had all

426
00:13:42,399 --> 00:13:46,639
the assets that were needed for them to

427
00:13:44,880 --> 00:13:47,440
be a really important foundation model

428
00:13:46,639 --> 00:13:48,480
company they obviously invented

429
00:13:47,440 --> 00:13:49,760
transformers but they had all the data

430
00:13:48,480 --> 00:13:51,839
they had all the capital they had TPUs

431
00:13:49,760 --> 00:13:53,200
and GPUs had like the best people for

432
00:13:51,839 --> 00:13:56,480
all sorts of things or some of the

433
00:13:53,200 --> 00:13:57,680
people. So, um, it felt inevitable and I

434
00:13:56,480 --> 00:13:58,959
think this feels the same to me. That

435
00:13:57,680 --> 00:14:01,040
doesn't mean it's right. Do you want to

436
00:13:58,959 --> 00:14:02,000
talk about IP as an M&A next year? What

437
00:14:01,040 --> 00:14:04,560
do you think will happen there? I think

438
00:14:02,000 --> 00:14:07,360
that's another big that's theme number

439
00:14:04,560 --> 00:14:09,199
four, five, I guess. You know, three was

440
00:14:07,360 --> 00:14:10,639
different types of models, four was

441
00:14:09,199 --> 00:14:12,560
robots and self-driving, and then five

442
00:14:10,639 --> 00:14:15,519
would be IPOs and M&A. What do you

443
00:14:12,560 --> 00:14:17,600
think? More IPOs, less IPOs, more M&A,

444
00:14:15,519 --> 00:14:19,040
less M&A, different types of M&A?

445
00:14:17,600 --> 00:14:21,360
>> It depends on whether or not the bottom

446
00:14:19,040 --> 00:14:23,600
of falls out of the AI market at some

447
00:14:21,360 --> 00:14:24,800
point, right? But I think regardless,

448
00:14:23,600 --> 00:14:26,240
>> what do you mean by the what do you mean

449
00:14:24,800 --> 00:14:28,480
the bottom falls out? Like what what

450
00:14:26,240 --> 00:14:30,399
what does that translate into?

451
00:14:28,480 --> 00:14:32,720
>> Uh I think people just get skittish

452
00:14:30,399 --> 00:14:34,480
about you you know the cycle here is

453
00:14:32,720 --> 00:14:37,680
like what are people scared of? They are

454
00:14:34,480 --> 00:14:40,880
concerned that demand isn't real. no

455
00:14:37,680 --> 00:14:44,399
demand isn't real um for AI to support

456
00:14:40,880 --> 00:14:47,440
the capex cycle that there is systemic

457
00:14:44,399 --> 00:14:50,000
risk from people passing the ball around

458
00:14:47,440 --> 00:14:52,160
in terms of who is actually responsible

459
00:14:50,000 --> 00:14:55,360
for the capex buildout and these credit

460
00:14:52,160 --> 00:14:57,279
agreements right or um you know pay on

461
00:14:55,360 --> 00:15:00,160
delivery contracts for data centers and

462
00:14:57,279 --> 00:15:03,120
for chips what else are they afraid of

463
00:15:00,160 --> 00:15:05,440
they're afraid of like the

464
00:15:03,120 --> 00:15:06,720
>> microlastics aka like too much

465
00:15:05,440 --> 00:15:08,959
concentration

466
00:15:06,720 --> 00:15:10,480
in Nvidia and a small number of other

467
00:15:08,959 --> 00:15:13,040
players. If you're like a big public

468
00:15:10,480 --> 00:15:13,839
markets investor, you're just like, you

469
00:15:13,040 --> 00:15:15,920
know, you

470
00:15:13,839 --> 00:15:17,600
>> silicon, it's too much silicon.

471
00:15:15,920 --> 00:15:19,040
>> It's too much silicon. You're damned if

472
00:15:17,600 --> 00:15:21,839
you do, you're damned if you don't. I

473
00:15:19,040 --> 00:15:23,920
was talking to a friend of mine who runs

474
00:15:21,839 --> 00:15:26,320
a large tech hedge fund

475
00:15:23,920 --> 00:15:28,320
>> and they're already like a foundation

476
00:15:26,320 --> 00:15:30,000
model investor in like multiple

477
00:15:28,320 --> 00:15:32,240
significant labs that may or may not go

478
00:15:30,000 --> 00:15:33,839
public in the next couple years. Yeah.

479
00:15:32,240 --> 00:15:35,839
And they're like, "Okay, well the

480
00:15:33,839 --> 00:15:37,920
question is, do you buy the IPO?" Their

481
00:15:35,839 --> 00:15:39,680
game theory on it was like, "Actually,

482
00:15:37,920 --> 00:15:42,800
no matter what I think about it, I have

483
00:15:39,680 --> 00:15:44,240
to do it because retail will want it

484
00:15:42,800 --> 00:15:46,079
>> because they like want to be part of the

485
00:15:44,240 --> 00:15:48,079
AI revolution." And then if you're a

486
00:15:46,079 --> 00:15:50,560
hedge fun, you get benchmarked on annual

487
00:15:48,079 --> 00:15:52,639
performance and because of the retail

488
00:15:50,560 --> 00:15:54,959
pop and some set of investors wanting to

489
00:15:52,639 --> 00:15:56,639
buy into it as a pure play where you're

490
00:15:54,959 --> 00:15:59,199
like, "Oh, I can't miss it like I missed

491
00:15:56,639 --> 00:16:02,079
Nvidia." Then you have to buy it. And so

492
00:15:59,199 --> 00:16:03,920
his view was like you buy the IPO

493
00:16:02,079 --> 00:16:05,120
regardless of your fundamental view of

494
00:16:03,920 --> 00:16:06,639
the company. And I was like, "Wow, this

495
00:16:05,120 --> 00:16:07,199
is not the investing job I know how to

496
00:16:06,639 --> 00:16:08,560
do."

497
00:16:07,199 --> 00:16:09,519
>> What do you think happens?

498
00:16:08,560 --> 00:16:11,920
>> I think there'll definitely be a lot

499
00:16:09,519 --> 00:16:14,240
more IPOs next year. Um, I think if one

500
00:16:11,920 --> 00:16:16,720
of the main AI companies goes out, it'll

501
00:16:14,240 --> 00:16:17,839
be probably do extremely well depending

502
00:16:16,720 --> 00:16:19,040
where they price. I mean, they obviously

503
00:16:17,839 --> 00:16:20,320
if they're overly aggressive, it won't,

504
00:16:19,040 --> 00:16:21,920
but in general, I think there's so much

505
00:16:20,320 --> 00:16:24,560
retail appetite to actually participate

506
00:16:21,920 --> 00:16:26,480
in AI besides Nvidia. Um, and then

507
00:16:24,560 --> 00:16:28,399
that'll just get a lot of other people

508
00:16:26,480 --> 00:16:29,920
to go public to just followers on it.

509
00:16:28,399 --> 00:16:32,079
So, I I do expect there'll be a lot of

510
00:16:29,920 --> 00:16:33,839
them. It's just one that even goes out.

511
00:16:32,079 --> 00:16:35,199
Uh, and then also it's a great way to

512
00:16:33,839 --> 00:16:37,199
raise huge amounts of money for some of

513
00:16:35,199 --> 00:16:38,800
these labs potentially. So, um, it'll be

514
00:16:37,199 --> 00:16:42,639
interesting to watch what happens there.

515
00:16:38,800 --> 00:16:45,839
Any other predictions for 26? Yeah, I I

516
00:16:42,639 --> 00:16:48,639
uh I think that I did not believe that

517
00:16:45,839 --> 00:16:52,000
we were going to see that many like

518
00:16:48,639 --> 00:16:54,480
unique consumer experiences

519
00:16:52,000 --> 00:16:57,199
>> besides like chat GPT. I think we are

520
00:16:54,480 --> 00:16:59,360
going to see like a slate of consumer

521
00:16:57,199 --> 00:17:00,880
hardware that mostly fails, but I'm

522
00:16:59,360 --> 00:17:02,720
still openminded to it. And then

523
00:17:00,880 --> 00:17:04,880
definitely actually like it remains to

524
00:17:02,720 --> 00:17:08,480
me see if any of these scales, but I am

525
00:17:04,880 --> 00:17:11,600
seeing magical experiences of like

526
00:17:08,480 --> 00:17:13,760
really different consumer agent software

527
00:17:11,600 --> 00:17:16,720
that I like I actually want and will

528
00:17:13,760 --> 00:17:17,760
use. And I I think people are barely

529
00:17:16,720 --> 00:17:19,280
beginning to

530
00:17:17,760 --> 00:17:21,520
>> well I these companies are in stealth

531
00:17:19,280 --> 00:17:22,880
right now, but I I do think that like

532
00:17:21,520 --> 00:17:24,720
there's going to be a lot more product

533
00:17:22,880 --> 00:17:26,079
people that experiment with this and

534
00:17:24,720 --> 00:17:28,799
model companies that experiment with

535
00:17:26,079 --> 00:17:30,480
this next year. Um and so I'm I'm pretty

536
00:17:28,799 --> 00:17:32,720
optimistic about that. Yeah, I agree

537
00:17:30,480 --> 00:17:33,919
with that 100%. And I think um the big

538
00:17:32,720 --> 00:17:35,520
question is what will end up being a

539
00:17:33,919 --> 00:17:37,679
breakout startup and it'll undoubtedly

540
00:17:35,520 --> 00:17:38,799
be some and then what will be a startup

541
00:17:37,679 --> 00:17:40,400
that will grow really fast and then

542
00:17:38,799 --> 00:17:42,320
it'll get cop copied by the main

543
00:17:40,400 --> 00:17:44,160
lab/google and then it just gets

544
00:17:42,320 --> 00:17:45,760
incorporated into the core product. And

545
00:17:44,160 --> 00:17:47,360
the the interesting thing is unless a

546
00:17:45,760 --> 00:17:49,120
company truly hits escape velocity and

547
00:17:47,360 --> 00:17:51,120
build a network effect or something else

548
00:17:49,120 --> 00:17:52,480
that's really defensible, usually

549
00:17:51,120 --> 00:17:54,960
incumbents can launch two three years

550
00:17:52,480 --> 00:17:56,400
later and catch up. And so if they have

551
00:17:54,960 --> 00:17:58,320
the distribution and they have the core

552
00:17:56,400 --> 00:17:59,360
product and they have but you know to

553
00:17:58,320 --> 00:18:00,559
your point I think it's very exciting

554
00:17:59,360 --> 00:18:02,720
and I've been waiting for this for a

555
00:18:00,559 --> 00:18:04,799
while. I think two years ago, three

556
00:18:02,720 --> 00:18:08,080
years ago, um this guy David Song who

557
00:18:04,799 --> 00:18:09,600
was on my team at the time ran a two

558
00:18:08,080 --> 00:18:12,160
quarter thing at Stanford where we had

559
00:18:09,600 --> 00:18:14,240
different game supply uh from the

560
00:18:12,160 --> 00:18:16,799
engineering programs there and it was

561
00:18:14,240 --> 00:18:19,120
like groups of people building consumer

562
00:18:16,799 --> 00:18:20,480
apps using AI because we said this wave

563
00:18:19,120 --> 00:18:22,880
of AI is so fascinating why didn't

564
00:18:20,480 --> 00:18:25,520
anybody building anything consumer so we

565
00:18:22,880 --> 00:18:28,880
basically just gave people free GPU to

566
00:18:25,520 --> 00:18:30,320
go and try stuff and there was no like

567
00:18:28,880 --> 00:18:31,760
obligation on their side to do anything

568
00:18:30,320 --> 00:18:33,600
with it you you know, in terms of us

569
00:18:31,760 --> 00:18:35,440
getting involved. It was just you go do

570
00:18:33,600 --> 00:18:37,600
cool stuff cuz this is such a good

571
00:18:35,440 --> 00:18:39,840
playground and it was really neat

572
00:18:37,600 --> 00:18:41,520
experiences that were being prototyped

573
00:18:39,840 --> 00:18:44,320
and then I was just shocked that nothing

574
00:18:41,520 --> 00:18:45,679
happened for a couple years in terms of

575
00:18:44,320 --> 00:18:46,880
you know really interesting consumer

576
00:18:45,679 --> 00:18:48,480
products. So I agree with you there's so

577
00:18:46,880 --> 00:18:49,679
much room for that and I always wonder

578
00:18:48,480 --> 00:18:51,280
is it because there's a different

579
00:18:49,679 --> 00:18:53,840
generation of founders who don't want to

580
00:18:51,280 --> 00:18:55,200
work on consumer or who've forgotten how

581
00:18:53,840 --> 00:18:58,000
because you know the big consumer

582
00:18:55,200 --> 00:19:00,240
companies have kind of aged out. Is it

583
00:18:58,000 --> 00:19:01,919
the incumbents are just too scary? Is it

584
00:19:00,240 --> 00:19:03,360
like what why is there so little

585
00:19:01,919 --> 00:19:05,760
innovation actually on the consumer side

586
00:19:03,360 --> 00:19:07,440
of AI? I still don't quite understand

587
00:19:05,760 --> 00:19:09,840
what the issue is.

588
00:19:07,440 --> 00:19:11,840
>> I Okay, let's let's like list the

589
00:19:09,840 --> 00:19:14,160
reasons. I do think that the incumbents

590
00:19:11,840 --> 00:19:15,840
are pretty scary. Um and anybody who was

591
00:19:14,160 --> 00:19:18,240
around for the last generation of

592
00:19:15,840 --> 00:19:20,240
interesting consumer ideas saw actually

593
00:19:18,240 --> 00:19:22,240
the ingestion of those ideas into the

594
00:19:20,240 --> 00:19:22,720
existing platform as you put out.

595
00:19:22,240 --> 00:19:25,200
>> Yeah.

596
00:19:22,720 --> 00:19:28,400
>> So there's that. I also think like the

597
00:19:25,200 --> 00:19:31,360
first instinct that that I've seen from

598
00:19:28,400 --> 00:19:33,280
companies uh from founders working on

599
00:19:31,360 --> 00:19:34,480
like new consumer experiences is

600
00:19:33,280 --> 00:19:36,880
essentially building like better

601
00:19:34,480 --> 00:19:38,320
versions of like last generation

602
00:19:36,880 --> 00:19:40,080
experiences with this generation

603
00:19:38,320 --> 00:19:41,760
technology and it ends up like not being

604
00:19:40,080 --> 00:19:43,840
that interesting. And so I actually

605
00:19:41,760 --> 00:19:46,559
think you have to be like either quite

606
00:19:43,840 --> 00:19:48,480
close to research or pretty creatively

607
00:19:46,559 --> 00:19:50,803
ambitious to build like something very

608
00:19:48,480 --> 00:19:52,480
different that has any chance. And so

609
00:19:50,803 --> 00:19:53,919
[clears throat] I think I think like

610
00:19:52,480 --> 00:19:55,600
there's just not that many people who

611
00:19:53,919 --> 00:19:57,360
have had that experience set or that

612
00:19:55,600 --> 00:19:58,000
creativity and now we're going to see

613
00:19:57,360 --> 00:19:59,440
it.

614
00:19:58,000 --> 00:20:02,240
>> Yeah, I think it's pretty exciting. The

615
00:19:59,440 --> 00:20:04,559
other thing is um I was talking to a

616
00:20:02,240 --> 00:20:07,039
really well-known consumer founder who's

617
00:20:04,559 --> 00:20:09,440
running you know a giant public company

618
00:20:07,039 --> 00:20:12,559
and his view is that perhaps in the

619
00:20:09,440 --> 00:20:15,600
entire world there's a few hundred great

620
00:20:12,559 --> 00:20:16,799
product people for consumer at least in

621
00:20:15,600 --> 00:20:17,919
terms of who are actually working on it.

622
00:20:16,799 --> 00:20:19,840
Obviously there's enormous human

623
00:20:17,919 --> 00:20:21,360
potential and people who aren't working

624
00:20:19,840 --> 00:20:22,720
in consumer products could and you know

625
00:20:21,360 --> 00:20:23,919
but of the people working consumer

626
00:20:22,720 --> 00:20:25,679
products he thinks at most there's a few

627
00:20:23,919 --> 00:20:27,200
hundred people who are exceptional who

628
00:20:25,679 --> 00:20:28,159
could actually come up with and launch

629
00:20:27,200 --> 00:20:30,640
their own product that would be

630
00:20:28,159 --> 00:20:32,080
interesting or good and so you could

631
00:20:30,640 --> 00:20:33,440
also just say say that maybe there's

632
00:20:32,080 --> 00:20:35,280
just a limitation on how many of these

633
00:20:33,440 --> 00:20:37,280
things can exist just given human

634
00:20:35,280 --> 00:20:38,559
potential within the set of people who

635
00:20:37,280 --> 00:20:40,080
are already doing it which I think is

636
00:20:38,559 --> 00:20:41,120
kind of an interesting argument I don't

637
00:20:40,080 --> 00:20:42,880
know if I agree with it but I thought it

638
00:20:41,120 --> 00:20:45,360
was an interesting argument that he made

639
00:20:42,880 --> 00:20:48,320
>> I would limit myself to that number if

640
00:20:45,360 --> 00:20:50,400
it it's also the set people who like

641
00:20:48,320 --> 00:20:52,320
have the context of like what is

642
00:20:50,400 --> 00:20:53,760
possible now.

643
00:20:52,320 --> 00:20:55,520
>> If you've got great consumer product

644
00:20:53,760 --> 00:20:58,000
instinct, but you're like work you're

645
00:20:55,520 --> 00:21:02,240
like grinding away on the like 50th

646
00:20:58,000 --> 00:21:04,080
iteration of an existing product like

647
00:21:02,240 --> 00:21:05,760
>> Yeah. Yeah. You're working on the the

648
00:21:04,080 --> 00:21:06,799
the little sub button in Gmail or

649
00:21:05,760 --> 00:21:08,000
whatever instead of actually going off

650
00:21:06,799 --> 00:21:08,400
and doing this 100%.

651
00:21:08,000 --> 00:21:10,159
>> Yeah.

652
00:21:08,400 --> 00:21:12,559
>> Cool. Anything else we should talk about

653
00:21:10,159 --> 00:21:15,440
or any other big predictions for 26? I

654
00:21:12,559 --> 00:21:17,919
feel like a very big um emergent thing

655
00:21:15,440 --> 00:21:21,280
that happened this year was the

656
00:21:17,919 --> 00:21:22,799
surprising funding of like Neolabs like

657
00:21:21,280 --> 00:21:23,919
three through eight. What do you think

658
00:21:22,799 --> 00:21:27,200
of that? What do you think about

659
00:21:23,919 --> 00:21:29,679
alternative architectures? Like do you

660
00:21:27,200 --> 00:21:32,159
have any point of view on um all of the

661
00:21:29,679 --> 00:21:34,320
effort around like getting reinforcement

662
00:21:32,159 --> 00:21:35,520
learning to be more general continual

663
00:21:34,320 --> 00:21:36,480
learning? Uh some of the research

664
00:21:35,520 --> 00:21:37,760
directions

665
00:21:36,480 --> 00:21:39,280
>> you know I think there's enormous

666
00:21:37,760 --> 00:21:41,440
amounts of really interesting research

667
00:21:39,280 --> 00:21:42,960
being done. So I, you know, there's a

668
00:21:41,440 --> 00:21:45,039
lot of juice to be squeezed out of these

669
00:21:42,960 --> 00:21:46,559
models still in different ways and I

670
00:21:45,039 --> 00:21:48,320
think that's really exciting. Well,

671
00:21:46,559 --> 00:21:50,080
ultimately these things become capital

672
00:21:48,320 --> 00:21:52,400
gains for certain types of approaches or

673
00:21:50,080 --> 00:21:53,679
models because we know scale really

674
00:21:52,400 --> 00:21:55,039
matters which means that eventually you

675
00:21:53,679 --> 00:21:56,559
have to have collapse into a handful of

676
00:21:55,039 --> 00:21:58,480
players because capital will aggregate

677
00:21:56,559 --> 00:21:59,840
to things that are working the most.

678
00:21:58,480 --> 00:22:01,360
They're generating revenue and so then

679
00:21:59,840 --> 00:22:03,600
the question is what are those things?

680
00:22:01,360 --> 00:22:05,200
At what point do things just get kind of

681
00:22:03,600 --> 00:22:06,240
locked in from a usage perspective for

682
00:22:05,200 --> 00:22:07,600
whatever reason? And there's all sorts

683
00:22:06,240 --> 00:22:10,640
of ways you can imagine this being built

684
00:22:07,600 --> 00:22:12,159
over time against some of the models. So

685
00:22:10,640 --> 00:22:13,919
I think it's interesting. I think it's

686
00:22:12,159 --> 00:22:14,640
exciting. I think we'll see how it plays

687
00:22:13,919 --> 00:22:17,200
out.

688
00:22:14,640 --> 00:22:20,799
>> I think to articulate what like the the

689
00:22:17,200 --> 00:22:22,960
arguments could be for, you know, new

690
00:22:20,799 --> 00:22:24,080
research directions is like Ilia, you

691
00:22:22,960 --> 00:22:25,520
know, did this interview [clears throat]

692
00:22:24,080 --> 00:22:28,240
recently where he describes it as the

693
00:22:25,520 --> 00:22:31,200
age of research. And to to paraphrase,

694
00:22:28,240 --> 00:22:33,600
he like basically says that yes, I

695
00:22:31,200 --> 00:22:36,000
believe in scaling of course, but you

696
00:22:33,600 --> 00:22:39,840
know, there's there's some

697
00:22:36,000 --> 00:22:42,480
floor of compute that is not infinite

698
00:22:39,840 --> 00:22:43,760
where we can test ideas at scale. And

699
00:22:42,480 --> 00:22:47,840
then if we have [clears throat] let's

700
00:22:43,760 --> 00:22:50,320
say secret ideas around like how to get

701
00:22:47,840 --> 00:22:52,000
to more rapid or more compute efficient

702
00:22:50,320 --> 00:22:54,559
improvement then it actually isn't just

703
00:22:52,000 --> 00:22:56,720
a straight resource battle which like

704
00:22:54,559 --> 00:23:00,320
the rat race does feel a little bit like

705
00:22:56,720 --> 00:23:03,039
today. Um, I think the other argument

706
00:23:00,320 --> 00:23:04,559
you you could take is actually like

707
00:23:03,039 --> 00:23:05,919
multiple architectures and people have

708
00:23:04,559 --> 00:23:08,880
done some research on this, but multiple

709
00:23:05,919 --> 00:23:13,360
architectures are really relevant at big

710
00:23:08,880 --> 00:23:15,440
domains of of um usefulness. They just

711
00:23:13,360 --> 00:23:17,200
haven't been scaled, right? And like

712
00:23:15,440 --> 00:23:20,559
there's enough capital out there to test

713
00:23:17,200 --> 00:23:21,840
them, be they like diffusion or um SSMs

714
00:23:20,559 --> 00:23:23,440
or whatever. And that's going to happen

715
00:23:21,840 --> 00:23:25,919
this next year. And then I think there's

716
00:23:23,440 --> 00:23:28,240
like a like a resource focus argument,

717
00:23:25,919 --> 00:23:29,919
right? If Ilia is describing that some

718
00:23:28,240 --> 00:23:31,120
set of labs they have an enormous amount

719
00:23:29,919 --> 00:23:33,039
of compute but they have to spend a lot

720
00:23:31,120 --> 00:23:35,360
of that compute on inference today then

721
00:23:33,039 --> 00:23:39,039
how much do you spend on your particular

722
00:23:35,360 --> 00:23:41,200
research direction uh be it

723
00:23:39,039 --> 00:23:43,280
self-improvement or post- training or

724
00:23:41,200 --> 00:23:45,280
emotional intelligence or very large

725
00:23:43,280 --> 00:23:46,240
scale out agent stuff.

726
00:23:45,280 --> 00:23:47,760
>> Yeah, it depends on what you're doing

727
00:23:46,240 --> 00:23:50,400
because the inference is what ends up

728
00:23:47,760 --> 00:23:51,360
then uh raising you money to pay for

729
00:23:50,400 --> 00:23:54,240
everything else because you're

730
00:23:51,360 --> 00:23:56,080
generating revenue. So I think uh sure

731
00:23:54,240 --> 00:23:58,080
that it's effectively your way to

732
00:23:56,080 --> 00:24:00,799
bootstrap into more and more scales. So,

733
00:23:58,080 --> 00:24:01,760
I always thought perhaps incorrectly. I

734
00:24:00,799 --> 00:24:03,679
I actually probably think it's

735
00:24:01,760 --> 00:24:05,840
incorrect, but I always thought that

736
00:24:03,679 --> 00:24:09,039
eventually you end up with evolutionary

737
00:24:05,840 --> 00:24:11,279
systems is really how you build AI

738
00:24:09,039 --> 00:24:12,880
because and maybe I'm overextulating up

739
00:24:11,279 --> 00:24:14,400
a biology where you know effectively

740
00:24:12,880 --> 00:24:15,919
your brain has a series of modules that

741
00:24:14,400 --> 00:24:19,440
have different functions or tasks,

742
00:24:15,919 --> 00:24:22,159
right? You have a visual system that's

743
00:24:19,440 --> 00:24:24,880
um you know highly sort of pre-wired to

744
00:24:22,159 --> 00:24:26,640
deal with vision really effectively. You

745
00:24:24,880 --> 00:24:28,480
have uh different areas of high pier

746
00:24:26,640 --> 00:24:30,720
thought and learning. You have memory.

747
00:24:28,480 --> 00:24:32,240
You have uh mirror neurons that are

748
00:24:30,720 --> 00:24:34,240
involved with empathy, right? Your brain

749
00:24:32,240 --> 00:24:35,360
is actually very um specialized in some

750
00:24:34,240 --> 00:24:36,640
ways. Although obviously there's people

751
00:24:35,360 --> 00:24:39,679
who are born with literally like half a

752
00:24:36,640 --> 00:24:40,640
brain hemisphere and the brain rewires

753
00:24:39,679 --> 00:24:42,640
and sort of covers all the

754
00:24:40,640 --> 00:24:46,000
functionality. But um there's a few

755
00:24:42,640 --> 00:24:49,279
famous cases like that. Uh but you know

756
00:24:46,000 --> 00:24:50,640
fundamentally um you have a lot of stuff

757
00:24:49,279 --> 00:24:52,640
that evolves into very specialized

758
00:24:50,640 --> 00:24:56,159
tasks. It's almost like ae or something,

759
00:24:52,640 --> 00:24:57,840
you know. And the question is the degree

760
00:24:56,159 --> 00:25:00,080
to which you recapitulate that as you're

761
00:24:57,840 --> 00:25:02,640
doing further development of AI. And

762
00:25:00,080 --> 00:25:04,400
when do you start just spawning off a

763
00:25:02,640 --> 00:25:06,240
bunch of instances of something and just

764
00:25:04,400 --> 00:25:08,159
have some utility function evolving

765
00:25:06,240 --> 00:25:09,679
against that you then have some

766
00:25:08,159 --> 00:25:11,200
selection and recombining and all the

767
00:25:09,679 --> 00:25:13,039
other stuff that you kind of do to to

768
00:25:11,200 --> 00:25:14,880
try and make some of that work versus

769
00:25:13,039 --> 00:25:16,320
how much of it is a more analytical

770
00:25:14,880 --> 00:25:19,120
approach or a more experimental and

771
00:25:16,320 --> 00:25:21,200
iterative approach or you know so it's

772
00:25:19,120 --> 00:25:22,799
or in a directed way. And so I think

773
00:25:21,200 --> 00:25:25,360
it's really interesting to ask cuz if

774
00:25:22,799 --> 00:25:27,279
you look again at biology as a as a

775
00:25:25,360 --> 00:25:30,799
potential precedent although maybe a

776
00:25:27,279 --> 00:25:33,039
very bad one. You look at protein design

777
00:25:30,799 --> 00:25:34,799
and for a long time there are these like

778
00:25:33,039 --> 00:25:36,000
super analytically designed proteins and

779
00:25:34,799 --> 00:25:39,120
then they came up with all these systems

780
00:25:36,000 --> 00:25:41,520
of this you know like phase display

781
00:25:39,120 --> 00:25:42,480
and like mutagenic scans and all sorts

782
00:25:41,520 --> 00:25:43,760
of things that give you dramatically

783
00:25:42,480 --> 00:25:46,159
better results than if you just sat and

784
00:25:43,760 --> 00:25:48,320
thought about it. And now of course we

785
00:25:46,159 --> 00:25:50,640
kind of solved it with AI where you have

786
00:25:48,320 --> 00:25:53,200
um all these 3D structural prediction

787
00:25:50,640 --> 00:25:55,520
that are actually very good right that

788
00:25:53,200 --> 00:25:56,799
that was um alpha fold and a few other

789
00:25:55,520 --> 00:25:59,039
things that really were breakthroughs

790
00:25:56,799 --> 00:26:01,200
there. So it feels like in the context

791
00:25:59,039 --> 00:26:02,559
of AI maybe eventually we end up there

792
00:26:01,200 --> 00:26:05,360
as well right where you just involve

793
00:26:02,559 --> 00:26:07,120
these systems and then that may be a

794
00:26:05,360 --> 00:26:09,120
very different type of approach and

795
00:26:07,120 --> 00:26:10,559
training and you know that that that may

796
00:26:09,120 --> 00:26:12,480
be where I think things really have a

797
00:26:10,559 --> 00:26:14,080
interesting break and that's one of the

798
00:26:12,480 --> 00:26:15,840
reasons arguably people are so focused

799
00:26:14,080 --> 00:26:17,360
on code because code is arguably a

800
00:26:15,840 --> 00:26:20,000
bootstrap into moving faster on

801
00:26:17,360 --> 00:26:22,400
development of AGI but I think it's kind

802
00:26:20,000 --> 00:26:24,720
of code plus self-evolution is really

803
00:26:22,400 --> 00:26:26,559
the the potential really interesting

804
00:26:24,720 --> 00:26:27,919
approach to it to to get some really

805
00:26:26,559 --> 00:26:28,640
fast lift off but Maybe not, right?

806
00:26:27,919 --> 00:26:32,640
We'll see.

807
00:26:28,640 --> 00:26:35,039
>> What is um the one prediction you have

808
00:26:32,640 --> 00:26:36,908
for 26 that has nothing to do with AI?

809
00:26:35,039 --> 00:26:38,400
>> Do you think about anything else, Sarah?

810
00:26:36,908 --> 00:26:39,360
[laughter]

811
00:26:38,400 --> 00:26:40,960
>> I do.

812
00:26:39,360 --> 00:26:42,880
>> I'm joking.

813
00:26:40,960 --> 00:26:44,400
>> Really?

814
00:26:42,880 --> 00:26:45,760
>> I mean, the other thing, by the way, one

815
00:26:44,400 --> 00:26:49,279
other prediction that does have to do

816
00:26:45,760 --> 00:26:50,960
with AI is I do think um defense will

817
00:26:49,279 --> 00:26:53,600
accelerate in terms of startups and

818
00:26:50,960 --> 00:26:55,279
defense tech and the shift to autonomous

819
00:26:53,600 --> 00:26:57,840
or not autonomous but to drone based

820
00:26:55,279 --> 00:26:59,440
systems in general. a massive reworking

821
00:26:57,840 --> 00:27:00,720
of how you think about war and defense

822
00:26:59,440 --> 00:27:02,960
and I think that's going to be a shoot

823
00:27:00,720 --> 00:27:04,480
shift that we'll see go even faster this

824
00:27:02,960 --> 00:27:06,159
coming year I think this is accelerating

825
00:27:04,480 --> 00:27:07,360
in part to you know how the Trump

826
00:27:06,159 --> 00:27:08,640
administration has been approaching it

827
00:27:07,360 --> 00:27:10,320
and the secretary of war and everybody

828
00:27:08,640 --> 00:27:11,600
there have been thinking about it but I

829
00:27:10,320 --> 00:27:12,880
think in part just you have enough

830
00:27:11,600 --> 00:27:14,080
density now of startups doing

831
00:27:12,880 --> 00:27:15,360
interesting things so I think that's the

832
00:27:14,080 --> 00:27:17,840
other thing that's like a huge shift

833
00:27:15,360 --> 00:27:19,440
that you know it's a hype cycle right

834
00:27:17,840 --> 00:27:20,799
now and I actually think again it's a

835
00:27:19,440 --> 00:27:23,200
little bit under thought about because

836
00:27:20,799 --> 00:27:24,720
it's it's going to be so big um outside

837
00:27:23,200 --> 00:27:25,919
of AI I mean I think there's obvious

838
00:27:24,720 --> 00:27:28,799
really interesting things happening in

839
00:27:25,919 --> 00:27:30,640
space SpaceX and Starlink and I think

840
00:27:28,799 --> 00:27:32,320
about communications and telefan that's

841
00:27:30,640 --> 00:27:33,919
a big shift. There's really interesting

842
00:27:32,320 --> 00:27:35,520
things in my opinion happening in energy

843
00:27:33,919 --> 00:27:37,279
and mining and you know I I think

844
00:27:35,520 --> 00:27:40,159
there's a lot going on in the world.

845
00:27:37,279 --> 00:27:43,120
>> I agree on defense

846
00:27:40,159 --> 00:27:45,039
with some like concern that you know we

847
00:27:43,120 --> 00:27:47,919
have to wait for budget to actually

848
00:27:45,039 --> 00:27:50,159
shift from contracts to primes to some

849
00:27:47,919 --> 00:27:52,799
of these new companies at scale. But the

850
00:27:50,159 --> 00:27:54,399
demand like the need to be competitive

851
00:27:52,799 --> 00:27:56,080
in a world that's increasingly

852
00:27:54,399 --> 00:27:59,120
autonomydriven

853
00:27:56,080 --> 00:28:01,760
um is like so obvious right and I think

854
00:27:59,120 --> 00:28:04,080
you know hype cycles and booms are good

855
00:28:01,760 --> 00:28:06,399
in that they bring a lot of people to

856
00:28:04,080 --> 00:28:08,320
the table you know capital

857
00:28:06,399 --> 00:28:10,640
>> founders people who want to work in the

858
00:28:08,320 --> 00:28:12,159
industry um and so you can make a lot of

859
00:28:10,640 --> 00:28:14,960
progress in a quick amount of time even

860
00:28:12,159 --> 00:28:17,440
if a lot of companies die

861
00:28:14,960 --> 00:28:19,120
>> and there's there's um more enthusiasm a

862
00:28:17,440 --> 00:28:21,200
very short period of time so I agree

863
00:28:19,120 --> 00:28:23,039
with that. And I also don't think that's

864
00:28:21,200 --> 00:28:25,200
necessarily bad, right? I

865
00:28:23,039 --> 00:28:27,039
>> What's your high prediction?

866
00:28:25,200 --> 00:28:28,259
>> I think that like I'm not the only one,

867
00:28:27,039 --> 00:28:31,679
but I think that the like

868
00:28:28,259 --> 00:28:34,000
[clears throat] GLP1 thing is just

869
00:28:31,679 --> 00:28:36,720
>> despite all of the enthusiasm, like

870
00:28:34,000 --> 00:28:39,679
still underrated for how much impact it

871
00:28:36,720 --> 00:28:42,880
is having, right? And so I think that

872
00:28:39,679 --> 00:28:44,960
the continual adoption of these is like

873
00:28:42,880 --> 00:28:48,399
inexurable. I actually think it creates

874
00:28:44,960 --> 00:28:51,440
a path that is interesting for like

875
00:28:48,399 --> 00:28:53,679
other peptide and hormone therapies.

876
00:28:51,440 --> 00:28:55,840
>> I think the fact that it has been so

877
00:28:53,679 --> 00:28:58,880
effective has like lots of second order

878
00:28:55,840 --> 00:29:01,840
effects both from people way like just

879
00:28:58,880 --> 00:29:04,880
being a lot less overweight like

880
00:29:01,840 --> 00:29:07,520
directly and the willingness to look at

881
00:29:04,880 --> 00:29:09,279
other engineered peptides or like I

882
00:29:07,520 --> 00:29:11,200
think it like everybody understands now

883
00:29:09,279 --> 00:29:13,039
that like

884
00:29:11,200 --> 00:29:15,039
>> delivery matters. there are these really

885
00:29:13,039 --> 00:29:17,120
incredible medicines and I think that

886
00:29:15,039 --> 00:29:20,240
the impact of that is going to like fuel

887
00:29:17,120 --> 00:29:21,919
much more investment in um anything that

888
00:29:20,240 --> 00:29:23,600
looks like that type of opportunity and

889
00:29:21,919 --> 00:29:25,360
so I think that's exciting.

890
00:29:23,600 --> 00:29:27,200
Yeah, I actually think um one thing that

891
00:29:25,360 --> 00:29:28,320
you mentioned is really interesting

892
00:29:27,200 --> 00:29:30,880
where if you look at the sort of

893
00:29:28,320 --> 00:29:32,480
biohacking community, there's a lot of

894
00:29:30,880 --> 00:29:33,919
peptide use now of different you know

895
00:29:32,480 --> 00:29:36,080
different peptides that will do

896
00:29:33,919 --> 00:29:37,520
different things in terms of you know

897
00:29:36,080 --> 00:29:38,960
somebody will have some chronic corporal

898
00:29:37,520 --> 00:29:40,640
cheerle thing and they'll fly to Dubai

899
00:29:38,960 --> 00:29:43,039
to get you know peptides injected or

900
00:29:40,640 --> 00:29:44,799
whatever and usually those are sort of

901
00:29:43,039 --> 00:29:46,880
early indicators of potential larger

902
00:29:44,799 --> 00:29:48,080
scale adoption society

903
00:29:46,880 --> 00:29:49,520
>> and so I think that's a really

904
00:29:48,080 --> 00:29:51,279
interesting trend right now in general

905
00:29:49,520 --> 00:29:53,919
like this whole like um world of

906
00:29:51,279 --> 00:29:56,240
peptides and their uses. and is there a

907
00:29:53,919 --> 00:29:57,440
hymns of peptides like what's the what's

908
00:29:56,240 --> 00:29:58,159
coming there so I think that's super

909
00:29:57,440 --> 00:30:00,559
interesting you know

910
00:29:58,159 --> 00:30:03,600
>> I also think like the biohacking

911
00:30:00,559 --> 00:30:06,960
community as you said it like the set of

912
00:30:03,600 --> 00:30:11,039
people who were really really early off

913
00:30:06,960 --> 00:30:14,240
label GLP-1 adopters um interested in

914
00:30:11,039 --> 00:30:17,039
longevity neurom modulation with

915
00:30:14,240 --> 00:30:19,440
ultrasound um stem cell injection for

916
00:30:17,039 --> 00:30:20,960
example like that has been like a fringe

917
00:30:19,440 --> 00:30:23,520
small community

918
00:30:20,960 --> 00:30:24,640
>> and I think that like I think it's going

919
00:30:23,520 --> 00:30:25,840
to get less French.

920
00:30:24,640 --> 00:30:27,200
>> Uh and a lot of these things

921
00:30:25,840 --> 00:30:28,480
traditionally 10 years ago came out of

922
00:30:27,200 --> 00:30:30,159
the bodybuilding community, right? The

923
00:30:28,480 --> 00:30:31,279
bodybuilding community was like creatine

924
00:30:30,159 --> 00:30:33,760
and all these things that are more

925
00:30:31,279 --> 00:30:35,520
broadly used now, but also other other

926
00:30:33,760 --> 00:30:37,360
things for sleep aids or other, you

927
00:30:35,520 --> 00:30:39,440
know, magnesium and all this stuff.

928
00:30:37,360 --> 00:30:41,039
>> And to round out this year-end episode,

929
00:30:39,440 --> 00:30:43,520
we've asked some of our friends for

930
00:30:41,039 --> 00:30:47,760
their predictions for 2026. I'm so

931
00:30:43,520 --> 00:30:51,440
curious. My prediction for next year is

932
00:30:47,760 --> 00:30:54,799
that uh the reasoning

933
00:30:51,440 --> 00:30:59,039
uh systems are going to translate

934
00:30:54,799 --> 00:31:01,440
directly uh to AIS that are much much

935
00:30:59,039 --> 00:31:04,000
more versatile, much much more robust

936
00:31:01,440 --> 00:31:06,640
and reasoning is going to impact is

937
00:31:04,000 --> 00:31:08,880
going to revolutionize not just not just

938
00:31:06,640 --> 00:31:11,360
language models but reasoning is going

939
00:31:08,880 --> 00:31:15,440
to impact every single industry from

940
00:31:11,360 --> 00:31:18,000
biology to uh self-driving cars to

941
00:31:15,440 --> 00:31:22,159
robotics. And so reasoning, I think, is

942
00:31:18,000 --> 00:31:24,320
is the big huge breakthrough that that

943
00:31:22,159 --> 00:31:26,399
um is going to transform a lot of

944
00:31:24,320 --> 00:31:29,679
different applications and industries.

945
00:31:26,399 --> 00:31:32,880
In 2026, AI will stop being a reactive

946
00:31:29,679 --> 00:31:35,840
tool that waits for us to prompt it.

947
00:31:32,880 --> 00:31:37,840
Instead, it will become very proactive

948
00:31:35,840 --> 00:31:41,519
and get deeply integrated in our work

949
00:31:37,840 --> 00:31:45,120
life. It'll go where we go, hear what we

950
00:31:41,519 --> 00:31:48,240
hear, know what tasks we need to work

951
00:31:45,120 --> 00:31:50,480
on, and in fact, most of the times

952
00:31:48,240 --> 00:31:53,600
complete those for us before we even ask

953
00:31:50,480 --> 00:31:56,159
it to do so. It'll be our coach that

954
00:31:53,600 --> 00:31:58,080
helps us improve our skills. It'll be

955
00:31:56,159 --> 00:32:01,519
our manager who helps us prioritize our

956
00:31:58,080 --> 00:32:03,039
work and manage our time. In short, it's

957
00:32:01,519 --> 00:32:05,200
going to be the best work companion we

958
00:32:03,039 --> 00:32:07,440
could wish for. I think the main AI

959
00:32:05,200 --> 00:32:09,039
prediction that I have for next year is

960
00:32:07,440 --> 00:32:10,320
I think context is just going to be the

961
00:32:09,039 --> 00:32:12,880
most important part of every single

962
00:32:10,320 --> 00:32:14,720
product. And honestly, like one of the

963
00:32:12,880 --> 00:32:17,679
best experiences I've had with it so far

964
00:32:14,720 --> 00:32:19,360
is just memory and chatbt. Like I think

965
00:32:17,679 --> 00:32:22,000
that there are going to be a lot more

966
00:32:19,360 --> 00:32:24,640
features that basically

967
00:32:22,000 --> 00:32:27,120
their goal is to extract the user intent

968
00:32:24,640 --> 00:32:28,960
and make the onus less on the user to

969
00:32:27,120 --> 00:32:30,960
basically give all of the models or the

970
00:32:28,960 --> 00:32:33,120
system or the product more and more

971
00:32:30,960 --> 00:32:35,279
context. So in other words, how do you

972
00:32:33,120 --> 00:32:38,240
put the onus on the product to actually

973
00:32:35,279 --> 00:32:40,480
extract that from the user instead of

974
00:32:38,240 --> 00:32:41,440
the user having to do all of the work to

975
00:32:40,480 --> 00:32:44,559
do this up front?

976
00:32:41,440 --> 00:32:46,559
>> My prediction for 2026 is there will be

977
00:32:44,559 --> 00:32:50,000
a whole new suite of product experiences

978
00:32:46,559 --> 00:32:52,720
that run on much faster inference.

979
00:32:50,000 --> 00:32:54,399
>> My prediction for 2026 is that we'll

980
00:32:52,720 --> 00:32:56,960
finally stop copy pasting stuff into

981
00:32:54,399 --> 00:32:59,039
chat boxes. Instead, I think we're going

982
00:32:56,960 --> 00:33:00,880
to have applications that have better

983
00:32:59,039 --> 00:33:02,640
use of screen sharing and context

984
00:33:00,880 --> 00:33:03,840
management across the sources that

985
00:33:02,640 --> 00:33:06,799
matter the most.

986
00:33:03,840 --> 00:33:08,799
>> One prediction for 2026, there's so much

987
00:33:06,799 --> 00:33:10,799
talk of agents right now and there has

988
00:33:08,799 --> 00:33:13,279
been for a while, but no one has truly

989
00:33:10,799 --> 00:33:15,360
created a mass scale consumer agentic

990
00:33:13,279 --> 00:33:17,840
AI. I think the models are there today

991
00:33:15,360 --> 00:33:19,360
for this to be possible. And in 2026, we

992
00:33:17,840 --> 00:33:21,200
will see the group that figures out the

993
00:33:19,360 --> 00:33:23,039
right interface and system and product

994
00:33:21,200 --> 00:33:24,880
that creates as big a step function and

995
00:33:23,039 --> 00:33:26,720
overall experience as chat did when it

996
00:33:24,880 --> 00:33:28,480
first came out. And I think this area is

997
00:33:26,720 --> 00:33:30,240
not nearly as seated to the labs as

998
00:33:28,480 --> 00:33:32,640
people assume. It really is anyone's

999
00:33:30,240 --> 00:33:34,640
ball game. Hello, Aaron here. First of

1000
00:33:32,640 --> 00:33:37,120
all, I get quite awkward around doing

1001
00:33:34,640 --> 00:33:39,679
selfie videos. This is my ninth take of

1002
00:33:37,120 --> 00:33:43,039
this video. Um so I hope it goes okay

1003
00:33:39,679 --> 00:33:44,799
but uh 2026 prediction would be that uh

1004
00:33:43,039 --> 00:33:47,919
this is going to be certainly the

1005
00:33:44,799 --> 00:33:50,240
continued year number two of uh AI

1006
00:33:47,919 --> 00:33:52,159
agents but in particular AI agents in

1007
00:33:50,240 --> 00:33:55,440
the enterprise in either deep vertical

1008
00:33:52,159 --> 00:33:57,200
or domain specific areas. Um I think

1009
00:33:55,440 --> 00:33:59,039
this is going to be the main way that we

1010
00:33:57,200 --> 00:34:01,600
actually take all of the progress that

1011
00:33:59,039 --> 00:34:03,120
we're seeing in AI models and actually

1012
00:34:01,600 --> 00:34:05,440
deliver them into the enterprise. You

1013
00:34:03,120 --> 00:34:06,720
have to be able to tie to the workflow

1014
00:34:05,440 --> 00:34:08,320
of the organization. You have to be able

1015
00:34:06,720 --> 00:34:10,079
to get access to the data that they

1016
00:34:08,320 --> 00:34:11,760
have. You have to have the right context

1017
00:34:10,079 --> 00:34:13,119
engineering to make the agents actually

1018
00:34:11,760 --> 00:34:14,720
work. And then you have to do the change

1019
00:34:13,119 --> 00:34:16,480
management that makes the agents

1020
00:34:14,720 --> 00:34:17,679
effective. So this is going to be a year

1021
00:34:16,480 --> 00:34:19,919
where we start to see this pattern

1022
00:34:17,679 --> 00:34:21,520
emerge more and more. Uh which equally

1023
00:34:19,919 --> 00:34:23,919
means that we need to ensure that we

1024
00:34:21,520 --> 00:34:26,480
have a lot more happening on agent

1025
00:34:23,919 --> 00:34:28,480
harnesses. So shout out to Aorvosu and

1026
00:34:26,480 --> 00:34:29,520
Dex for that answer. Uh but it's

1027
00:34:28,480 --> 00:34:31,599
definitely going to be the year of age

1028
00:34:29,520 --> 00:34:33,919
and harness and seeing how do you start

1029
00:34:31,599 --> 00:34:36,480
to get you know an order of magnitude

1030
00:34:33,919 --> 00:34:38,000
improvement on the model's capabilities

1031
00:34:36,480 --> 00:34:40,320
by having all the right scaffolding

1032
00:34:38,000 --> 00:34:42,639
around the model. Uh and then finally it

1033
00:34:40,320 --> 00:34:45,839
will be the year of uh economically

1034
00:34:42,639 --> 00:34:47,760
useful evals. Um so really starting to

1035
00:34:45,839 --> 00:34:49,520
figure out how these models end up doing

1036
00:34:47,760 --> 00:34:51,760
a lot more knowledge worker tasks in the

1037
00:34:49,520 --> 00:34:53,440
economy. Um and that's going to uh we're

1038
00:34:51,760 --> 00:34:54,879
going to see a lot more of that in 2026.

1039
00:34:53,440 --> 00:34:57,440
We saw some previews of that this year

1040
00:34:54,879 --> 00:34:59,359
with Apex and GDP Val uh and a handful

1041
00:34:57,440 --> 00:35:01,359
of others. We're going to see way more

1042
00:34:59,359 --> 00:35:04,240
of that. So, those are the predictions

1043
00:35:01,359 --> 00:35:07,200
and we'll see you uh in 2026.

1044
00:35:04,240 --> 00:35:09,680
>> I think 2026 is going to be a very

1045
00:35:07,200 --> 00:35:13,280
interesting year for American open

1046
00:35:09,680 --> 00:35:15,200
models. Over the last year, the frontier

1047
00:35:13,280 --> 00:35:17,280
of open intelligence shifted from

1048
00:35:15,200 --> 00:35:20,800
America to China, starting with the

1049
00:35:17,280 --> 00:35:23,839
release of Deep Seek at the end of 2024.

1050
00:35:20,800 --> 00:35:25,599
and American institutions were slow to

1051
00:35:23,839 --> 00:35:29,200
notice this erosion of American

1052
00:35:25,599 --> 00:35:31,440
leadership in open intelligence but uh I

1053
00:35:29,200 --> 00:35:33,520
think they've noticed in a big way over

1054
00:35:31,440 --> 00:35:36,079
the last half year both from the

1055
00:35:33,520 --> 00:35:37,839
government level from the enterprise

1056
00:35:36,079 --> 00:35:39,920
level and there are some really

1057
00:35:37,839 --> 00:35:42,240
interesting uh neolabs starting to come

1058
00:35:39,920 --> 00:35:43,839
out with open intelligence as their

1059
00:35:42,240 --> 00:35:47,359
directive and there are a few of these

1060
00:35:43,839 --> 00:35:49,280
not just reflection and these companies

1061
00:35:47,359 --> 00:35:50,320
are starting to produce some very

1062
00:35:49,280 --> 00:35:53,280
interesting

1063
00:35:50,320 --> 00:35:56,960
small open models and next year I think

1064
00:35:53,280 --> 00:35:58,800
we'll see the US regaining leadership at

1065
00:35:56,960 --> 00:36:00,640
the open weight frontier at the largest

1066
00:35:58,800 --> 00:36:04,880
scale and I'm really excited to see

1067
00:36:00,640 --> 00:36:07,440
that. Hey folks, my prediction for 2026

1068
00:36:04,880 --> 00:36:09,760
is that I think we will see AI become

1069
00:36:07,440 --> 00:36:12,640
much more politicized. I think we'll see

1070
00:36:09,760 --> 00:36:15,760
it become a major point of discussion

1071
00:36:12,640 --> 00:36:17,359
for the 2026 midterm elections and some

1072
00:36:15,760 --> 00:36:18,640
people will come out strongly against

1073
00:36:17,359 --> 00:36:21,040
it. Some people will come out. It's

1074
00:36:18,640 --> 00:36:22,720
probably supportive of it. And um I'm

1075
00:36:21,040 --> 00:36:26,000
not sure which side's going to win out.

1076
00:36:22,720 --> 00:36:28,880
>> 2025 has marked an incredible year in AI

1077
00:36:26,000 --> 00:36:30,880
drug discovery. In the past year alone,

1078
00:36:28,880 --> 00:36:32,880
we've gone from being able to design

1079
00:36:30,880 --> 00:36:35,280
simple molecules on the computer to

1080
00:36:32,880 --> 00:36:37,440
designing simple antibodies and now most

1081
00:36:35,280 --> 00:36:39,920
recently fulllength antibodies with

1082
00:36:37,440 --> 00:36:42,880
drug-like properties zero shot on the

1083
00:36:39,920 --> 00:36:45,839
computer. If 2025 has been the year of

1084
00:36:42,880 --> 00:36:48,240
research in AI drug discovery, 2026 will

1085
00:36:45,839 --> 00:36:50,160
be the year of deployment. The models

1086
00:36:48,240 --> 00:36:51,839
have finally entered an era where

1087
00:36:50,160 --> 00:36:54,400
they're becoming really useful for drug

1088
00:36:51,839 --> 00:36:56,400
discovery. Not only do they make things

1089
00:36:54,400 --> 00:36:58,400
faster, but they're also allowing us to

1090
00:36:56,400 --> 00:37:00,079
go after really challenging targets

1091
00:36:58,400 --> 00:37:01,520
which have been traditionally really

1092
00:37:00,079 --> 00:37:03,920
difficult to do with traditional

1093
00:37:01,520 --> 00:37:05,760
techniques. I'm really excited to see

1094
00:37:03,920 --> 00:37:08,000
what comes next because the models show

1095
00:37:05,760 --> 00:37:10,320
no signs of slowing down. Okay, my

1096
00:37:08,000 --> 00:37:12,560
prediction for 2026 is it will be the

1097
00:37:10,320 --> 00:37:14,880
year that YOLO dies. we will begin

1098
00:37:12,560 --> 00:37:17,440
transforming ourselves from a you only

1099
00:37:14,880 --> 00:37:19,359
live once to don't die. I think right

1100
00:37:17,440 --> 00:37:21,119
now we're kind of a suicidal species. We

1101
00:37:19,359 --> 00:37:23,520
do very primitive things. We poison

1102
00:37:21,119 --> 00:37:25,119
ourselves with what we eat. We design

1103
00:37:23,520 --> 00:37:27,520
our lives so that we slowly kill

1104
00:37:25,119 --> 00:37:29,680
ourselves. Companies make profits by

1105
00:37:27,520 --> 00:37:31,760
making us addicted and miserable. We

1106
00:37:29,680 --> 00:37:33,920
destroy the only home we have. And

1107
00:37:31,760 --> 00:37:36,560
somehow we celebrate these things as

1108
00:37:33,920 --> 00:37:38,000
virtue. I think it's all backwards. And

1109
00:37:36,560 --> 00:37:40,079
I think one day we'll look back and

1110
00:37:38,000 --> 00:37:42,800
we'll be pretty astonished that we

1111
00:37:40,079 --> 00:37:44,240
behaved like this. Um I think the simp

1112
00:37:42,800 --> 00:37:47,520
the shift coming is going to be simple

1113
00:37:44,240 --> 00:37:50,400
and radical that we say yes to life and

1114
00:37:47,520 --> 00:37:52,880
no to death. It's simple but I think it

1115
00:37:50,400 --> 00:37:55,440
could be in response to AI's progress.

1116
00:37:52,880 --> 00:37:57,920
And we do this defiantly as a form of

1117
00:37:55,440 --> 00:37:59,839
unification. Um I think it does require

1118
00:37:57,920 --> 00:38:02,240
a lot of courage for us though to say we

1119
00:37:59,839 --> 00:38:04,160
recognize how sacred our existence is.

1120
00:38:02,240 --> 00:38:06,640
We don't want to throw it away and we

1121
00:38:04,160 --> 00:38:08,800
want to defend it with every bit of

1122
00:38:06,640 --> 00:38:10,480
courage and strength we have uh because

1123
00:38:08,800 --> 00:38:12,800
it is so precious. I think it's going to

1124
00:38:10,480 --> 00:38:14,240
be the year we end yolo and the

1125
00:38:12,800 --> 00:38:15,599
beginning of don't die.

1126
00:38:14,240 --> 00:38:17,040
>> The most striking thing about next year

1127
00:38:15,599 --> 00:38:18,480
is that the other forms of knowledge

1128
00:38:17,040 --> 00:38:20,480
work going to experience what software

1129
00:38:18,480 --> 00:38:22,400
engineers are feeling right now where

1130
00:38:20,480 --> 00:38:23,599
they went from typing you know most of

1131
00:38:22,400 --> 00:38:25,200
their lines of code at the beginning of

1132
00:38:23,599 --> 00:38:26,560
the year to typing barely any of them at

1133
00:38:25,200 --> 00:38:28,480
the end of the year. I think of this as

1134
00:38:26,560 --> 00:38:30,320
the claude code experience but for all

1135
00:38:28,480 --> 00:38:31,839
forms of knowledge work. I also think

1136
00:38:30,320 --> 00:38:33,839
that probably continual learning gets

1137
00:38:31,839 --> 00:38:35,599
sold in a satisfying way that we see the

1138
00:38:33,839 --> 00:38:37,440
first test deployments of home robots

1139
00:38:35,599 --> 00:38:39,280
and the software engineering itself goes

1140
00:38:37,440 --> 00:38:40,560
utterly wild next year.

1141
00:38:39,280 --> 00:38:41,599
>> Hey, I'm Ben Inspector.

1142
00:38:40,560 --> 00:38:43,359
>> I'm Ash inspectctor

1143
00:38:41,599 --> 00:38:45,280
>> and our prediction is that 2026 is the

1144
00:38:43,359 --> 00:38:46,720
year of energy efficient AI.

1145
00:38:45,280 --> 00:38:48,000
>> Data center buildouts are primarily

1146
00:38:46,720 --> 00:38:49,760
constrained by energy power

1147
00:38:48,000 --> 00:38:51,280
availability, great interconnects, high

1148
00:38:49,760 --> 00:38:53,280
voltage equipment, things like that.

1149
00:38:51,280 --> 00:38:55,520
Which is why XAI's Colossus was

1150
00:38:53,280 --> 00:38:56,880
initially powered by on-site gas trends.

1151
00:38:55,520 --> 00:38:59,280
The thing is the demand for comput is

1152
00:38:56,880 --> 00:39:01,040
continuing to grow. Labs, neolabs like

1153
00:38:59,280 --> 00:39:02,880
us and startups like cursor have a

1154
00:39:01,040 --> 00:39:04,480
pretty remarkably insatable demand for

1155
00:39:02,880 --> 00:39:05,599
both training and compute and this

1156
00:39:04,480 --> 00:39:07,839
demand is currently outstripping our

1157
00:39:05,599 --> 00:39:09,520
ability to push lots onto the grid. This

1158
00:39:07,839 --> 00:39:11,119
means that in 2026 it will be really

1159
00:39:09,520 --> 00:39:13,440
important to squeeze every available bit

1160
00:39:11,119 --> 00:39:15,040
of tons out of every wallet.

1161
00:39:13,440 --> 00:39:16,640
>> That said, in the long term, chips

1162
00:39:15,040 --> 00:39:18,240
probably matter more than power because

1163
00:39:16,640 --> 00:39:20,079
chips depreciate much more quickly than

1164
00:39:18,240 --> 00:39:21,280
the underlying power infrastructure. So

1165
00:39:20,079 --> 00:39:23,680
for example with data center power

1166
00:39:21,280 --> 00:39:25,359
supplies a titan per kilowatt hour the

1167
00:39:23,680 --> 00:39:27,359
chips cost section order amount more

1168
00:39:25,359 --> 00:39:28,400
than the power in a 5year depreciation

1169
00:39:27,359 --> 00:39:30,720
cycle.

1170
00:39:28,400 --> 00:39:32,400
>> So in 2026 we think intelligence per

1171
00:39:30,720 --> 00:39:33,920
watch is really important to squeeze as

1172
00:39:32,400 --> 00:39:35,760
much intelligence you can out of every

1173
00:39:33,920 --> 00:39:37,599
unit of energy but in the long term we

1174
00:39:35,760 --> 00:39:38,400
think it's the chips that matter more.

1175
00:39:37,599 --> 00:39:40,560
>> Happy holidays.

1176
00:39:38,400 --> 00:39:41,200
>> Happy new year.

1177
00:39:40,560 --> 00:39:42,640
>> Thanks to the year

1178
00:39:41,200 --> 00:39:47,280
>> happy 2026.

1179
00:39:42,640 --> 00:39:49,280
>> Happy 2026 listeners. Thank you.

1180
00:39:47,280 --> 00:39:51,520
Find us on Twitter [music] at no prior

1181
00:39:49,280 --> 00:39:53,280
pod. Subscribe to our YouTube channel if

1182
00:39:51,520 --> 00:39:55,520
you want to see our faces. Follow the

1183
00:39:53,280 --> 00:39:56,960
show [music] on Apple Podcasts, Spotify,

1184
00:39:55,520 --> 00:39:58,880
or wherever you listen. That way, you

1185
00:39:56,960 --> 00:39:59,920
get a new episode every week. And sign

1186
00:39:58,880 --> 00:40:01,760
up for emails [music] or find

1187
00:39:59,920 --> 00:40:04,760
transcripts for every episode at

1188
00:40:01,760 --> 00:40:04,760
no-bers.com.

