1
00:00:00,080 --> 00:00:02,399
You know that 1.4 trillion you

2
00:00:01,439 --> 00:00:03,760
mentioned, we'll spend it over a very

3
00:00:02,399 --> 00:00:05,359
long period of time. I wish we could do

4
00:00:03,760 --> 00:00:07,520
it faster. I think it would be great to

5
00:00:05,359 --> 00:00:09,280
just lay it out for everyone once and

6
00:00:07,520 --> 00:00:11,280
for all how those numbers are going to

7
00:00:09,280 --> 00:00:14,240
work. Exponential growth is usually very

8
00:00:11,280 --> 00:00:16,320
hard for people. OpenAI CEO Sam Alman

9
00:00:14,240 --> 00:00:18,880
joins us to talk about OpenAI's plan to

10
00:00:16,320 --> 00:00:21,039
win as the AI race tightens, how the

11
00:00:18,880 --> 00:00:23,920
infrastructure math makes sense, and

12
00:00:21,039 --> 00:00:26,480
when an OpenAI IPO might be coming. And

13
00:00:23,920 --> 00:00:27,519
Sam is with us here in studio today.

14
00:00:26,480 --> 00:00:30,240
Sam, welcome to the show.

15
00:00:27,519 --> 00:00:32,000
>> Thanks for having me. So, OpenAI is 10

16
00:00:30,240 --> 00:00:34,960
years old and crazy to me.

17
00:00:32,000 --> 00:00:38,239
>> Chachi PT is three, but the competition

18
00:00:34,960 --> 00:00:40,719
is intensifying. Um, this place we're at

19
00:00:38,239 --> 00:00:43,600
OpenAI headquarters was in a code red is

20
00:00:40,719 --> 00:00:46,559
in a code red. Um, after Gemini 3 came

21
00:00:43,600 --> 00:00:48,640
out and everywhere you look, there are

22
00:00:46,559 --> 00:00:51,600
companies that are trying to take a

23
00:00:48,640 --> 00:00:53,280
little bit of OpenAI's advantage. And

24
00:00:51,600 --> 00:00:55,520
for the first time I can remember, it

25
00:00:53,280 --> 00:00:58,160
doesn't seem like this company has a

26
00:00:55,520 --> 00:01:01,359
clear lead. So I'm curious to hear your

27
00:00:58,160 --> 00:01:04,080
perspective on how open AI will emerge

28
00:01:01,359 --> 00:01:05,760
from this moment and when first of all

29
00:01:04,080 --> 00:01:08,400
on the code red point we view those as

30
00:01:05,760 --> 00:01:10,159
like relatively low stakes somewhat

31
00:01:08,400 --> 00:01:12,960
frequent things to do. Uh I think that

32
00:01:10,159 --> 00:01:14,479
it's good to be paranoid and act quickly

33
00:01:12,960 --> 00:01:15,680
when a potential competitive threat

34
00:01:14,479 --> 00:01:17,200
emerges. This has happened to us in the

35
00:01:15,680 --> 00:01:20,159
past that happened earlier this year

36
00:01:17,200 --> 00:01:21,759
with Deepseek. Um and

37
00:01:20,159 --> 00:01:24,720
>> there was a code red back then too.

38
00:01:21,759 --> 00:01:26,240
>> Yeah. There there's there's a saying

39
00:01:24,720 --> 00:01:29,680
about pandemics which is something like

40
00:01:26,240 --> 00:01:31,759
when when a pandemic starts

41
00:01:29,680 --> 00:01:33,119
every bit of action you take at the

42
00:01:31,759 --> 00:01:34,720
beginning is worth much more than action

43
00:01:33,119 --> 00:01:36,640
you take later and most people don't do

44
00:01:34,720 --> 00:01:38,240
enough early on and then panic later and

45
00:01:36,640 --> 00:01:40,799
certainly saw that during the covid

46
00:01:38,240 --> 00:01:42,479
pandemic. Um

47
00:01:40,799 --> 00:01:44,400
but I sort of think of that philosophy

48
00:01:42,479 --> 00:01:47,840
as how we respond to competitive

49
00:01:44,400 --> 00:01:49,119
threats. Uh and you know it's I think

50
00:01:47,840 --> 00:01:50,960
it's good to be a little paranoid.

51
00:01:49,119 --> 00:01:52,560
Gemini 3 has not or at least has not so

52
00:01:50,960 --> 00:01:54,720
far had the impact we were worried it

53
00:01:52,560 --> 00:01:56,560
might but it did in the same way the

54
00:01:54,720 --> 00:01:59,200
Deepse seek did identify some weaknesses

55
00:01:56,560 --> 00:02:01,360
in our product offering strategy and

56
00:01:59,200 --> 00:02:02,799
we're addressing those very quickly. I

57
00:02:01,360 --> 00:02:06,079
don't think we'll be in this code red

58
00:02:02,799 --> 00:02:08,720
that much longer. Uh you know like these

59
00:02:06,079 --> 00:02:10,000
are not these are historically these

60
00:02:08,720 --> 00:02:12,959
have been kind of like six or eight week

61
00:02:10,000 --> 00:02:14,720
things for us. Um

62
00:02:12,959 --> 00:02:17,120
but I'm glad we're doing it. Uh just

63
00:02:14,720 --> 00:02:18,319
today we launched uh a new image model

64
00:02:17,120 --> 00:02:20,080
which is a great thing and that's

65
00:02:18,319 --> 00:02:22,400
something consumers really wanted. Um,

66
00:02:20,080 --> 00:02:23,760
last week we launched 5.2 which uh is

67
00:02:22,400 --> 00:02:26,239
going over extremely well and growing

68
00:02:23,760 --> 00:02:28,800
very quickly. Uh, we'll have a few other

69
00:02:26,239 --> 00:02:30,239
things to uh launch and then we'll also

70
00:02:28,800 --> 00:02:32,000
have some continuous improvements like

71
00:02:30,239 --> 00:02:34,640
speeding up the service. But, you know,

72
00:02:32,000 --> 00:02:36,720
I think this is like my guess is we'll

73
00:02:34,640 --> 00:02:40,080
be doing these once maybe twice a year

74
00:02:36,720 --> 00:02:42,080
for a long time and that's uh part of

75
00:02:40,080 --> 00:02:45,120
really just making sure that we win in

76
00:02:42,080 --> 00:02:46,560
our space. Um, a lot of other companies

77
00:02:45,120 --> 00:02:50,319
will do great too and I'm happy for

78
00:02:46,560 --> 00:02:54,080
them. But, you know, CatchBT is still uh

79
00:02:50,319 --> 00:02:55,840
by far by far the dominant uh

80
00:02:54,080 --> 00:02:58,000
chatbot in the market and I expect that

81
00:02:55,840 --> 00:03:00,160
lead to increase not decrease over time.

82
00:02:58,000 --> 00:03:02,720
Um,

83
00:03:00,160 --> 00:03:04,400
the the models will get good everywhere,

84
00:03:02,720 --> 00:03:06,720
but a lot of the reasons that people use

85
00:03:04,400 --> 00:03:08,400
a product, consumer or enterprise, uh,

86
00:03:06,720 --> 00:03:11,280
have much more to do than just with the

87
00:03:08,400 --> 00:03:13,120
model. And we've, you know, been

88
00:03:11,280 --> 00:03:15,519
expecting this for a while. So we try to

89
00:03:13,120 --> 00:03:17,680
build the whole cohesive set of things

90
00:03:15,519 --> 00:03:19,920
that it takes to make sure that we are

91
00:03:17,680 --> 00:03:22,239
you know the product that people most

92
00:03:19,920 --> 00:03:25,440
want to use. Um I think competition is

93
00:03:22,239 --> 00:03:27,680
good. It pushes us to be better. Uh but

94
00:03:25,440 --> 00:03:29,120
I think we'll do great in chat. I think

95
00:03:27,680 --> 00:03:31,599
we'll do great in enterprise and in the

96
00:03:29,120 --> 00:03:33,599
future years. Other new categories I

97
00:03:31,599 --> 00:03:35,840
expect we'll do great there too. I I

98
00:03:33,599 --> 00:03:38,560
think people really want to use one AI

99
00:03:35,840 --> 00:03:40,319
platform. People use their phone at

100
00:03:38,560 --> 00:03:41,599
their personal life and they want to use

101
00:03:40,319 --> 00:03:43,120
the same kind of phone at work most of

102
00:03:41,599 --> 00:03:45,040
the time. We're seeing the same thing

103
00:03:43,120 --> 00:03:46,480
with AI. Uh the strength of chatgbt

104
00:03:45,040 --> 00:03:48,400
consumer is really helping us win the

105
00:03:46,480 --> 00:03:49,760
enterprise. Uh of course enterprises

106
00:03:48,400 --> 00:03:51,680
need different offerings but people

107
00:03:49,760 --> 00:03:53,760
think about okay I know this company

108
00:03:51,680 --> 00:03:58,239
open and I know how to use this chat GPT

109
00:03:53,760 --> 00:03:59,840
interface. Um so the strategy is make

110
00:03:58,239 --> 00:04:01,120
the best models build the best product

111
00:03:59,840 --> 00:04:02,720
around it and have enough infrastructure

112
00:04:01,120 --> 00:04:04,879
to serve it at scale.

113
00:04:02,720 --> 00:04:06,720
>> Yeah there is an incumbent advantage. uh

114
00:04:04,879 --> 00:04:09,360
chat I think earlier this year was

115
00:04:06,720 --> 00:04:11,680
around 400 million weekly active users.

116
00:04:09,360 --> 00:04:14,480
Now it's at 800 million reports say

117
00:04:11,680 --> 00:04:15,920
approaching 900 million. Um but then on

118
00:04:14,480 --> 00:04:19,519
the other side you have distribution

119
00:04:15,920 --> 00:04:21,359
advantages at places like Google. And so

120
00:04:19,519 --> 00:04:23,600
I'm curious to hear your perspective if

121
00:04:21,359 --> 00:04:25,680
the models do you think the models are

122
00:04:23,600 --> 00:04:27,759
going to commoditize? And if they do

123
00:04:25,680 --> 00:04:29,680
what matters most? Is it distribution?

124
00:04:27,759 --> 00:04:31,440
Is it how well you build your

125
00:04:29,680 --> 00:04:34,960
applications? Is it something else that

126
00:04:31,440 --> 00:04:36,479
I'm not thinking of? I don't think

127
00:04:34,960 --> 00:04:39,440
commoditization is quite the right

128
00:04:36,479 --> 00:04:41,520
framework to think about the models.

129
00:04:39,440 --> 00:04:43,520
There will be areas where different

130
00:04:41,520 --> 00:04:46,479
models excel at different things. For

131
00:04:43,520 --> 00:04:48,000
the kind of normal use cases of chatting

132
00:04:46,479 --> 00:04:50,000
with a model, maybe there will be a lot

133
00:04:48,000 --> 00:04:51,040
of great options. For scientific

134
00:04:50,000 --> 00:04:52,400
discovery, you will want the thing

135
00:04:51,040 --> 00:04:54,800
that's right at the edge that is

136
00:04:52,400 --> 00:04:57,600
optimized for science perhaps. Um so

137
00:04:54,800 --> 00:05:01,600
models will have different strengths and

138
00:04:57,600 --> 00:05:03,600
the most economic value I think will be

139
00:05:01,600 --> 00:05:06,479
created by models at the frontier and we

140
00:05:03,600 --> 00:05:08,960
plan to be ahead there. Um and we're

141
00:05:06,479 --> 00:05:10,400
like very proud that 52 is the best

142
00:05:08,960 --> 00:05:11,680
reasoning model in the world and the one

143
00:05:10,400 --> 00:05:14,080
that scientists are having the most

144
00:05:11,680 --> 00:05:15,440
progress with but also um we're very

145
00:05:14,080 --> 00:05:17,520
proud that it's what enterprises are

146
00:05:15,440 --> 00:05:20,080
saying is the best at all the tasks that

147
00:05:17,520 --> 00:05:22,639
a business needs to to you know do its

148
00:05:20,080 --> 00:05:24,000
work. Um

149
00:05:22,639 --> 00:05:25,680
so there will be you know times that

150
00:05:24,000 --> 00:05:27,600
we're ahead in some areas and behind in

151
00:05:25,680 --> 00:05:31,039
others but the overall most intelligent

152
00:05:27,600 --> 00:05:33,120
model I expect to have uh significant

153
00:05:31,039 --> 00:05:34,479
value even in a world where free models

154
00:05:33,120 --> 00:05:36,960
can do a lot of the stuff that people

155
00:05:34,479 --> 00:05:38,560
that people need. The the products will

156
00:05:36,960 --> 00:05:40,960
really matter. Distribution and brand as

157
00:05:38,560 --> 00:05:43,440
you said will really matter. Um in

158
00:05:40,960 --> 00:05:45,759
chatbt for example personalization is

159
00:05:43,440 --> 00:05:46,960
extremely sticky. People love the fact

160
00:05:45,759 --> 00:05:49,199
that the model, get to know them over

161
00:05:46,960 --> 00:05:52,400
time, and you'll see us push on that uh

162
00:05:49,199 --> 00:05:54,400
much much more. Um,

163
00:05:52,400 --> 00:05:56,800
people have experiences with these

164
00:05:54,400 --> 00:06:00,880
models that they then really kind of

165
00:05:56,800 --> 00:06:02,880
associate with it. Uh, and you I

166
00:06:00,880 --> 00:06:05,120
remember someone telling me once like

167
00:06:02,880 --> 00:06:07,039
you kind of pick a toothpaste once in

168
00:06:05,120 --> 00:06:10,319
your life and buy it forever or most

169
00:06:07,039 --> 00:06:11,840
people do that apparently. Um and people

170
00:06:10,319 --> 00:06:14,240
talk about it. They have one magical

171
00:06:11,840 --> 00:06:15,520
experience with ChachiPT.

172
00:06:14,240 --> 00:06:18,080
Healthcare is like a famous example

173
00:06:15,520 --> 00:06:20,000
where people put their um you know they

174
00:06:18,080 --> 00:06:21,440
put a blood test into Chachi or put the

175
00:06:20,000 --> 00:06:22,560
symptoms in and they figure out they

176
00:06:21,440 --> 00:06:24,720
have something and they go to a doctor

177
00:06:22,560 --> 00:06:26,479
and they get cured of something they

178
00:06:24,720 --> 00:06:28,479
couldn't figure out before. Like those

179
00:06:26,479 --> 00:06:30,880
users are very sticky. Uh to say nothing

180
00:06:28,479 --> 00:06:33,120
of the personalization on on top of it.

181
00:06:30,880 --> 00:06:37,039
Um

182
00:06:33,120 --> 00:06:40,800
there will be all the product stuff. uh

183
00:06:37,039 --> 00:06:44,800
we just launched our browser uh recently

184
00:06:40,800 --> 00:06:47,840
and I think that's pointing at a new uh

185
00:06:44,800 --> 00:06:49,919
you know pretty good potential mode for

186
00:06:47,840 --> 00:06:52,000
us. Uh the devices are further off but

187
00:06:49,919 --> 00:06:52,960
I'm very excited to to do that. So I

188
00:06:52,000 --> 00:06:55,120
think there'll be all these pieces and

189
00:06:52,960 --> 00:06:57,759
on the enterprise uh what creates the

190
00:06:55,120 --> 00:06:59,039
the mode or the competitive advantage um

191
00:06:57,759 --> 00:07:00,720
I expect that to be a little bit

192
00:06:59,039 --> 00:07:02,400
different but in the same way that

193
00:07:00,720 --> 00:07:04,240
personalization to a user is very

194
00:07:02,400 --> 00:07:05,919
important in consumer there will be a

195
00:07:04,240 --> 00:07:08,880
similar concept of personalization to an

196
00:07:05,919 --> 00:07:11,520
enterprise where a company will have a

197
00:07:08,880 --> 00:07:14,479
relationship with a company like ours uh

198
00:07:11,520 --> 00:07:16,960
and they will connect their data to that

199
00:07:14,479 --> 00:07:19,440
and you'll be able to use a bunch of

200
00:07:16,960 --> 00:07:20,960
agents from different companies running

201
00:07:19,440 --> 00:07:22,479
that and it'll kind of like make sure

202
00:07:20,960 --> 00:07:24,319
that information is handled the right

203
00:07:22,479 --> 00:07:26,720
way and I expect that'll be pretty

204
00:07:24,319 --> 00:07:28,479
sticky too. Um we already have more than

205
00:07:26,720 --> 00:07:30,000
uh a million people think of us largely

206
00:07:28,479 --> 00:07:30,880
as a consumer company but we have

207
00:07:30,000 --> 00:07:31,440
>> we're going to definitely get into

208
00:07:30,880 --> 00:07:32,639
enterprise.

209
00:07:31,440 --> 00:07:34,639
>> Yeah. You know like

210
00:07:32,639 --> 00:07:35,520
>> share the stat.

211
00:07:34,639 --> 00:07:36,080
>> Well actually

212
00:07:35,520 --> 00:07:37,360
>> a million

213
00:07:36,080 --> 00:07:40,800
>> we have more than a million enterprise

214
00:07:37,360 --> 00:07:44,240
users but we have like just absolutely

215
00:07:40,800 --> 00:07:46,720
rapid adoption of the API. Um and like

216
00:07:44,240 --> 00:07:48,560
the API business grew faster for us this

217
00:07:46,720 --> 00:07:51,440
year than even Chad GPT

218
00:07:48,560 --> 00:07:53,919
>> really. Um so the enterprise stuff is

219
00:07:51,440 --> 00:07:55,360
also

220
00:07:53,919 --> 00:07:58,160
you know it's really happening starting

221
00:07:55,360 --> 00:08:00,000
this year. Can I just go back to this

222
00:07:58,160 --> 00:08:02,639
maybe if commoditization is not the

223
00:08:00,000 --> 00:08:04,080
right word model some maybe parody for

224
00:08:02,639 --> 00:08:05,360
everyday users

225
00:08:04,080 --> 00:08:08,319
>> uh because you you started off your

226
00:08:05,360 --> 00:08:09,759
answer saying okay maybe um everyday use

227
00:08:08,319 --> 00:08:11,440
it will feel the same but at the

228
00:08:09,759 --> 00:08:13,840
frontier it's going to feel really

229
00:08:11,440 --> 00:08:17,680
different. Um when it comes to chat

230
00:08:13,840 --> 00:08:21,120
GPT's ability to grow um if I'll just

231
00:08:17,680 --> 00:08:23,280
use Google as an example. If Chat GPT uh

232
00:08:21,120 --> 00:08:25,599
and Gemini are built on a model that

233
00:08:23,280 --> 00:08:27,599
feels similar for everyday uses, how big

234
00:08:25,599 --> 00:08:29,440
of a threat is the fact that you know

235
00:08:27,599 --> 00:08:31,759
Google has all these surfaces through

236
00:08:29,440 --> 00:08:33,919
which it can push out Gemini whereas

237
00:08:31,759 --> 00:08:34,640
Chat GPT is is fighting for every new

238
00:08:33,919 --> 00:08:38,080
user.

239
00:08:34,640 --> 00:08:41,120
>> I I think Google is still a huge threat

240
00:08:38,080 --> 00:08:43,839
uh you know extremely powerful company.

241
00:08:41,120 --> 00:08:45,600
If Google had really decided to take us

242
00:08:43,839 --> 00:08:48,720
seriously

243
00:08:45,600 --> 00:08:50,959
in 200

244
00:08:48,720 --> 00:08:52,720
23, let's say, we would have been in a

245
00:08:50,959 --> 00:08:55,440
really bad place. I think they would

246
00:08:52,720 --> 00:08:56,959
have just been able to smash us. Um, but

247
00:08:55,440 --> 00:08:58,480
their AI effort at the time was kind of

248
00:08:56,959 --> 00:09:00,080
going in not quite the right direction

249
00:08:58,480 --> 00:09:01,360
productwise. They didn't, you know, they

250
00:09:00,080 --> 00:09:02,560
had their own code red at one point, but

251
00:09:01,360 --> 00:09:04,240
they didn't take it that seriously.

252
00:09:02,560 --> 00:09:06,080
Everyone's doing code reds out here.

253
00:09:04,240 --> 00:09:08,399
>> Um, and then

254
00:09:06,080 --> 00:09:09,600
>> and also Google has probably the

255
00:09:08,399 --> 00:09:10,959
greatest business model in the whole

256
00:09:09,600 --> 00:09:14,000
tech industry.

257
00:09:10,959 --> 00:09:18,080
Um, and I think they will be slow to

258
00:09:14,000 --> 00:09:21,120
give that up. Um, but bolting AI into

259
00:09:18,080 --> 00:09:22,560
web search, I don't I may be wrong.

260
00:09:21,120 --> 00:09:24,640
Maybe like drinking the Kool-Aid here. I

261
00:09:22,560 --> 00:09:27,360
don't think that'll work as well as

262
00:09:24,640 --> 00:09:28,800
reimagining the whole, this is actually

263
00:09:27,360 --> 00:09:30,720
a broader trend I think is interesting.

264
00:09:28,800 --> 00:09:32,160
Bolting AI onto the existing way of

265
00:09:30,720 --> 00:09:34,160
doing things, I don't think is going to

266
00:09:32,160 --> 00:09:36,240
work well as redesigning stuff in the

267
00:09:34,160 --> 00:09:37,920
sort of like AI first world. was part of

268
00:09:36,240 --> 00:09:39,360
why we wanted to do the consumer devices

269
00:09:37,920 --> 00:09:43,440
in the first place, but it applies at

270
00:09:39,360 --> 00:09:45,360
many other levels. Um, if you stick AI

271
00:09:43,440 --> 00:09:47,120
into a messaging app that's doing a nice

272
00:09:45,360 --> 00:09:48,800
job summarizing your messages and

273
00:09:47,120 --> 00:09:51,120
drafting responses for you, that is

274
00:09:48,800 --> 00:09:52,399
definitely a little better. But I don't

275
00:09:51,120 --> 00:09:53,760
think that's the end state. That is not

276
00:09:52,399 --> 00:09:56,000
the idea of you have this like really

277
00:09:53,760 --> 00:09:57,440
smart AI that is like acting as your

278
00:09:56,000 --> 00:09:59,360
agent, talking to everybody else's agent

279
00:09:57,440 --> 00:10:01,200
and figuring out when to bother you,

280
00:09:59,360 --> 00:10:03,120
when not to bother you, and how to, you

281
00:10:01,200 --> 00:10:05,680
know, what decisions it can handle and

282
00:10:03,120 --> 00:10:06,880
when it needs to ask you. So

283
00:10:05,680 --> 00:10:09,120
similar things for search, similar

284
00:10:06,880 --> 00:10:10,800
things for like productivity suites. I

285
00:10:09,120 --> 00:10:12,080
suspect

286
00:10:10,800 --> 00:10:15,279
it always takes longer than you think,

287
00:10:12,080 --> 00:10:17,120
but I suspect we will see new

288
00:10:15,279 --> 00:10:20,079
products in in the major categories that

289
00:10:17,120 --> 00:10:21,760
are just totally built around AI rather

290
00:10:20,079 --> 00:10:23,440
than bolting AI in. And I think this is

291
00:10:21,760 --> 00:10:24,959
a weakness of Google's even though they

292
00:10:23,440 --> 00:10:26,640
have this huge distribution advantage.

293
00:10:24,959 --> 00:10:28,959
>> Yeah, I' I've spoken with so many people

294
00:10:26,640 --> 00:10:30,320
about this question. uh when Chetchup PT

295
00:10:28,959 --> 00:10:32,320
came out initially, I think it was

296
00:10:30,320 --> 00:10:35,120
Bendic Devon that suggested you might

297
00:10:32,320 --> 00:10:37,120
not want to put AI in Excel. You might

298
00:10:35,120 --> 00:10:39,279
want to just reimagine how you use

299
00:10:37,120 --> 00:10:40,880
Excel. And to me, in my mind, that was

300
00:10:39,279 --> 00:10:42,640
like you upload your numbers and then

301
00:10:40,880 --> 00:10:44,160
you talk to your numbers. Well, one of

302
00:10:42,640 --> 00:10:46,320
the things people have found as they've

303
00:10:44,160 --> 00:10:48,560
developed this stuff is there needs to

304
00:10:46,320 --> 00:10:51,120
be some sort of backend there.

305
00:10:48,560 --> 00:10:53,680
>> So, is it that you sort of build the

306
00:10:51,120 --> 00:10:56,160
backend and then you interact with it

307
00:10:53,680 --> 00:10:57,279
with AI as if it's a new software

308
00:10:56,160 --> 00:10:58,480
program?

309
00:10:57,279 --> 00:10:59,760
Yeah, that's kind of what's happening.

310
00:10:58,480 --> 00:11:01,200
>> Why wouldn't you then be able to just

311
00:10:59,760 --> 00:11:03,040
bolt it on on top?

312
00:11:01,200 --> 00:11:06,399
>> Yeah, I mean, you can bolt it on on top,

313
00:11:03,040 --> 00:11:07,839
but the

314
00:11:06,399 --> 00:11:10,240
>> I spent a lot of my day in various

315
00:11:07,839 --> 00:11:11,920
messaging apps,

316
00:11:10,240 --> 00:11:14,480
including email, including text, Slack,

317
00:11:11,920 --> 00:11:16,720
whatever. I think that's just the wrong

318
00:11:14,480 --> 00:11:18,000
interface. So, you can bolt AI on top of

319
00:11:16,720 --> 00:11:21,200
those, and again, it's like a little bit

320
00:11:18,000 --> 00:11:22,959
better, but what I would rather do is

321
00:11:21,200 --> 00:11:24,560
just sort of like have the ability to

322
00:11:22,959 --> 00:11:26,800
say in the morning, here are the things

323
00:11:24,560 --> 00:11:27,839
I want to get done today. Here's what

324
00:11:26,800 --> 00:11:28,959
I'm worried about. Here's what I'm

325
00:11:27,839 --> 00:11:31,920
thinking about. Here's what I'd like to

326
00:11:28,959 --> 00:11:33,279
happen. I do not want to be I do not

327
00:11:31,920 --> 00:11:34,399
want to spend all day messaging people.

328
00:11:33,279 --> 00:11:35,360
I do not want you to summarize them. I

329
00:11:34,399 --> 00:11:37,120
do not want you to show me a bunch of

330
00:11:35,360 --> 00:11:38,480
drafts. Deal with everything you can.

331
00:11:37,120 --> 00:11:41,440
You know me. You know these people. You

332
00:11:38,480 --> 00:11:45,279
know what I want to get done. Um and

333
00:11:41,440 --> 00:11:46,880
then you know like batch every couple of

334
00:11:45,279 --> 00:11:49,680
hours updates to me if you need

335
00:11:46,880 --> 00:11:52,480
something. But that's a very different

336
00:11:49,680 --> 00:11:53,040
flow than the way these apps work right

337
00:11:52,480 --> 00:11:54,720
now.

338
00:11:53,040 --> 00:11:56,480
>> Yeah. And I was going to ask you what

339
00:11:54,720 --> 00:11:59,360
ChachiBT is going to look like in the

340
00:11:56,480 --> 00:12:02,000
next year and then the next two years.

341
00:11:59,360 --> 00:12:03,760
Is that kind of where it's going?

342
00:12:02,000 --> 00:12:05,519
>> To be perfectly honest, I expected by

343
00:12:03,760 --> 00:12:07,440
this point Chachi BT would have looked

344
00:12:05,519 --> 00:12:09,120
more different than it did at launch.

345
00:12:07,440 --> 00:12:10,959
>> What did you anticipate? I didn't know.

346
00:12:09,120 --> 00:12:12,240
I just thought like that chat interface

347
00:12:10,959 --> 00:12:15,600
was not going to go as far as it turned

348
00:12:12,240 --> 00:12:17,120
out to go. H like we I mean it was put

349
00:12:15,600 --> 00:12:19,839
up

350
00:12:17,120 --> 00:12:21,600
it looks better now, but it is broadly

351
00:12:19,839 --> 00:12:23,680
similar to when it was put up as like a

352
00:12:21,600 --> 00:12:26,079
research preview. was not even meant to

353
00:12:23,680 --> 00:12:28,079
be a product. We knew that the text

354
00:12:26,079 --> 00:12:30,320
interface was very good, you know, like

355
00:12:28,079 --> 00:12:32,720
the everyone's used to texting their

356
00:12:30,320 --> 00:12:35,360
friends and they like it. Um, the chat

357
00:12:32,720 --> 00:12:39,440
interface was very good, but

358
00:12:35,360 --> 00:12:43,360
I would have thought to be as big and as

359
00:12:39,440 --> 00:12:45,040
significantly used for real work of a

360
00:12:43,360 --> 00:12:48,320
product as what we have now, the

361
00:12:45,040 --> 00:12:50,880
interface would have had to go

362
00:12:48,320 --> 00:12:52,639
much further than it has now. I still

363
00:12:50,880 --> 00:12:55,279
think it should do that but there is

364
00:12:52,639 --> 00:12:57,120
something about the generality of the

365
00:12:55,279 --> 00:13:01,399
current interface that I underestimated

366
00:12:57,120 --> 00:13:01,399
the power of. Um

367
00:13:01,839 --> 00:13:07,200
what I

368
00:13:03,680 --> 00:13:08,560
think should happen of course is that um

369
00:13:07,200 --> 00:13:09,760
AI should be able to generate different

370
00:13:08,560 --> 00:13:11,120
kinds of interfaces for different kinds

371
00:13:09,760 --> 00:13:12,160
of tasks. So if you are talking about

372
00:13:11,120 --> 00:13:13,519
your numbers it should be able to show

373
00:13:12,160 --> 00:13:14,560
you that in different ways and you

374
00:13:13,519 --> 00:13:17,200
should be able to interact with it in

375
00:13:14,560 --> 00:13:18,959
different ways. Um

376
00:13:17,200 --> 00:13:20,720
it and we have a little bit of this with

377
00:13:18,959 --> 00:13:22,399
features like canvas. It should be way

378
00:13:20,720 --> 00:13:23,680
more interactive. It's like right now,

379
00:13:22,399 --> 00:13:25,680
you know, it's kind of a back and forth

380
00:13:23,680 --> 00:13:27,839
conversation. It'd be nice if you could

381
00:13:25,680 --> 00:13:30,000
just be talking about an object and it

382
00:13:27,839 --> 00:13:31,200
could be continuously updating. You have

383
00:13:30,000 --> 00:13:33,680
more questions, more thoughts, more

384
00:13:31,200 --> 00:13:37,360
information comes in. Um, it'd be nice

385
00:13:33,680 --> 00:13:38,480
to be more proactive over time where it

386
00:13:37,360 --> 00:13:40,720
maybe does understand what you want to

387
00:13:38,480 --> 00:13:41,920
get done that day and it's continuously

388
00:13:40,720 --> 00:13:43,360
working for you in the background and

389
00:13:41,920 --> 00:13:44,800
send you updates. And you see part of

390
00:13:43,360 --> 00:13:46,079
this the way people are using codecs

391
00:13:44,800 --> 00:13:48,639
which I think is one of the most

392
00:13:46,079 --> 00:13:51,120
exciting

393
00:13:48,639 --> 00:13:56,720
things that happened this year is codecs

394
00:13:51,120 --> 00:13:58,160
got really good. Uh and that points to

395
00:13:56,720 --> 00:14:01,040
a lot of what I hope the shape of the

396
00:13:58,160 --> 00:14:04,040
future looks like. Um

397
00:14:01,040 --> 00:14:04,040
but

398
00:14:04,160 --> 00:14:07,760
it is surprising to me. I was going to

399
00:14:06,000 --> 00:14:09,519
say embarrassing but it's not. I mean

400
00:14:07,760 --> 00:14:11,760
clearly it's been super successful. Uh

401
00:14:09,519 --> 00:14:14,079
it is surprising me how little CHBT has

402
00:14:11,760 --> 00:14:16,480
changed over the last three years.

403
00:14:14,079 --> 00:14:17,519
>> Yep. It the interface works.

404
00:14:16,480 --> 00:14:19,680
>> Yeah.

405
00:14:17,519 --> 00:14:21,600
>> But I guess what the guts have changed

406
00:14:19,680 --> 00:14:24,959
and you talked a little bit about how

407
00:14:21,600 --> 00:14:26,320
personalization is big uh to me and I

408
00:14:24,959 --> 00:14:28,240
think this has been one of your

409
00:14:26,320 --> 00:14:30,959
preferred features too. Memory has been

410
00:14:28,240 --> 00:14:33,040
a real difference maker. Um, I've been

411
00:14:30,959 --> 00:14:35,040
having a conversation with ChachiPT

412
00:14:33,040 --> 00:14:37,920
about a forthcoming trip that has lots

413
00:14:35,040 --> 00:14:39,920
of planning elements for weeks now and I

414
00:14:37,920 --> 00:14:41,600
can just come in in a new window and be

415
00:14:39,920 --> 00:14:43,680
like, "All right, let's pick up on this

416
00:14:41,600 --> 00:14:45,519
trip." And it it has the context and it

417
00:14:43,680 --> 00:14:47,360
knows knows the guide I'm going with. It

418
00:14:45,519 --> 00:14:49,360
knows what I'm doing. Uh, the fact that

419
00:14:47,360 --> 00:14:51,920
I've been like planning fitness for it

420
00:14:49,360 --> 00:14:55,839
and can really synthesize all of those

421
00:14:51,920 --> 00:14:59,440
things. How good can memory get? I think

422
00:14:55,839 --> 00:15:00,800
we have no conception because the human

423
00:14:59,440 --> 00:15:02,320
limit like even if you have the world's

424
00:15:00,800 --> 00:15:04,800
best

425
00:15:02,320 --> 00:15:07,199
personal assistant

426
00:15:04,800 --> 00:15:08,800
they don't they can't remember every

427
00:15:07,199 --> 00:15:10,160
word you've ever said in your life. They

428
00:15:08,800 --> 00:15:11,519
can't have read every email. They can't

429
00:15:10,160 --> 00:15:14,880
have read every document you've ever

430
00:15:11,519 --> 00:15:16,480
written. They can't be you know looking

431
00:15:14,880 --> 00:15:18,639
at all your work every day and

432
00:15:16,480 --> 00:15:21,600
remembering every little detail. They

433
00:15:18,639 --> 00:15:23,120
can't be a participant in your life to

434
00:15:21,600 --> 00:15:25,120
that degree. And no human has like

435
00:15:23,120 --> 00:15:26,639
infinite perfect memory.

436
00:15:25,120 --> 00:15:28,079
Um,

437
00:15:26,639 --> 00:15:29,760
and AI is definitely going to be able to

438
00:15:28,079 --> 00:15:31,279
do that. And we actually talk a lot

439
00:15:29,760 --> 00:15:32,800
about this, like right now, memory is

440
00:15:31,279 --> 00:15:34,480
still very crude, very early. We're in

441
00:15:32,800 --> 00:15:36,160
like the, you know, the GBT2 era of

442
00:15:34,480 --> 00:15:40,720
memory.

443
00:15:36,160 --> 00:15:42,959
But what it's going to be like when

444
00:15:40,720 --> 00:15:44,639
it really does remember every detail of

445
00:15:42,959 --> 00:15:46,800
your entire life and personalized across

446
00:15:44,639 --> 00:15:49,759
all of that and not just the facts, but

447
00:15:46,800 --> 00:15:51,120
like the little small preferences that

448
00:15:49,759 --> 00:15:52,880
you had that you maybe like didn't even

449
00:15:51,120 --> 00:15:55,600
think to indicate, but the AI can pick

450
00:15:52,880 --> 00:15:56,560
up on. Uh,

451
00:15:55,600 --> 00:15:58,000
I think that's going to be super

452
00:15:56,560 --> 00:16:00,720
powerful. That's one of the features

453
00:15:58,000 --> 00:16:02,399
that still maybe not 2026 thing, but

454
00:16:00,720 --> 00:16:03,199
that's one of the parts of this I'm most

455
00:16:02,399 --> 00:16:04,800
excited for.

456
00:16:03,199 --> 00:16:07,440
>> Yeah. I was speaking with a

457
00:16:04,800 --> 00:16:09,680
neuroscientist on the show and he

458
00:16:07,440 --> 00:16:11,120
mentioned that you don't you can't find

459
00:16:09,680 --> 00:16:12,560
thoughts in the brain. Like the brain

460
00:16:11,120 --> 00:16:14,560
doesn't have a place to store thoughts,

461
00:16:12,560 --> 00:16:16,800
but computing there's a place to store

462
00:16:14,560 --> 00:16:21,040
them. So, you can keep all of them. And

463
00:16:16,800 --> 00:16:23,759
as these bots do keep our thoughts, um,

464
00:16:21,040 --> 00:16:25,440
of course there's a privacy concern. And

465
00:16:23,759 --> 00:16:26,959
but the other thing is something that's

466
00:16:25,440 --> 00:16:29,440
going to be interesting is we'll really

467
00:16:26,959 --> 00:16:31,120
build relationships with them. I think

468
00:16:29,440 --> 00:16:33,040
it's been one of the more underrated

469
00:16:31,120 --> 00:16:35,120
things about this entire moment is that

470
00:16:33,040 --> 00:16:36,720
people have felt that these bots are

471
00:16:35,120 --> 00:16:39,839
their companions, are looking out for

472
00:16:36,720 --> 00:16:42,800
them. Um, and I'm curious to hear your

473
00:16:39,839 --> 00:16:46,000
perspective. Um, when you think about

474
00:16:42,800 --> 00:16:47,600
the level of I don't know if intimacy is

475
00:16:46,000 --> 00:16:50,480
the right word, but companionship people

476
00:16:47,600 --> 00:16:52,800
have with these bots, um, is there a

477
00:16:50,480 --> 00:16:54,959
dial that you can turn to be like, oh,

478
00:16:52,800 --> 00:16:57,440
let's make sure people become really

479
00:16:54,959 --> 00:16:59,199
close with these things, or, you know,

480
00:16:57,440 --> 00:17:01,920
we turn the dial a little bit further

481
00:16:59,199 --> 00:17:04,319
and there's an arms distance uh, between

482
00:17:01,920 --> 00:17:06,400
them and and if there is that dial,

483
00:17:04,319 --> 00:17:08,959
>> how do you modulate that the right way?

484
00:17:06,400 --> 00:17:12,079
There are definitely more people

485
00:17:08,959 --> 00:17:14,079
than I realize that want to have, let's

486
00:17:12,079 --> 00:17:15,120
call it close companionship. You I don't

487
00:17:14,079 --> 00:17:16,160
know what the right word is like.

488
00:17:15,120 --> 00:17:18,400
Relationship doesn't feel quite right.

489
00:17:16,160 --> 00:17:19,760
Companionship doesn't feel quite right.

490
00:17:18,400 --> 00:17:21,439
I I don't know what to call it, but they

491
00:17:19,760 --> 00:17:23,199
want to have whatever this deep

492
00:17:21,439 --> 00:17:25,360
connection with an AI. There there are

493
00:17:23,199 --> 00:17:28,880
more people that want that at the

494
00:17:25,360 --> 00:17:31,200
current level of model capability than I

495
00:17:28,880 --> 00:17:32,640
thought. And there's like a whole bunch

496
00:17:31,200 --> 00:17:35,039
of reasons why I think we underestimated

497
00:17:32,640 --> 00:17:36,320
this, but at the beginning of this year,

498
00:17:35,039 --> 00:17:38,720
it was considered a very strange thing

499
00:17:36,320 --> 00:17:39,840
to say you wanted that. Maybe some a lot

500
00:17:38,720 --> 00:17:42,240
of people still don't revealed

501
00:17:39,840 --> 00:17:45,600
preference.

502
00:17:42,240 --> 00:17:47,280
You know, people like their AI chatbot

503
00:17:45,600 --> 00:17:50,080
to get to know them and be warm to them

504
00:17:47,280 --> 00:17:52,720
and be supportive and there's value

505
00:17:50,080 --> 00:17:54,080
there even for people who in some cases

506
00:17:52,720 --> 00:17:55,600
even for people who say they they don't

507
00:17:54,080 --> 00:17:59,799
care about that uh still have a

508
00:17:55,600 --> 00:17:59,799
preference for it. I

509
00:18:00,000 --> 00:18:02,960
I think there's some version of this

510
00:18:01,360 --> 00:18:04,799
which can be super healthy and I think

511
00:18:02,960 --> 00:18:06,640
you know adult users should get a lot of

512
00:18:04,799 --> 00:18:08,720
choice in where on the spectrum they

513
00:18:06,640 --> 00:18:10,640
want to be. There are definitely

514
00:18:08,720 --> 00:18:12,320
versions of it that seem to me unhealthy

515
00:18:10,640 --> 00:18:14,880
although I'm sure a lot of people will

516
00:18:12,320 --> 00:18:16,640
choose to do that. Um and then there's

517
00:18:14,880 --> 00:18:20,240
some people who definitely want the

518
00:18:16,640 --> 00:18:24,720
driest most effect efficient tool

519
00:18:20,240 --> 00:18:27,360
uh possible. So I suspect like lots of

520
00:18:24,720 --> 00:18:29,360
other technologies,

521
00:18:27,360 --> 00:18:32,000
we will run the experiment. We will find

522
00:18:29,360 --> 00:18:35,679
that there's unknown unknowns, good and

523
00:18:32,000 --> 00:18:39,000
bad about it. And society will over time

524
00:18:35,679 --> 00:18:39,000
figure out

525
00:18:39,360 --> 00:18:43,360
how to how to think about where people

526
00:18:41,919 --> 00:18:45,039
should set that dial and then people

527
00:18:43,360 --> 00:18:45,760
have huge choice and set it in very

528
00:18:45,039 --> 00:18:47,360
different places.

529
00:18:45,760 --> 00:18:49,360
>> So your your thought is allow people

530
00:18:47,360 --> 00:18:51,360
basically to determine this.

531
00:18:49,360 --> 00:18:54,160
>> Yes, definitely. But I I don't think we

532
00:18:51,360 --> 00:18:56,400
know like how far it's supposed to go,

533
00:18:54,160 --> 00:18:58,320
like how far we should allow it to go.

534
00:18:56,400 --> 00:19:02,720
We're we're going to give people quite a

535
00:18:58,320 --> 00:19:05,039
bit of personal freedom here. Um there

536
00:19:02,720 --> 00:19:07,039
are examples of things that uh we've

537
00:19:05,039 --> 00:19:09,120
talked about that,

538
00:19:07,039 --> 00:19:12,400
you know, other services will offer, but

539
00:19:09,120 --> 00:19:14,960
we we won't. Um like we're not going to

540
00:19:12,400 --> 00:19:16,160
let we're not going to have RAI, you

541
00:19:14,960 --> 00:19:17,520
know, try to convince people that should

542
00:19:16,160 --> 00:19:19,360
be like in an exclusive romantic

543
00:19:17,520 --> 00:19:20,080
relationship with them, for example.

544
00:19:19,360 --> 00:19:21,600
got to keep it open.

545
00:19:20,080 --> 00:19:24,400
>> But I'm sure that will No, I'm sure that

546
00:19:21,600 --> 00:19:26,000
that will happen with other services, I

547
00:19:24,400 --> 00:19:27,440
guess. Yeah, because the stickier it is,

548
00:19:26,000 --> 00:19:29,919
the more money that service makes. The

549
00:19:27,440 --> 00:19:31,280
whole all these possibilities kind of

550
00:19:29,919 --> 00:19:33,840
they're a little bit scary when you

551
00:19:31,280 --> 00:19:36,480
think about them a little bit deeply.

552
00:19:33,840 --> 00:19:38,720
>> Totally. This is one that really does

553
00:19:36,480 --> 00:19:40,160
that I personally, you know, you can see

554
00:19:38,720 --> 00:19:42,320
the ways that this goes really wrong.

555
00:19:40,160 --> 00:19:44,160
>> Yeah. Uh, you mentioned Enterprise.

556
00:19:42,320 --> 00:19:46,640
Let's talk about Enterprise. you were at

557
00:19:44,160 --> 00:19:48,799
a lunch with some editors and CEOs of

558
00:19:46,640 --> 00:19:51,120
some news companies in New York last

559
00:19:48,799 --> 00:19:53,840
week and told them that enterprise is

560
00:19:51,120 --> 00:19:55,360
going to be a major priority uh for

561
00:19:53,840 --> 00:19:58,000
OpenAI next year.

562
00:19:55,360 --> 00:20:01,120
>> U I'd love to hear a little bit more

563
00:19:58,000 --> 00:20:03,280
about um why that's a priority, how you

564
00:20:01,120 --> 00:20:05,919
think you stack up against anthropic. I

565
00:20:03,280 --> 00:20:08,400
know people will say this is a pivot for

566
00:20:05,919 --> 00:20:09,919
OpenAI that has been consumer focused.

567
00:20:08,400 --> 00:20:11,760
So just give us an overview about the

568
00:20:09,919 --> 00:20:13,679
enterprise plan. Our strategy was always

569
00:20:11,760 --> 00:20:16,000
consumer first. Uh there were a few

570
00:20:13,679 --> 00:20:20,480
reasons for that. One, the models were

571
00:20:16,000 --> 00:20:22,400
not robust and skilled enough uh for

572
00:20:20,480 --> 00:20:24,400
most enterprise uses and now now they're

573
00:20:22,400 --> 00:20:25,760
they're getting there. The second was we

574
00:20:24,400 --> 00:20:28,799
had this like clear opportunity to win

575
00:20:25,760 --> 00:20:30,320
in consumer and those are rare and hard

576
00:20:28,799 --> 00:20:32,320
to come by and I think if you win in

577
00:20:30,320 --> 00:20:35,440
consumer it makes it massively easier to

578
00:20:32,320 --> 00:20:37,120
win in enterprise and we are we are

579
00:20:35,440 --> 00:20:39,039
seeing that now. Um but as I mentioned

580
00:20:37,120 --> 00:20:40,720
earlier this was a year where we

581
00:20:39,039 --> 00:20:43,840
enterprise growth outpaced consumer

582
00:20:40,720 --> 00:20:45,840
growth. Uh and given where the models

583
00:20:43,840 --> 00:20:48,159
are today where they will get to next

584
00:20:45,840 --> 00:20:50,640
year we think this is the time where we

585
00:20:48,159 --> 00:20:53,440
can

586
00:20:50,640 --> 00:20:55,679
build a really significant enterprise

587
00:20:53,440 --> 00:20:57,520
business quite rapidly. I mean I think

588
00:20:55,679 --> 00:21:00,400
and we already have one but it can it

589
00:20:57,520 --> 00:21:01,679
can grow much more. Um

590
00:21:00,400 --> 00:21:04,960
companies seem ready for it. The

591
00:21:01,679 --> 00:21:07,840
technology seems ready for it. the, you

592
00:21:04,960 --> 00:21:10,799
know, coding is the biggest example so

593
00:21:07,840 --> 00:21:12,240
far, but there are others that are now

594
00:21:10,799 --> 00:21:13,919
growing, other verticals that are now

595
00:21:12,240 --> 00:21:15,520
growing very quickly. And we're starting

596
00:21:13,919 --> 00:21:17,600
to hear enterprises say, you know, I

597
00:21:15,520 --> 00:21:19,200
really just want an AI platform.

598
00:21:17,600 --> 00:21:23,200
>> Which vertical company?

599
00:21:19,200 --> 00:21:24,559
>> Um, finance science is the one I'm most

600
00:21:23,200 --> 00:21:27,120
excited about of everything happening

601
00:21:24,559 --> 00:21:31,559
right now. Personally, um, customer

602
00:21:27,120 --> 00:21:31,559
support is doing great. Uh

603
00:21:32,480 --> 00:21:37,600
but but yeah the the

604
00:21:35,600 --> 00:21:39,120
we have this thing called GDP though.

605
00:21:37,600 --> 00:21:40,320
>> I was going to ask you about that. Can I

606
00:21:39,120 --> 00:21:42,000
actually throw my question out about

607
00:21:40,320 --> 00:21:44,320
that? All right. Cuz I wrote to Aaron

608
00:21:42,000 --> 00:21:45,840
Levy the CEO of Box and I said I'm going

609
00:21:44,320 --> 00:21:47,679
to meet with Sam. What should I ask him?

610
00:21:45,840 --> 00:21:49,360
He goes throw a question out about GDP

611
00:21:47,679 --> 00:21:51,840
val. Right. So this is the measure of

612
00:21:49,360 --> 00:21:53,360
how AI performs in knowledge work tasks.

613
00:21:51,840 --> 00:21:56,320
And I said okay. I went back to the

614
00:21:53,360 --> 00:21:58,720
release of GPT 5.2 to the model that uh

615
00:21:56,320 --> 00:22:00,480
you recently released and looked at the

616
00:21:58,720 --> 00:22:03,280
GDP valid chart. Now this of course is

617
00:22:00,480 --> 00:22:07,280
an open AI evaluation. Um that being

618
00:22:03,280 --> 00:22:09,440
said the uh GPT5 thinking model so this

619
00:22:07,280 --> 00:22:12,320
is the model released in the in the

620
00:22:09,440 --> 00:22:15,120
summer. It ti it tied uh knowledge

621
00:22:12,320 --> 00:22:16,000
workers at 38% of test or tied beat or

622
00:22:15,120 --> 00:22:22,080
tied

623
00:22:16,000 --> 00:22:25,360
>> um GP so 38.8% GPT 5.2 2 thinking beat

624
00:22:22,080 --> 00:22:29,679
or tied at 70.9%

625
00:22:25,360 --> 00:22:31,200
of knowledge work tasks and GPT 5.2 pro

626
00:22:29,679 --> 00:22:33,360
74.1%

627
00:22:31,200 --> 00:22:36,559
of knowledge work tasks and it passed

628
00:22:33,360 --> 00:22:38,480
the threshold of um being expert level

629
00:22:36,559 --> 00:22:41,679
it it handled it looks like something

630
00:22:38,480 --> 00:22:43,440
like 60% of expert tasks uh of tasks

631
00:22:41,679 --> 00:22:45,440
that would make it you know on par with

632
00:22:43,440 --> 00:22:47,039
an expert in the knowledge work. What

633
00:22:45,440 --> 00:22:48,799
are the implications of the fact that

634
00:22:47,039 --> 00:22:50,720
these models can do that much knowledge

635
00:22:48,799 --> 00:22:51,760
work? So, you know, you were asking

636
00:22:50,720 --> 00:22:52,559
about verticals, and I think that's a

637
00:22:51,760 --> 00:22:53,679
great question, but the thing that was

638
00:22:52,559 --> 00:22:56,640
going through my mind and why I kind of

639
00:22:53,679 --> 00:22:59,280
was stumbling a little bit is that Eval,

640
00:22:56,640 --> 00:23:01,520
I think it's like 40 something different

641
00:22:59,280 --> 00:23:03,840
verticals that a business has to do.

642
00:23:01,520 --> 00:23:06,000
>> There's make a PowerPoint, do this legal

643
00:23:03,840 --> 00:23:07,679
analysis, you know, write up this little

644
00:23:06,000 --> 00:23:12,320
web app, all this stuff.

645
00:23:07,679 --> 00:23:14,400
>> And and the eval is do experts prefer

646
00:23:12,320 --> 00:23:16,559
the output of the model relative to

647
00:23:14,400 --> 00:23:18,080
other experts

648
00:23:16,559 --> 00:23:19,760
for a lot of the things that a business

649
00:23:18,080 --> 00:23:21,919
has to do. Now, these are small, well

650
00:23:19,760 --> 00:23:24,159
scopeed tasks. These don't get the kind

651
00:23:21,919 --> 00:23:26,320
of complicated, open-ended, creative

652
00:23:24,159 --> 00:23:27,760
work that, you know, figuring out a new

653
00:23:26,320 --> 00:23:31,280
product. These don't get a lot of

654
00:23:27,760 --> 00:23:33,679
collaborative team things. But

655
00:23:31,280 --> 00:23:35,520
a co-orker that you can assign an hour's

656
00:23:33,679 --> 00:23:38,320
worth of tasks to and get something you

657
00:23:35,520 --> 00:23:40,880
like better back 74 or 70% of time if

658
00:23:38,320 --> 00:23:42,880
you want to pay less is still pretty

659
00:23:40,880 --> 00:23:46,000
extraordinary. If you went back to the

660
00:23:42,880 --> 00:23:47,280
launch of Chat TBT 3 years ago and said

661
00:23:46,000 --> 00:23:49,440
we were going to have that in 3 years,

662
00:23:47,280 --> 00:23:52,400
most people would say absolutely not.

663
00:23:49,440 --> 00:23:54,159
Um, and so as we think about how

664
00:23:52,400 --> 00:23:56,240
enterprises are going to integrate this,

665
00:23:54,159 --> 00:23:58,320
it's no longer like just that it can do

666
00:23:56,240 --> 00:24:01,120
code. It's all of these knowledge work

667
00:23:58,320 --> 00:24:05,600
tasks you can kind of farm out to the

668
00:24:01,120 --> 00:24:08,559
AI. uh and

669
00:24:05,600 --> 00:24:10,960
that's going to take a while to really

670
00:24:08,559 --> 00:24:12,880
kind of figure out how enterprises

671
00:24:10,960 --> 00:24:14,480
integrate with it but should be quite

672
00:24:12,880 --> 00:24:15,760
substantial. I know you're not an

673
00:24:14,480 --> 00:24:17,440
economist, so I'm not going to ask you

674
00:24:15,760 --> 00:24:19,360
like what is the macro impact on jobs,

675
00:24:17,440 --> 00:24:21,600
but let me just read you one uh line

676
00:24:19,360 --> 00:24:24,000
that I heard uh you know in in terms of

677
00:24:21,600 --> 00:24:26,960
how this impacts jobs uh from Blood in

678
00:24:24,000 --> 00:24:28,799
the Machine on Substack. Um this is from

679
00:24:26,960 --> 00:24:31,279
a technical copywriter. They said,

680
00:24:28,799 --> 00:24:33,440
"Chatbots came in and made it so my job

681
00:24:31,279 --> 00:24:35,600
was managing the bots instead of a team

682
00:24:33,440 --> 00:24:37,120
of reps." Okay, that that to me seems

683
00:24:35,600 --> 00:24:39,120
like it's going to happen often. But

684
00:24:37,120 --> 00:24:41,200
then this person continued and said once

685
00:24:39,120 --> 00:24:43,360
the bots were sufficiently trained up to

686
00:24:41,200 --> 00:24:46,720
offer good enough support then I was

687
00:24:43,360 --> 00:24:48,320
out. Um is that is that the is that

688
00:24:46,720 --> 00:24:50,080
going to become more common? Is that

689
00:24:48,320 --> 00:24:51,440
what bad companies are going to do?

690
00:24:50,080 --> 00:24:54,000
Because if you have a human who's going

691
00:24:51,440 --> 00:24:55,520
to be able to sort of orchestrate a

692
00:24:54,000 --> 00:24:56,960
bunch of different bots then you might

693
00:24:55,520 --> 00:24:59,840
want to keep them. I don't know. How do

694
00:24:56,960 --> 00:25:01,200
you think about this? So I I agree with

695
00:24:59,840 --> 00:25:03,120
you that it's clear to see how

696
00:25:01,200 --> 00:25:08,640
everyone's going to be managing like a

697
00:25:03,120 --> 00:25:10,320
lot of AI uh doing different stuff. Um

698
00:25:08,640 --> 00:25:11,440
eventually like any good manager

699
00:25:10,320 --> 00:25:13,279
hopefully your team gets better and

700
00:25:11,440 --> 00:25:15,679
better but you just take on more scope

701
00:25:13,279 --> 00:25:17,760
and more responsibility. I am not I am

702
00:25:15,679 --> 00:25:19,919
not a jobs dumer.

703
00:25:17,760 --> 00:25:24,400
Um short term I have some worry. I think

704
00:25:19,919 --> 00:25:28,000
the transition is likely to be rough uh

705
00:25:24,400 --> 00:25:32,000
in some cases but

706
00:25:28,000 --> 00:25:34,480
we are so deeply wired to care about

707
00:25:32,000 --> 00:25:38,159
other people what other people do. We

708
00:25:34,480 --> 00:25:40,159
are so we seem to be so focused on

709
00:25:38,159 --> 00:25:43,679
relative status and always wanting more

710
00:25:40,159 --> 00:25:45,279
and to be of use and service to express

711
00:25:43,679 --> 00:25:47,440
creative spirit whatever whatever

712
00:25:45,279 --> 00:25:49,840
whatever has driven us this long. I

713
00:25:47,440 --> 00:25:52,080
don't think that's going away. Now I do

714
00:25:49,840 --> 00:25:53,440
think the jobs of the future or I don't

715
00:25:52,080 --> 00:25:54,559
even know if jobs is the right word.

716
00:25:53,440 --> 00:25:56,720
Whatever we're all going to do all day

717
00:25:54,559 --> 00:26:00,480
in 2050 probably looks very different

718
00:25:56,720 --> 00:26:02,240
than it does today. Um

719
00:26:00,480 --> 00:26:03,760
but I but I I don't have any of this

720
00:26:02,240 --> 00:26:05,039
like oh life is going to be without

721
00:26:03,760 --> 00:26:07,760
meaning and the economy is going to

722
00:26:05,039 --> 00:26:09,919
totally break. Like we will find I hope

723
00:26:07,760 --> 00:26:13,679
much more meaning and the economy I

724
00:26:09,919 --> 00:26:14,799
think will significantly change but I I

725
00:26:13,679 --> 00:26:18,480
think you just don't bet against

726
00:26:14,799 --> 00:26:19,919
evolutionary biology. Um

727
00:26:18,480 --> 00:26:22,080
you know I think a lot about how we can

728
00:26:19,919 --> 00:26:23,279
automate all the functions at OpenAI and

729
00:26:22,080 --> 00:26:24,960
then even more than that I think about

730
00:26:23,279 --> 00:26:26,720
like what it means to have an AI CEO of

731
00:26:24,960 --> 00:26:29,679
Open AI. Doesn't bother me. I'm like

732
00:26:26,720 --> 00:26:31,039
thrilled for it. I won't fight it. Uh

733
00:26:29,679 --> 00:26:32,559
like I don't want to be I don't want to

734
00:26:31,039 --> 00:26:34,240
be the person hanging on being like I

735
00:26:32,559 --> 00:26:36,400
can do this better the the handmade way.

736
00:26:34,240 --> 00:26:38,960
>> AI CEO just make a bunch of decisions to

737
00:26:36,400 --> 00:26:41,360
sort of like direct all of our resources

738
00:26:38,960 --> 00:26:41,919
to giving AI more energy and power. It's

739
00:26:41,360 --> 00:26:43,760
like

740
00:26:41,919 --> 00:26:45,360
>> um I mean no you would really put a

741
00:26:43,760 --> 00:26:47,600
guard rail on

742
00:26:45,360 --> 00:26:50,559
>> Yeah. Like obviously you don't want an

743
00:26:47,600 --> 00:26:54,080
AI CEO that is not governed by humans,

744
00:26:50,559 --> 00:26:58,360
but if you think about

745
00:26:54,080 --> 00:26:58,360
if if you think about maybe like

746
00:26:58,400 --> 00:27:03,760
a this is a crazy analogy, but I'll give

747
00:27:02,000 --> 00:27:07,120
it anyway. If you think about a version

748
00:27:03,760 --> 00:27:09,279
where like every person in the world was

749
00:27:07,120 --> 00:27:13,039
effectively on the board of directors of

750
00:27:09,279 --> 00:27:15,760
an AI company and got to, you know, tell

751
00:27:13,039 --> 00:27:17,120
the AI CEO what to do and fire them if

752
00:27:15,760 --> 00:27:18,720
they weren't doing a good job at that

753
00:27:17,120 --> 00:27:20,880
and, you know, got governance on the

754
00:27:18,720 --> 00:27:23,520
decisions, but the AI CEO got to try to

755
00:27:20,880 --> 00:27:26,320
like execute the wishes of the board.

756
00:27:23,520 --> 00:27:27,760
Um,

757
00:27:26,320 --> 00:27:28,880
I think to people of the future that

758
00:27:27,760 --> 00:27:30,799
might seem like quite a reasonable

759
00:27:28,880 --> 00:27:32,159
thing. Okay, so we're going to uh move

760
00:27:30,799 --> 00:27:34,240
to infrastructure in a minute, but

761
00:27:32,159 --> 00:27:39,039
before we leave this section on models

762
00:27:34,240 --> 00:27:42,320
and capabilities, when's GP GPT6 coming?

763
00:27:39,039 --> 00:27:44,640
Um, I expect I don't know when we'll

764
00:27:42,320 --> 00:27:47,279
call a model GPT

765
00:27:44,640 --> 00:27:49,200
6. Uh,

766
00:27:47,279 --> 00:27:51,520
but I would expect new models that are

767
00:27:49,200 --> 00:27:52,799
significant gains from 5.2 in the first

768
00:27:51,520 --> 00:27:55,760
quarter of next year.

769
00:27:52,799 --> 00:27:57,600
>> What does significant gains mean?

770
00:27:55,760 --> 00:28:00,559
I don't have like an eval score in mind

771
00:27:57,600 --> 00:28:04,159
for you yet but uh more enterprise side

772
00:28:00,559 --> 00:28:06,320
of things or definitely both the there

773
00:28:04,159 --> 00:28:08,799
will be a lot of improvements to the

774
00:28:06,320 --> 00:28:11,279
model for consumers uh the main thing

775
00:28:08,799 --> 00:28:13,679
consumers want right now is not more IQ

776
00:28:11,279 --> 00:28:15,039
enterprises still do want more IQ so uh

777
00:28:13,679 --> 00:28:17,600
we'll improve the model in different

778
00:28:15,039 --> 00:28:20,960
ways for the kind of for different uses

779
00:28:17,600 --> 00:28:22,240
but uh I our goal is a model that

780
00:28:20,960 --> 00:28:25,840
everybody likes much better

781
00:28:22,240 --> 00:28:28,080
>> so infrastructure you have 1.4 trillion

782
00:28:25,840 --> 00:28:30,559
thereabouts and commitments uh to build

783
00:28:28,080 --> 00:28:31,440
infrastructure. I've listened to a lot

784
00:28:30,559 --> 00:28:33,840
of what you've said about

785
00:28:31,440 --> 00:28:35,919
infrastructure. Um here are some of the

786
00:28:33,840 --> 00:28:37,760
things you said. If people knew what we

787
00:28:35,919 --> 00:28:40,080
could do with compute, they would want

788
00:28:37,760 --> 00:28:42,640
way way more. You said the gap between

789
00:28:40,080 --> 00:28:46,480
what we could offer today versus 10x

790
00:28:42,640 --> 00:28:49,360
compute and 100x compute is substantial.

791
00:28:46,480 --> 00:28:50,559
Uh what what can you help flesh that out

792
00:28:49,360 --> 00:28:53,279
a little bit? What are you going to do

793
00:28:50,559 --> 00:28:54,480
with uh so much more compute?

794
00:28:53,279 --> 00:28:55,679
Well, I mentioned this earlier a little

795
00:28:54,480 --> 00:28:58,559
bit. The thing I'm personally more

796
00:28:55,679 --> 00:29:00,159
excited, most excited about is to use AI

797
00:28:58,559 --> 00:29:02,960
and lots of compute to discover new

798
00:29:00,159 --> 00:29:04,559
science. I am a believer that scientific

799
00:29:02,960 --> 00:29:07,039
discovery is the high order bit of how

800
00:29:04,559 --> 00:29:09,200
the world gets better for everybody. And

801
00:29:07,039 --> 00:29:11,760
if we can throw huge amounts of compute

802
00:29:09,200 --> 00:29:13,600
at scientific problems and discover new

803
00:29:11,760 --> 00:29:15,120
knowledge, which the tiniest bit is

804
00:29:13,600 --> 00:29:17,120
starting to happen now, it's very early.

805
00:29:15,120 --> 00:29:19,360
These are very small things but you know

806
00:29:17,120 --> 00:29:20,880
my learning in history of this field is

807
00:29:19,360 --> 00:29:22,480
once the squiggles start and it lifts

808
00:29:20,880 --> 00:29:24,320
off the x-axis a little bit we know how

809
00:29:22,480 --> 00:29:25,760
to make that better and better. Um but

810
00:29:24,320 --> 00:29:28,159
that takes huge amounts of compute to

811
00:29:25,760 --> 00:29:30,080
do. So that's one area we're like

812
00:29:28,159 --> 00:29:32,399
throwing lots of AI at discovering new

813
00:29:30,080 --> 00:29:35,360
science curing disease lots of other

814
00:29:32,399 --> 00:29:38,000
things. Um,

815
00:29:35,360 --> 00:29:42,880
a kind of recent cool example here is we

816
00:29:38,000 --> 00:29:44,880
built the Sora Android app using codecs

817
00:29:42,880 --> 00:29:46,799
and

818
00:29:44,880 --> 00:29:48,559
they did it in like less than a month.

819
00:29:46,799 --> 00:29:49,840
They used a huge amount. One of the nice

820
00:29:48,559 --> 00:29:51,360
things about working at OpenAI is you

821
00:29:49,840 --> 00:29:53,679
don't get any limits on codecs. They

822
00:29:51,360 --> 00:29:55,200
used a huge amount of tokens, but they

823
00:29:53,679 --> 00:29:57,600
were able to do what would normally have

824
00:29:55,200 --> 00:30:01,679
taken a lot of people much longer and

825
00:29:57,600 --> 00:30:03,760
Codex kind of mostly did it for us. And

826
00:30:01,679 --> 00:30:06,480
you can imagine that going much further

827
00:30:03,760 --> 00:30:09,679
where entire companies can build their

828
00:30:06,480 --> 00:30:12,320
products using lots of compute.

829
00:30:09,679 --> 00:30:13,760
Um

830
00:30:12,320 --> 00:30:15,760
people have talked a lot about how video

831
00:30:13,760 --> 00:30:19,120
models are going to point towards these

832
00:30:15,760 --> 00:30:20,799
generated real-time generated user user

833
00:30:19,120 --> 00:30:23,520
interfaces. That will take a lot of

834
00:30:20,799 --> 00:30:25,520
compute. Um

835
00:30:23,520 --> 00:30:28,480
enterprises that want to transform their

836
00:30:25,520 --> 00:30:30,720
business will use a lot of compute. uh

837
00:30:28,480 --> 00:30:32,159
doctors that want to offer good

838
00:30:30,720 --> 00:30:34,000
personalized health care that are like

839
00:30:32,159 --> 00:30:36,000
constantly

840
00:30:34,000 --> 00:30:37,520
measuring every sign they can get from

841
00:30:36,000 --> 00:30:40,799
each individual patient. You can imagine

842
00:30:37,520 --> 00:30:44,320
that using a lot of compute. Uh it it's

843
00:30:40,799 --> 00:30:46,480
hard to frame how much

844
00:30:44,320 --> 00:30:48,720
compute we're already

845
00:30:46,480 --> 00:30:51,760
using to generate AI output in the

846
00:30:48,720 --> 00:30:53,840
world. Um but these are horribly rough

847
00:30:51,760 --> 00:30:56,320
numbers. So, uh, and I think it's like

848
00:30:53,840 --> 00:30:57,679
undisiplined to talk this way, but I I

849
00:30:56,320 --> 00:30:59,279
always find these like mental thought

850
00:30:57,679 --> 00:31:02,640
experiments a little bit useful, so

851
00:30:59,279 --> 00:31:05,640
forgive me for the sloppiness. Um, let's

852
00:31:02,640 --> 00:31:05,640
say

853
00:31:05,840 --> 00:31:11,120
that an AI company today might be

854
00:31:07,840 --> 00:31:13,120
generating something on the order of 10

855
00:31:11,120 --> 00:31:15,760
trillion tokens a day out of Frontier

856
00:31:13,120 --> 00:31:19,120
models. Um,

857
00:31:15,760 --> 00:31:20,720
you know, more, but not it's not like a

858
00:31:19,120 --> 00:31:24,440
a quadrillion tokens for anybody, I

859
00:31:20,720 --> 00:31:24,440
don't think. Um

860
00:31:24,720 --> 00:31:27,840
let's say there's 8 billion people in

861
00:31:26,320 --> 00:31:29,760
the world and let's say on average

862
00:31:27,840 --> 00:31:31,600
someone's these are I think totally

863
00:31:29,760 --> 00:31:33,360
wrong but let's say someone the average

864
00:31:31,600 --> 00:31:36,000
number of tokens outputed by a person

865
00:31:33,360 --> 00:31:39,519
per day is like

866
00:31:36,000 --> 00:31:41,600
uh 20,000.

867
00:31:39,519 --> 00:31:43,760
You can then start to and the token you

868
00:31:41,600 --> 00:31:45,600
can to be fair then we have to compare

869
00:31:43,760 --> 00:31:47,360
the output tokens of a model provider

870
00:31:45,600 --> 00:31:48,640
today not not all the tokens consumed

871
00:31:47,360 --> 00:31:53,760
but you can start to look at this and

872
00:31:48,640 --> 00:31:56,000
you can say hm we're going to have these

873
00:31:53,760 --> 00:31:58,799
models at a company be outputting more

874
00:31:56,000 --> 00:32:01,440
tokens per day than all of humanity put

875
00:31:58,799 --> 00:32:04,799
together and then 10 times that and then

876
00:32:01,440 --> 00:32:05,919
100 times that. And you know, in some

877
00:32:04,799 --> 00:32:07,840
sense it's like a really silly

878
00:32:05,919 --> 00:32:10,000
comparison,

879
00:32:07,840 --> 00:32:12,880
but in some sense it gives a magnitude

880
00:32:10,000 --> 00:32:14,480
for like how much of the intellectual

881
00:32:12,880 --> 00:32:17,519
crunching on the planet is like human

882
00:32:14,480 --> 00:32:20,799
brains versus AI brains. And that's kind

883
00:32:17,519 --> 00:32:24,399
of the relative growth rates there are

884
00:32:20,799 --> 00:32:25,840
are interesting. And so I'm wondering

885
00:32:24,399 --> 00:32:27,840
are do you know that there is this

886
00:32:25,840 --> 00:32:29,919
demand to use this compute like

887
00:32:27,840 --> 00:32:31,760
potentially like so for instance would

888
00:32:29,919 --> 00:32:34,720
we have surefires like scientific

889
00:32:31,760 --> 00:32:37,200
breakthroughs if you know open AAI were

890
00:32:34,720 --> 00:32:40,240
to put double the compute towards

891
00:32:37,200 --> 00:32:41,840
science or or with medicine like are

892
00:32:40,240 --> 00:32:43,840
would we have you know that clear

893
00:32:41,840 --> 00:32:46,399
ability to assist doctors like

894
00:32:43,840 --> 00:32:48,640
>> how much of this is sort of uh

895
00:32:46,399 --> 00:32:50,880
supposition of what's to happen versus

896
00:32:48,640 --> 00:32:52,240
clear understanding based off of what

897
00:32:50,880 --> 00:32:54,000
you see today IC

898
00:32:52,240 --> 00:32:55,919
>> everything everything based off what we

899
00:32:54,000 --> 00:32:57,600
see today is that it will happen. It

900
00:32:55,919 --> 00:32:59,840
does not mean some crazy thing can't

901
00:32:57,600 --> 00:33:01,120
happen in the future. Someone could

902
00:32:59,840 --> 00:33:02,799
discover some completely new

903
00:33:01,120 --> 00:33:05,200
architecture and there could be a 10,000

904
00:33:02,799 --> 00:33:06,640
times you know efficiency gain and then

905
00:33:05,200 --> 00:33:09,600
we would have really probably overbuilt

906
00:33:06,640 --> 00:33:12,159
for a while. But everything we see right

907
00:33:09,600 --> 00:33:13,440
now about how quickly the models are

908
00:33:12,159 --> 00:33:14,720
getting better at each new level, how

909
00:33:13,440 --> 00:33:16,320
much more people want to use them, each

910
00:33:14,720 --> 00:33:17,760
time we can bring the cost down, how

911
00:33:16,320 --> 00:33:21,640
much more people really want to use

912
00:33:17,760 --> 00:33:21,640
them. Um,

913
00:33:21,840 --> 00:33:25,720
everything about that indicates

914
00:33:25,760 --> 00:33:31,760
to me that there will be increasing

915
00:33:29,120 --> 00:33:35,279
demand and people using these for

916
00:33:31,760 --> 00:33:37,279
wonderful things, for silly things. Um,

917
00:33:35,279 --> 00:33:41,440
but

918
00:33:37,279 --> 00:33:44,799
it it just so seems like

919
00:33:41,440 --> 00:33:47,760
this is the shape of the future. Um

920
00:33:44,799 --> 00:33:49,120
it's not just like it's not just you

921
00:33:47,760 --> 00:33:50,640
know how many tokens we can do per day.

922
00:33:49,120 --> 00:33:52,159
It's how fast we can do them as these

923
00:33:50,640 --> 00:33:53,360
coding models have gotten better. They

924
00:33:52,159 --> 00:33:54,240
can think for a really long time but you

925
00:33:53,360 --> 00:33:55,840
don't want to wait for a really long

926
00:33:54,240 --> 00:33:57,200
time. So there will be other dimensions.

927
00:33:55,840 --> 00:34:00,320
It will not just be the number of tokens

928
00:33:57,200 --> 00:34:02,960
that that we can do. Um but the demand

929
00:34:00,320 --> 00:34:05,360
for intelligence across a small number

930
00:34:02,960 --> 00:34:07,600
of axes

931
00:34:05,360 --> 00:34:10,000
and what we can do with those you know

932
00:34:07,600 --> 00:34:11,839
if you're like if you have like a really

933
00:34:10,000 --> 00:34:14,800
difficult healthcare problem do you want

934
00:34:11,839 --> 00:34:16,560
to use 5.2 or do you want to use 5.2 pro

935
00:34:14,800 --> 00:34:18,240
even if it takes dramatically more

936
00:34:16,560 --> 00:34:20,960
tokens I'll go with the better model. I

937
00:34:18,240 --> 00:34:23,919
think you will um can let's just try to

938
00:34:20,960 --> 00:34:26,000
go one level deeper. Um

939
00:34:23,919 --> 00:34:28,399
going to the scientific discovery, can

940
00:34:26,000 --> 00:34:29,919
you give an example of like a scientist

941
00:34:28,399 --> 00:34:32,079
it doesn't have to well maybe it's one

942
00:34:29,919 --> 00:34:34,800
that you know today that's like I have

943
00:34:32,079 --> 00:34:37,200
problem X and if I put you know compute

944
00:34:34,800 --> 00:34:39,359
Y towards it I will solve it but I'm not

945
00:34:37,200 --> 00:34:40,960
able to today. There was a thing this

946
00:34:39,359 --> 00:34:43,200
morning on Twitter where a bunch of

947
00:34:40,960 --> 00:34:45,119
mathematicians were saying they were all

948
00:34:43,200 --> 00:34:47,040
like replying to each other's tweets. Uh

949
00:34:45,119 --> 00:34:49,359
they're like I was really skeptical that

950
00:34:47,040 --> 00:34:50,879
LM's were ever going to be good. 5.2 is

951
00:34:49,359 --> 00:34:54,399
the one that crossed the boundary for

952
00:34:50,879 --> 00:34:56,320
me. it did it you know figured out this

953
00:34:54,399 --> 00:34:58,640
it with some help it did this small

954
00:34:56,320 --> 00:35:00,160
proof it it discovered this small thing

955
00:34:58,640 --> 00:35:01,839
but it's this is actually changing my

956
00:35:00,160 --> 00:35:02,960
workflow and then people were piling on

957
00:35:01,839 --> 00:35:04,560
saying yeah me too I mean some people

958
00:35:02,960 --> 00:35:05,520
were saying 5.1 was already there not

959
00:35:04,560 --> 00:35:07,920
many

960
00:35:05,520 --> 00:35:09,920
>> um but

961
00:35:07,920 --> 00:35:11,599
that that was like that's a very recent

962
00:35:09,920 --> 00:35:13,599
example this model's only been out for 5

963
00:35:11,599 --> 00:35:15,599
days or something where people are like

964
00:35:13,599 --> 00:35:17,280
all right you know the mathematic

965
00:35:15,599 --> 00:35:18,880
>> the mathematics research community seems

966
00:35:17,280 --> 00:35:19,440
to say like okay something important

967
00:35:18,880 --> 00:35:20,800
just happened

968
00:35:19,440 --> 00:35:22,480
>> I've seen Greg Brockman has been

969
00:35:20,800 --> 00:35:24,480
highlighting getting all these different

970
00:35:22,480 --> 00:35:27,359
mathematical scientific uses in his feed

971
00:35:24,480 --> 00:35:30,560
and something has clicked I think with

972
00:35:27,359 --> 00:35:32,240
5.2 um among these communities. So it'll

973
00:35:30,560 --> 00:35:34,000
be interesting to see what happens as as

974
00:35:32,240 --> 00:35:36,000
things progress.

975
00:35:34,000 --> 00:35:38,400
>> We don't

976
00:35:36,000 --> 00:35:40,000
like one of the hard parts about compute

977
00:35:38,400 --> 00:35:43,040
>> at this scale is you have to do it so

978
00:35:40,000 --> 00:35:44,079
far in advance. So you know that 1.4

979
00:35:43,040 --> 00:35:45,440
trillion you mentioned we'll spend it

980
00:35:44,079 --> 00:35:47,119
over a very long period of time. I wish

981
00:35:45,440 --> 00:35:48,240
we could do it faster. I think there

982
00:35:47,119 --> 00:35:52,400
would be demand if we could do it

983
00:35:48,240 --> 00:35:54,560
faster. Um, but

984
00:35:52,400 --> 00:35:57,920
it just takes an enormously long time to

985
00:35:54,560 --> 00:36:00,079
build these projects and the energy to

986
00:35:57,920 --> 00:36:01,280
run the data centers and the chips and

987
00:36:00,079 --> 00:36:02,880
the systems and the networking and

988
00:36:01,280 --> 00:36:05,680
everything else. Um, so that will be

989
00:36:02,880 --> 00:36:06,960
over a while, but you know, we

990
00:36:05,680 --> 00:36:08,880
from a year ago to now, we probably

991
00:36:06,960 --> 00:36:10,320
about tripled our compute. We'll triple

992
00:36:08,880 --> 00:36:13,920
our compute again next year, hopefully

993
00:36:10,320 --> 00:36:15,440
again after that. um revenue grows even

994
00:36:13,920 --> 00:36:18,640
a little bit faster than that but it

995
00:36:15,440 --> 00:36:22,960
does roughly track our compute

996
00:36:18,640 --> 00:36:24,800
fleet. Uh so we

997
00:36:22,960 --> 00:36:26,640
we have never yet found a situation

998
00:36:24,800 --> 00:36:29,200
where we can't really well monetize all

999
00:36:26,640 --> 00:36:30,240
the compute we have. Um if we had I

1000
00:36:29,200 --> 00:36:31,520
think if we had you know double the

1001
00:36:30,240 --> 00:36:32,320
compute we'd be at double the revenue

1002
00:36:31,520 --> 00:36:33,599
right now.

1003
00:36:32,320 --> 00:36:36,240
>> Okay let's let's talk about numbers

1004
00:36:33,599 --> 00:36:38,720
since you brought it up. Um revenue is

1005
00:36:36,240 --> 00:36:42,079
growing. uh compute spend is growing but

1006
00:36:38,720 --> 00:36:44,320
compute spend still outpaces revenue

1007
00:36:42,079 --> 00:36:46,480
growth. Uh I think the numbers that have

1008
00:36:44,320 --> 00:36:49,839
been reported are OpenAI is supposed to

1009
00:36:46,480 --> 00:36:53,599
lose something like 120 billion between

1010
00:36:49,839 --> 00:36:56,880
now and 120 and 2028 29 where you're

1011
00:36:53,599 --> 00:36:58,640
going to become profitable. Um so talk a

1012
00:36:56,880 --> 00:37:00,880
little bit about like how does that

1013
00:36:58,640 --> 00:37:05,440
change? Where does the turn happen? I

1014
00:37:00,880 --> 00:37:06,960
mean, as revenue grows and as inference

1015
00:37:05,440 --> 00:37:09,839
becomes a larger and larger part of the

1016
00:37:06,960 --> 00:37:12,079
fleet, it eventually uh subsumes the

1017
00:37:09,839 --> 00:37:14,000
training expense. So, that's the plan.

1018
00:37:12,079 --> 00:37:16,640
Spend a lot of money training but make

1019
00:37:14,000 --> 00:37:19,440
more and more. Uh if we if we weren't

1020
00:37:16,640 --> 00:37:21,920
continuing to grow our training

1021
00:37:19,440 --> 00:37:25,839
costs by so much, uh we would be

1022
00:37:21,920 --> 00:37:27,680
profitable way way earlier. Um but the

1023
00:37:25,839 --> 00:37:29,119
bet we're making is to invest very

1024
00:37:27,680 --> 00:37:32,160
aggressively in training these big

1025
00:37:29,119 --> 00:37:34,960
models. The whole world is wondering um

1026
00:37:32,160 --> 00:37:37,599
how your revenue will line up with the

1027
00:37:34,960 --> 00:37:41,040
spend. Uh the question's been asked if

1028
00:37:37,599 --> 00:37:43,359
the trajectory is to hit 20 billion in

1029
00:37:41,040 --> 00:37:47,040
revenue this year and the the spend

1030
00:37:43,359 --> 00:37:48,880
commitment is 1.4 trillion. Um so I

1031
00:37:47,040 --> 00:37:49,359
think it would be great just over a very

1032
00:37:48,880 --> 00:37:50,960
long period.

1033
00:37:49,359 --> 00:37:52,240
>> Yeah. Over and that's why I wanted to

1034
00:37:50,960 --> 00:37:54,240
bring it up to you. I think it would be

1035
00:37:52,240 --> 00:37:56,320
great to just lay it out for everyone

1036
00:37:54,240 --> 00:37:59,440
once and for all how those numbers are

1037
00:37:56,320 --> 00:38:02,800
going to work. It's it's very hard to

1038
00:37:59,440 --> 00:38:04,160
like really I I I find that one thing I

1039
00:38:02,800 --> 00:38:06,160
certainly can't do it and very few

1040
00:38:04,160 --> 00:38:08,000
people I've ever met can do it. You

1041
00:38:06,160 --> 00:38:09,440
know, you can like you have good

1042
00:38:08,000 --> 00:38:11,119
intuition for a lot of mathematical

1043
00:38:09,440 --> 00:38:13,440
things in your head, but exponential

1044
00:38:11,119 --> 00:38:16,079
growth is usually very hard for people

1045
00:38:13,440 --> 00:38:18,000
to do a good quick mental framework on

1046
00:38:16,079 --> 00:38:19,760
like for whatever reason there were a

1047
00:38:18,000 --> 00:38:21,440
lot of things that evolution needed us

1048
00:38:19,760 --> 00:38:23,599
to be able to do well with math in our

1049
00:38:21,440 --> 00:38:27,920
heads. Modeling exponential growth

1050
00:38:23,599 --> 00:38:30,480
doesn't seem to be one of them. Um so

1051
00:38:27,920 --> 00:38:32,320
the thing we believe is that we can stay

1052
00:38:30,480 --> 00:38:35,839
on

1053
00:38:32,320 --> 00:38:37,520
a very steep

1054
00:38:35,839 --> 00:38:39,280
growth curve of revenue for quite a

1055
00:38:37,520 --> 00:38:41,359
while and everything we see right now

1056
00:38:39,280 --> 00:38:43,440
continues to indicate that we cannot do

1057
00:38:41,359 --> 00:38:45,359
it if we don't have the compute. uh

1058
00:38:43,440 --> 00:38:47,680
again we're so compute constrained uh

1059
00:38:45,359 --> 00:38:50,560
and it hits the revenue line so hard

1060
00:38:47,680 --> 00:38:51,920
that I think if we get to a point where

1061
00:38:50,560 --> 00:38:54,960
we have like a lot of compute sitting

1062
00:38:51,920 --> 00:38:56,480
around that we can't monetize on a you

1063
00:38:54,960 --> 00:38:58,640
know profitable per unit of compute

1064
00:38:56,480 --> 00:39:00,720
basis be very reasonable to say okay

1065
00:38:58,640 --> 00:39:03,119
this is like a little how's this all

1066
00:39:00,720 --> 00:39:05,680
going to work but

1067
00:39:03,119 --> 00:39:07,680
we've penciled this out a bunch of ways

1068
00:39:05,680 --> 00:39:10,800
uh we will of course also get more

1069
00:39:07,680 --> 00:39:12,720
efficient uh on like a flops per dollar

1070
00:39:10,800 --> 00:39:14,160
basis as you know all of the work we've

1071
00:39:12,720 --> 00:39:18,800
been doing to make comput cheaper comes

1072
00:39:14,160 --> 00:39:20,400
to pass. Um, but

1073
00:39:18,800 --> 00:39:21,599
we see this consumer growth, we see this

1074
00:39:20,400 --> 00:39:24,400
enterprise growth. There's a whole bunch

1075
00:39:21,599 --> 00:39:26,240
of new kinds of businesses that

1076
00:39:24,400 --> 00:39:28,079
have we haven't even launched yet but

1077
00:39:26,240 --> 00:39:30,640
will. Um, but compute is really the

1078
00:39:28,079 --> 00:39:32,640
lifeblood that enables all of this. So

1079
00:39:30,640 --> 00:39:34,000
we, you know, there's like checkpoints

1080
00:39:32,640 --> 00:39:36,640
along the way and if we're a little bit

1081
00:39:34,000 --> 00:39:40,480
wrong about our timing or math, we can

1082
00:39:36,640 --> 00:39:43,200
we have some flexibility, but

1083
00:39:40,480 --> 00:39:44,400
we have always been in a comput deficit.

1084
00:39:43,200 --> 00:39:46,720
It has always constrained what we're

1085
00:39:44,400 --> 00:39:48,320
able to do. Uh I unfortunately think

1086
00:39:46,720 --> 00:39:49,520
that will always be the case, but I wish

1087
00:39:48,320 --> 00:39:51,599
it were less the case and I'd like to

1088
00:39:49,520 --> 00:39:53,040
get it to be less of the case over time.

1089
00:39:51,599 --> 00:39:54,880
Uh because I think there's so many great

1090
00:39:53,040 --> 00:39:56,880
products and services that we can

1091
00:39:54,880 --> 00:39:58,880
deliver and it'll be a great business.

1092
00:39:56,880 --> 00:39:59,839
Okay. So, it's effectively training

1093
00:39:58,880 --> 00:40:02,720
costs go down

1094
00:39:59,839 --> 00:40:04,960
>> as a percentage go up overall. But yeah,

1095
00:40:02,720 --> 00:40:06,720
>> and then your expectation is through

1096
00:40:04,960 --> 00:40:08,560
things like this this enterprise push

1097
00:40:06,720 --> 00:40:12,000
through things like people being willing

1098
00:40:08,560 --> 00:40:14,240
uh to pay for chat GPT through the API,

1099
00:40:12,000 --> 00:40:16,240
OpenAI will be able to grow revenue

1100
00:40:14,240 --> 00:40:18,160
enough to pay for it with revenue.

1101
00:40:16,240 --> 00:40:20,160
>> Yeah, that is the plan.

1102
00:40:18,160 --> 00:40:22,720
>> Now, I think the thing so the market's

1103
00:40:20,160 --> 00:40:24,560
been kind of losing its mind over this

1104
00:40:22,720 --> 00:40:27,200
um recently. I think the thing that has

1105
00:40:24,560 --> 00:40:30,880
spooked the market has been the debt has

1106
00:40:27,200 --> 00:40:32,800
entered uh into this equation. And the

1107
00:40:30,880 --> 00:40:33,760
idea around debt is you take debt out

1108
00:40:32,800 --> 00:40:36,560
when there's something that's

1109
00:40:33,760 --> 00:40:38,160
predictable. Um and then companies will

1110
00:40:36,560 --> 00:40:40,160
take the debt out, they'll build and

1111
00:40:38,160 --> 00:40:42,720
they'll have predictable revenue.

1112
00:40:40,160 --> 00:40:45,520
>> But it's it's the this is a new

1113
00:40:42,720 --> 00:40:47,359
category. It's it is unpredictable. Um

1114
00:40:45,520 --> 00:40:48,560
is is that how do you think about the

1115
00:40:47,359 --> 00:40:51,280
fact that that debt has entered the

1116
00:40:48,560 --> 00:40:54,560
picture here? So, first of all, I think

1117
00:40:51,280 --> 00:40:56,319
the market more lost its mind when

1118
00:40:54,560 --> 00:40:57,520
earlier this year, you know, we would

1119
00:40:56,319 --> 00:40:59,599
like meet with some company and that

1120
00:40:57,520 --> 00:41:01,040
company's stock would go up 20% or 15%

1121
00:40:59,599 --> 00:41:04,240
the next day. That was crazy.

1122
00:41:01,040 --> 00:41:05,599
>> That felt really unhealthy. Um, I'm

1123
00:41:04,240 --> 00:41:07,760
actually happy that there's like a

1124
00:41:05,599 --> 00:41:10,079
little bit more skepticism and

1125
00:41:07,760 --> 00:41:11,599
rationality in the market now cuz uh it

1126
00:41:10,079 --> 00:41:14,160
felt to me like we were just totally

1127
00:41:11,599 --> 00:41:17,119
heading towards a very unstable bubble

1128
00:41:14,160 --> 00:41:18,480
and now I think people are some degree

1129
00:41:17,119 --> 00:41:20,160
of discipline. So I actually think

1130
00:41:18,480 --> 00:41:21,359
things are I think people went crazy

1131
00:41:20,160 --> 00:41:26,640
earlier and now people are being more

1132
00:41:21,359 --> 00:41:30,720
rational on the debt front. I I think we

1133
00:41:26,640 --> 00:41:33,119
do kind of we know that if we build

1134
00:41:30,720 --> 00:41:36,560
infrastructure we the industry someone's

1135
00:41:33,119 --> 00:41:39,839
going to get value out of it. And it's

1136
00:41:36,560 --> 00:41:42,160
still it's still totally early. I agree

1137
00:41:39,839 --> 00:41:43,280
with you. But I don't think anyone's

1138
00:41:42,160 --> 00:41:46,240
still questioning there's not going to

1139
00:41:43,280 --> 00:41:49,040
be value from like AI infrastructure.

1140
00:41:46,240 --> 00:41:51,599
And so I think it is reasonable for debt

1141
00:41:49,040 --> 00:41:52,800
to

1142
00:41:51,599 --> 00:41:54,079
enter this market. I think there will

1143
00:41:52,800 --> 00:41:56,079
also be other kinds of financial

1144
00:41:54,079 --> 00:41:59,119
instruments. I suspect we'll see some

1145
00:41:56,079 --> 00:42:01,520
unreasonable ones as people really you

1146
00:41:59,119 --> 00:42:04,240
know innovate about how to finance this

1147
00:42:01,520 --> 00:42:05,680
sort of stuff. But you know like lending

1148
00:42:04,240 --> 00:42:07,359
companies money to build data centers

1149
00:42:05,680 --> 00:42:09,760
that that seems fine to me. I think the

1150
00:42:07,359 --> 00:42:11,440
the fear is that um if things don't

1151
00:42:09,760 --> 00:42:14,880
continue at pace like here's one

1152
00:42:11,440 --> 00:42:16,400
scenario um and you'll probably disagree

1153
00:42:14,880 --> 00:42:20,000
with this but like the model progress

1154
00:42:16,400 --> 00:42:22,319
saturates uh then the the infrastructure

1155
00:42:20,000 --> 00:42:24,880
becomes worth less than the anticipated

1156
00:42:22,319 --> 00:42:26,400
value was and then yes those data

1157
00:42:24,880 --> 00:42:28,480
centers will be worth something to

1158
00:42:26,400 --> 00:42:30,000
someone but it could be that they get

1159
00:42:28,480 --> 00:42:32,079
liquidated and someone buys them at a

1160
00:42:30,000 --> 00:42:33,440
discount. Yeah. And and I do suspect by

1161
00:42:32,079 --> 00:42:34,880
the way there will be some like booms

1162
00:42:33,440 --> 00:42:39,040
and busts along the way. These things

1163
00:42:34,880 --> 00:42:41,200
are never a perfectly smooth line. Um,

1164
00:42:39,040 --> 00:42:43,200
first of all, it seems very clear to me,

1165
00:42:41,200 --> 00:42:45,200
and this is like a thing I happily would

1166
00:42:43,200 --> 00:42:46,560
bet the company on, that the models are

1167
00:42:45,200 --> 00:42:48,400
going to get much much better. We have

1168
00:42:46,560 --> 00:42:50,319
like a pretty good window into this.

1169
00:42:48,400 --> 00:42:54,160
We're very confident about that. Even if

1170
00:42:50,319 --> 00:42:55,440
they did not, I think the

1171
00:42:54,160 --> 00:42:56,720
there's like a lot of inertia in the

1172
00:42:55,440 --> 00:42:59,599
world. It takes a while to figure out

1173
00:42:56,720 --> 00:43:03,119
how to adapt to things. The overhang of

1174
00:42:59,599 --> 00:43:05,040
the economic value that I believe 5.2 2

1175
00:43:03,119 --> 00:43:06,480
represents relative to what the world

1176
00:43:05,040 --> 00:43:08,880
has figured out how to get out of it so

1177
00:43:06,480 --> 00:43:11,839
far is so huge that even if you froze

1178
00:43:08,880 --> 00:43:13,359
the model at 5.2 to how much more like

1179
00:43:11,839 --> 00:43:16,400
value can you create and thus revenue

1180
00:43:13,359 --> 00:43:18,480
can you drive? I bet a huge amount. In

1181
00:43:16,400 --> 00:43:21,680
fact, you didn't ask this, but if I can

1182
00:43:18,480 --> 00:43:23,520
go on a rant for a second. Um,

1183
00:43:21,680 --> 00:43:26,880
we used to talk a lot about this 2x2

1184
00:43:23,520 --> 00:43:28,480
matrix of short timelines,

1185
00:43:26,880 --> 00:43:30,880
long timelines, slow takeoff, fast

1186
00:43:28,480 --> 00:43:32,240
takeoff, and where we felt at different

1187
00:43:30,880 --> 00:43:34,000
times the kind of probability was

1188
00:43:32,240 --> 00:43:36,079
shifting, and that that was going to be

1189
00:43:34,000 --> 00:43:38,640
you could kind of understand a lot of

1190
00:43:36,079 --> 00:43:40,880
the decisions and strategy that the

1191
00:43:38,640 --> 00:43:42,319
world should optimize for based off of

1192
00:43:40,880 --> 00:43:46,440
where you were going to be on that 2x

1193
00:43:42,319 --> 00:43:46,440
two matrix. Um,

1194
00:43:47,599 --> 00:43:52,800
there's like a Z-axis in my head in my

1195
00:43:50,240 --> 00:43:56,560
picture of this that's emerged, which is

1196
00:43:52,800 --> 00:44:00,240
small overhang, big overhang. And

1197
00:43:56,560 --> 00:44:01,599
I I kind of thought that

1198
00:44:00,240 --> 00:44:04,640
I guess I didn't think about that hard,

1199
00:44:01,599 --> 00:44:06,480
but uh like my retro on this is I must

1200
00:44:04,640 --> 00:44:08,480
have assumed that the overhang was not

1201
00:44:06,480 --> 00:44:10,880
going to be that massive that if the

1202
00:44:08,480 --> 00:44:12,240
models had a lot of value in them, the

1203
00:44:10,880 --> 00:44:14,720
world was pretty quickly going to figure

1204
00:44:12,240 --> 00:44:16,160
out how to deploy that. But it looks to

1205
00:44:14,720 --> 00:44:17,520
me now like the overhang is going to be

1206
00:44:16,160 --> 00:44:20,480
massive in most of the world. You'll

1207
00:44:17,520 --> 00:44:22,400
have these like areas like you know some

1208
00:44:20,480 --> 00:44:24,560
some set of coders that'll get massively

1209
00:44:22,400 --> 00:44:26,800
more productive by adopting these tools.

1210
00:44:24,560 --> 00:44:30,319
But on the whole

1211
00:44:26,800 --> 00:44:31,680
you have this crazy smart model that to

1212
00:44:30,319 --> 00:44:33,359
be perfectly honest most people are

1213
00:44:31,680 --> 00:44:35,599
still asking this similar questions they

1214
00:44:33,359 --> 00:44:37,280
did in the GPD4 realm. Scientists

1215
00:44:35,599 --> 00:44:38,640
different coders different maybe

1216
00:44:37,280 --> 00:44:42,720
knowledge work is going to get different

1217
00:44:38,640 --> 00:44:44,240
but but there is a huge overhang and

1218
00:44:42,720 --> 00:44:46,240
that has a bunch of very strange

1219
00:44:44,240 --> 00:44:47,599
consequences for the world. I we have

1220
00:44:46,240 --> 00:44:49,599
not wrapped our head around all the ways

1221
00:44:47,599 --> 00:44:51,359
that's going to play out yet, but is

1222
00:44:49,599 --> 00:44:53,200
very much not what I would have expected

1223
00:44:51,359 --> 00:44:55,200
a few years ago. I have a question for

1224
00:44:53,200 --> 00:44:57,280
you about this uh capability overhang.

1225
00:44:55,200 --> 00:45:00,400
Basically, the models can do a lot more

1226
00:44:57,280 --> 00:45:03,280
than they've been doing. Um I I'm trying

1227
00:45:00,400 --> 00:45:04,640
to figure out how um the models can be

1228
00:45:03,280 --> 00:45:07,040
that much better than they're being used

1229
00:45:04,640 --> 00:45:09,040
for, but a lot of businesses when they

1230
00:45:07,040 --> 00:45:10,720
try to implement them, they're not

1231
00:45:09,040 --> 00:45:12,720
getting a return on their investment.

1232
00:45:10,720 --> 00:45:14,800
>> Um or at least that's what they tell

1233
00:45:12,720 --> 00:45:16,160
MIT. I'm not sure quite how to think

1234
00:45:14,800 --> 00:45:18,800
about that because we hear all these

1235
00:45:16,160 --> 00:45:20,960
businesses saying, you know, if you 10x

1236
00:45:18,800 --> 00:45:22,560
the price of GPT 5.2, we would still pay

1237
00:45:20,960 --> 00:45:23,599
for it. Like you're hugely underpricing

1238
00:45:22,560 --> 00:45:24,400
this, we're getting all this value out

1239
00:45:23,599 --> 00:45:25,920
of it.

1240
00:45:24,400 --> 00:45:28,960
>> Um,

1241
00:45:25,920 --> 00:45:30,720
so I don't that doesn't seem right to

1242
00:45:28,960 --> 00:45:32,560
me. Certainly, if you talk about like

1243
00:45:30,720 --> 00:45:35,520
what coders say, they're like, "This is,

1244
00:45:32,560 --> 00:45:36,400
you know, I'd pay 100 times the price or

1245
00:45:35,520 --> 00:45:38,079
whatever." Um,

1246
00:45:36,400 --> 00:45:40,000
>> just be bureaucracy that's messing

1247
00:45:38,079 --> 00:45:41,520
things up. Let's say you believe the GDP

1248
00:45:40,000 --> 00:45:42,800
valve numbers and maybe you don't for

1249
00:45:41,520 --> 00:45:45,920
good reason maybe they're wrong but let

1250
00:45:42,800 --> 00:45:48,880
let's say it were true and for kind of

1251
00:45:45,920 --> 00:45:51,359
these wellsp specified not super long

1252
00:45:48,880 --> 00:45:53,520
duration knowledge work tasks seven out

1253
00:45:51,359 --> 00:45:57,040
of 10 times you would be as happy or

1254
00:45:53,520 --> 00:45:59,520
happier with the 5.2 output.

1255
00:45:57,040 --> 00:46:01,440
You should then be using that a lot. And

1256
00:45:59,520 --> 00:46:03,359
yet it takes people so long to change

1257
00:46:01,440 --> 00:46:06,240
their workflow. are so used to asking

1258
00:46:03,359 --> 00:46:09,680
the junior analyst to make a deck or

1259
00:46:06,240 --> 00:46:12,000
whatever that they're going to like it

1260
00:46:09,680 --> 00:46:15,280
just that's stickier than I thought it

1261
00:46:12,000 --> 00:46:17,520
was. You know, I still kind of run my

1262
00:46:15,280 --> 00:46:19,280
workflow in very much the same way.

1263
00:46:17,520 --> 00:46:21,599
Although I know that I could be using AI

1264
00:46:19,280 --> 00:46:23,280
much more than I am. Yep. All right, we

1265
00:46:21,599 --> 00:46:25,040
got 10 minutes left. I got Wow, that was

1266
00:46:23,280 --> 00:46:26,880
quick. I got four questions. Uh let's

1267
00:46:25,040 --> 00:46:30,400
see if we can lightning round uh through

1268
00:46:26,880 --> 00:46:32,640
them. So, uh, the device that you're

1269
00:46:30,400 --> 00:46:36,240
working on. We'll be back with OpenAI

1270
00:46:32,640 --> 00:46:41,119
CEO Sam Alman right after this. Um, what

1271
00:46:36,240 --> 00:46:43,200
I've heard, phone size, no screen. Um,

1272
00:46:41,119 --> 00:46:45,520
why couldn't it be an app if it's the

1273
00:46:43,200 --> 00:46:47,200
phone if it's the phone without a

1274
00:46:45,520 --> 00:46:48,880
screen? First, we're going to do a f a

1275
00:46:47,200 --> 00:46:51,440
small family of devices. It will not be

1276
00:46:48,880 --> 00:46:54,560
a single device. uh there will be over

1277
00:46:51,440 --> 00:46:56,480
time a

1278
00:46:54,560 --> 00:46:58,240
this is this is not speculation so I'm

1279
00:46:56,480 --> 00:46:59,920
may try not to be totally wrong but I

1280
00:46:58,240 --> 00:47:02,960
think there will be a shift over time to

1281
00:46:59,920 --> 00:47:06,240
the way people use computers where they

1282
00:47:02,960 --> 00:47:09,680
go from a sort of

1283
00:47:06,240 --> 00:47:11,200
dumb reactive thing to a very smart

1284
00:47:09,680 --> 00:47:12,480
proactive thing that is understanding

1285
00:47:11,200 --> 00:47:16,079
your whole life your context everything

1286
00:47:12,480 --> 00:47:19,280
going on around you very aware of

1287
00:47:16,079 --> 00:47:23,119
the people around you physically or

1288
00:47:19,280 --> 00:47:26,400
close to you via a computer that you're

1289
00:47:23,119 --> 00:47:30,480
working with. And I don't think current

1290
00:47:26,400 --> 00:47:32,480
devices are well suited

1291
00:47:30,480 --> 00:47:33,920
to that kind of world. And I am a big

1292
00:47:32,480 --> 00:47:38,079
believer that we like we work at the

1293
00:47:33,920 --> 00:47:40,800
limit of our devices. you know, you have

1294
00:47:38,079 --> 00:47:42,800
that computer and it has a bunch of

1295
00:47:40,800 --> 00:47:45,280
design choices. Like it could be open or

1296
00:47:42,800 --> 00:47:46,800
closed, but it can't be, you know,

1297
00:47:45,280 --> 00:47:49,520
there's not like a okay, pay attention

1298
00:47:46,800 --> 00:47:50,800
to this interview, but be closed and

1299
00:47:49,520 --> 00:47:54,319
like whisper in my ear if I forget to

1300
00:47:50,800 --> 00:47:55,760
ask Sam a question or whatever. Um,

1301
00:47:54,319 --> 00:47:58,480
>> maybe that would be helpful. And there's

1302
00:47:55,760 --> 00:48:01,200
like, you know, there's like

1303
00:47:58,480 --> 00:48:03,760
a screen and that like limits you to the

1304
00:48:01,200 --> 00:48:05,200
kind of same way we've had graphical

1305
00:48:03,760 --> 00:48:07,440
user interfaces working for many

1306
00:48:05,200 --> 00:48:09,040
decades. And there's,

1307
00:48:07,440 --> 00:48:10,560
you know, a keyboard that was built to

1308
00:48:09,040 --> 00:48:14,000
like slow down how fast you could get

1309
00:48:10,560 --> 00:48:15,440
information into it. And these have just

1310
00:48:14,000 --> 00:48:16,960
been unquestioned assumptions for a long

1311
00:48:15,440 --> 00:48:20,720
time, but they worked. And then this

1312
00:48:16,960 --> 00:48:24,079
totally new thing came along and it

1313
00:48:20,720 --> 00:48:28,319
opens up a possibility space. But

1314
00:48:24,079 --> 00:48:30,640
I don't think the current form factor of

1315
00:48:28,319 --> 00:48:32,960
devices is the optimal fit. It'd be very

1316
00:48:30,640 --> 00:48:34,960
odd if it were for this like incredible

1317
00:48:32,960 --> 00:48:37,119
new affordance we have. Oh man, we could

1318
00:48:34,960 --> 00:48:38,960
talk for an hour about this, but um

1319
00:48:37,119 --> 00:48:42,000
let's move on to the next one. Cloud.

1320
00:48:38,960 --> 00:48:44,160
You've talked about building a cloud. Um

1321
00:48:42,000 --> 00:48:46,559
here's a an email we got from a

1322
00:48:44,160 --> 00:48:49,040
listener. At my company, we're moving

1323
00:48:46,559 --> 00:48:52,559
off Azure and directly integrating with

1324
00:48:49,040 --> 00:48:55,200
OpenAI to power our AI experiences in

1325
00:48:52,559 --> 00:48:57,680
the product. The focus is to insert a

1326
00:48:55,200 --> 00:49:01,440
stream of trillions of tokens powering

1327
00:48:57,680 --> 00:49:03,359
AI experiences through the stack. Is is

1328
00:49:01,440 --> 00:49:05,040
that the plan to build a big big cloud

1329
00:49:03,359 --> 00:49:06,319
business in that in that way?

1330
00:49:05,040 --> 00:49:07,680
>> First of all, trillions of tokens, a lot

1331
00:49:06,319 --> 00:49:09,440
of tokens. And if you know you asked

1332
00:49:07,680 --> 00:49:11,920
about the need for compute and our

1333
00:49:09,440 --> 00:49:13,599
enterprise strategy like

1334
00:49:11,920 --> 00:49:15,440
>> enterprises have been clear with us

1335
00:49:13,599 --> 00:49:18,000
about how many tokens they'd like to buy

1336
00:49:15,440 --> 00:49:20,720
from us and we are going to again fail

1337
00:49:18,000 --> 00:49:23,440
in 2026 to meet demand but the strategy

1338
00:49:20,720 --> 00:49:26,400
is companies

1339
00:49:23,440 --> 00:49:28,400
most companies seem to want to come to a

1340
00:49:26,400 --> 00:49:30,720
company like us and say I'd like the

1341
00:49:28,400 --> 00:49:32,960
name of my company with AI. I need an

1342
00:49:30,720 --> 00:49:34,559
API customized for my company. I need

1343
00:49:32,960 --> 00:49:36,559
Chach Enterprise customized for my

1344
00:49:34,559 --> 00:49:38,319
company. I need a platform that can like

1345
00:49:36,559 --> 00:49:39,760
run all these agents that I can trust my

1346
00:49:38,319 --> 00:49:42,079
data on. I need the ability to get

1347
00:49:39,760 --> 00:49:45,119
trillions of tokens into my product. I

1348
00:49:42,079 --> 00:49:50,640
need the ability to have all my internal

1349
00:49:45,119 --> 00:49:52,400
processes be more efficient and

1350
00:49:50,640 --> 00:49:54,480
we don't currently have like a great

1351
00:49:52,400 --> 00:49:55,359
all-in-one offering for them and we'd

1352
00:49:54,480 --> 00:49:57,200
like to make that.

1353
00:49:55,359 --> 00:49:58,880
>> Is your ambition to put it up there with

1354
00:49:57,200 --> 00:50:00,800
the AWS and Ashers of the world?

1355
00:49:58,880 --> 00:50:04,079
>> Uh I think it's I think it's a different

1356
00:50:00,800 --> 00:50:06,640
kind of thing than those. like I don't I

1357
00:50:04,079 --> 00:50:09,200
don't really have an ambition to go

1358
00:50:06,640 --> 00:50:10,559
offer whatever all the services you have

1359
00:50:09,200 --> 00:50:13,280
to offer to host a website or I don't

1360
00:50:10,559 --> 00:50:16,280
even know but uh but I I I think the

1361
00:50:13,280 --> 00:50:16,280
concept

1362
00:50:16,480 --> 00:50:22,440
yeah my my guess is that people will

1363
00:50:18,800 --> 00:50:22,440
continue to have their

1364
00:50:22,480 --> 00:50:26,559
call it web cloud and then I think there

1365
00:50:24,880 --> 00:50:28,480
will be this other thing where like a

1366
00:50:26,559 --> 00:50:30,000
company will be like I need an AI

1367
00:50:28,480 --> 00:50:31,359
platform for everything that I want to

1368
00:50:30,000 --> 00:50:32,559
do internally the service I want to

1369
00:50:31,359 --> 00:50:35,359
offer whatever

1370
00:50:32,559 --> 00:50:36,960
and you know like it does kind of live

1371
00:50:35,359 --> 00:50:38,480
on the physical hardware in some sense

1372
00:50:36,960 --> 00:50:39,680
but

1373
00:50:38,480 --> 00:50:41,359
I think it'll be a fairly different

1374
00:50:39,680 --> 00:50:43,920
product offering. Uh let's talk about

1375
00:50:41,359 --> 00:50:45,040
discovery quickly. Um you've said

1376
00:50:43,920 --> 00:50:47,599
something that's been really interesting

1377
00:50:45,040 --> 00:50:49,599
to me uh you that you think that the

1378
00:50:47,599 --> 00:50:51,440
models or maybe it's people working with

1379
00:50:49,599 --> 00:50:53,119
models or the models will make small

1380
00:50:51,440 --> 00:50:55,680
discoveries next year and big ones

1381
00:50:53,119 --> 00:50:58,079
within five. Is that the models? Is it

1382
00:50:55,680 --> 00:50:59,440
people working alongside them? And what

1383
00:50:58,079 --> 00:51:01,359
makes you confident that that's going to

1384
00:50:59,440 --> 00:51:03,520
happen? Yeah, people using the models

1385
00:51:01,359 --> 00:51:05,119
like the the models that can like figure

1386
00:51:03,520 --> 00:51:08,240
out their own questions to ask that does

1387
00:51:05,119 --> 00:51:11,280
feel further off. But if the world is

1388
00:51:08,240 --> 00:51:13,440
benefiting from new knowledge like we

1389
00:51:11,280 --> 00:51:17,920
should be very thrilled and you know

1390
00:51:13,440 --> 00:51:19,440
like I think the the whole course of

1391
00:51:17,920 --> 00:51:20,880
human progress has been that we build

1392
00:51:19,440 --> 00:51:23,040
these better tools and then people use

1393
00:51:20,880 --> 00:51:24,160
them to do more things and then out of

1394
00:51:23,040 --> 00:51:26,240
that process they build more tools and

1395
00:51:24,160 --> 00:51:27,760
it's this like scaffolding that we climb

1396
00:51:26,240 --> 00:51:30,160
like layer by layer, generation by

1397
00:51:27,760 --> 00:51:32,800
generation, discovery by discovery and

1398
00:51:30,160 --> 00:51:34,640
the fact that a human's asking the

1399
00:51:32,800 --> 00:51:36,400
question I think in no way diminishes

1400
00:51:34,640 --> 00:51:40,000
the value of the tool. All right. So, I

1401
00:51:36,400 --> 00:51:41,680
I think it's great. I'm all happy. Um

1402
00:51:40,000 --> 00:51:42,720
I at the beginning of this year, I

1403
00:51:41,680 --> 00:51:44,720
thought the small discoveries were going

1404
00:51:42,720 --> 00:51:46,720
to start in 2026. They started in 2025

1405
00:51:44,720 --> 00:51:48,000
in late 2025. Again, these are very

1406
00:51:46,720 --> 00:51:50,960
small. I really don't want to overstate

1407
00:51:48,000 --> 00:51:53,119
them, but

1408
00:51:50,960 --> 00:51:55,119
anything

1409
00:51:53,119 --> 00:51:57,680
is feels qualitatively to me very

1410
00:51:55,119 --> 00:52:00,000
different than nothing. And certainly in

1411
00:51:57,680 --> 00:52:01,599
the when we launched three years ago,

1412
00:52:00,000 --> 00:52:03,200
that model was not going to make any new

1413
00:52:01,599 --> 00:52:07,319
contribution to the total of human

1414
00:52:03,200 --> 00:52:07,319
knowledge. um

1415
00:52:08,720 --> 00:52:11,920
what it looks like from here to five

1416
00:52:10,480 --> 00:52:14,400
years from now. This journey to big

1417
00:52:11,920 --> 00:52:16,480
discoveries, I suspect it's just like

1418
00:52:14,400 --> 00:52:17,920
like the normal hill climb of AI. It

1419
00:52:16,480 --> 00:52:19,520
just gets like a little bit better every

1420
00:52:17,920 --> 00:52:22,880
quarter and then all of a sudden we're

1421
00:52:19,520 --> 00:52:25,119
like, whoa, humans augmented by these

1422
00:52:22,880 --> 00:52:28,000
models are doing things that humans 5

1423
00:52:25,119 --> 00:52:30,000
years ago just absolutely couldn't do.

1424
00:52:28,000 --> 00:52:31,839
And

1425
00:52:30,000 --> 00:52:33,680
you know, whether we mostly attribute

1426
00:52:31,839 --> 00:52:35,280
that to smarter humans or smarter

1427
00:52:33,680 --> 00:52:37,839
models, as long as we get the scientific

1428
00:52:35,280 --> 00:52:41,359
discoveries, I'm very happy either way.

1429
00:52:37,839 --> 00:52:44,079
IPO next year. I don't know. Do you want

1430
00:52:41,359 --> 00:52:46,079
to be a public company?

1431
00:52:44,079 --> 00:52:48,480
>> Um, you seem like you can operate

1432
00:52:46,079 --> 00:52:51,599
private for a long time. Would you go

1433
00:52:48,480 --> 00:52:52,240
before you needed to

1434
00:52:51,599 --> 00:52:53,440
terms of funding?

1435
00:52:52,240 --> 00:52:57,520
>> There's like a whole bunch of things at

1436
00:52:53,440 --> 00:52:59,599
play here. I do think it's cool that

1437
00:52:57,520 --> 00:53:03,200
public markets get to participate in

1438
00:52:59,599 --> 00:53:05,680
value creation and you know in some

1439
00:53:03,200 --> 00:53:09,680
sense we will be very late to go public

1440
00:53:05,680 --> 00:53:12,160
if you look at any previous company. Um

1441
00:53:09,680 --> 00:53:16,480
it's wonderful to be a private company.

1442
00:53:12,160 --> 00:53:18,160
Uh we need lots of capital. Uh

1443
00:53:16,480 --> 00:53:19,680
we're going to you know cross all of the

1444
00:53:18,160 --> 00:53:22,400
sort of shareholder limits and stuff at

1445
00:53:19,680 --> 00:53:24,400
some point. So,

1446
00:53:22,400 --> 00:53:29,599
am I excited

1447
00:53:24,400 --> 00:53:30,880
to be a public company CEO? 0%. Um, am I

1448
00:53:29,599 --> 00:53:34,319
excited for Open Eye to be a public

1449
00:53:30,880 --> 00:53:36,800
company? In some ways, I am. And in some

1450
00:53:34,319 --> 00:53:38,960
ways, I think it'll be really annoying.

1451
00:53:36,800 --> 00:53:42,079
I listened to your Theo van interview

1452
00:53:38,960 --> 00:53:42,800
very closely. Uh, great interview.

1453
00:53:42,079 --> 00:53:45,680
>> He was really cool.

1454
00:53:42,800 --> 00:53:46,319
>> Theo really knows what he's talking.

1455
00:53:45,680 --> 00:53:48,640
He's

1456
00:53:46,319 --> 00:53:50,960
>> He did his homework. You told him, this

1457
00:53:48,640 --> 00:53:54,480
was right before GPT5 came out, that

1458
00:53:50,960 --> 00:53:58,000
GPT5 is smarter than us in almost every

1459
00:53:54,480 --> 00:54:00,319
way. Uh, I I thought that that was the

1460
00:53:58,000 --> 00:54:02,800
definition of AGI. Does is that isn't

1461
00:54:00,319 --> 00:54:04,720
that AGI? And and if not, has the term

1462
00:54:02,800 --> 00:54:07,520
become somewhat meaningless? These

1463
00:54:04,720 --> 00:54:10,000
models are clearly extremely smart on a

1464
00:54:07,520 --> 00:54:11,440
sort of raw horsepower basis. You know,

1465
00:54:10,000 --> 00:54:13,200
there's all this stuff on the last

1466
00:54:11,440 --> 00:54:17,839
couple of days about GPT 5.2 who has an

1467
00:54:13,200 --> 00:54:19,760
IQ of 147 or 144 or 151 or whatever it

1468
00:54:17,839 --> 00:54:22,160
is. It's like, you know, depending on

1469
00:54:19,760 --> 00:54:24,400
whose test it's like it's some high

1470
00:54:22,160 --> 00:54:27,520
number and you have like a lot of

1471
00:54:24,400 --> 00:54:29,040
experts in their field saying

1472
00:54:27,520 --> 00:54:30,559
it can do these amazing things and it's

1473
00:54:29,040 --> 00:54:32,000
like contributing it's making it more

1474
00:54:30,559 --> 00:54:35,599
effective. You have the GDP things we

1475
00:54:32,000 --> 00:54:37,680
talked about. One thing you don't have

1476
00:54:35,599 --> 00:54:39,839
is

1477
00:54:37,680 --> 00:54:42,720
the ability for the model to not be able

1478
00:54:39,839 --> 00:54:44,559
to do something today, realize it can't

1479
00:54:42,720 --> 00:54:45,760
go off and figure out how to learn to

1480
00:54:44,559 --> 00:54:46,880
get good at that thing, learn to

1481
00:54:45,760 --> 00:54:49,760
understand it, and when you come back

1482
00:54:46,880 --> 00:54:54,720
the next day, it gets it right. And that

1483
00:54:49,760 --> 00:54:58,000
kind of continuous learning like

1484
00:54:54,720 --> 00:55:00,800
toddlers can do it. It does seem to me

1485
00:54:58,000 --> 00:55:02,720
like an important part of what we need

1486
00:55:00,800 --> 00:55:04,000
to build. Now, can you have something

1487
00:55:02,720 --> 00:55:06,160
that most people would consider an AGI

1488
00:55:04,000 --> 00:55:07,520
without that? I would say clear. I mean,

1489
00:55:06,160 --> 00:55:10,240
there's a lot of people that would say

1490
00:55:07,520 --> 00:55:12,559
we're at AGI with our current models.

1491
00:55:10,240 --> 00:55:14,400
Um,

1492
00:55:12,559 --> 00:55:16,000
I think almost everyone would agree that

1493
00:55:14,400 --> 00:55:17,359
if we were at the current level of

1494
00:55:16,000 --> 00:55:21,359
intelligence and had that other thing,

1495
00:55:17,359 --> 00:55:25,920
it would clearly be very AGI like. Um,

1496
00:55:21,359 --> 00:55:27,280
but maybe most of the world will say,

1497
00:55:25,920 --> 00:55:29,280
"Okay, fine. Even without that, like

1498
00:55:27,280 --> 00:55:31,680
it's doing most knowledge tasks that

1499
00:55:29,280 --> 00:55:34,079
matter. um smarter than us in mo most of

1500
00:55:31,680 --> 00:55:35,359
us in most ways. We're at AGI. You know,

1501
00:55:34,079 --> 00:55:37,520
it's discovering small piece of new

1502
00:55:35,359 --> 00:55:39,920
science. We're at AGI. What I think this

1503
00:55:37,520 --> 00:55:41,119
means is that the term although it's

1504
00:55:39,920 --> 00:55:45,119
been very hard for all of us to stop

1505
00:55:41,119 --> 00:55:49,440
using is very underdefined, right?

1506
00:55:45,119 --> 00:55:51,440
I I have a I have a a can like one thing

1507
00:55:49,440 --> 00:55:52,960
I would love

1508
00:55:51,440 --> 00:55:54,160
since we got wrong with AGI. We never

1509
00:55:52,960 --> 00:55:55,280
define that that you know the new term

1510
00:55:54,160 --> 00:55:58,559
everyone's focused about is when we get

1511
00:55:55,280 --> 00:56:01,760
to super intelligence. Um so my proposal

1512
00:55:58,559 --> 00:56:04,960
is that we agree that you know AGI kind

1513
00:56:01,760 --> 00:56:07,599
of went whooshing by. It was didn't

1514
00:56:04,960 --> 00:56:09,599
change the world that much or it will in

1515
00:56:07,599 --> 00:56:12,400
the long term but okay fine we've built

1516
00:56:09,599 --> 00:56:14,000
AGIs at some point you know we're in

1517
00:56:12,400 --> 00:56:15,599
this like fuzzy period where some people

1518
00:56:14,000 --> 00:56:17,040
think we have and some people think we

1519
00:56:15,599 --> 00:56:19,359
have and more people will think we have

1520
00:56:17,040 --> 00:56:22,880
and and then we'll say okay what's next?

1521
00:56:19,359 --> 00:56:26,400
Um, a candidate definition for super

1522
00:56:22,880 --> 00:56:28,960
intelligence is when a system can do a

1523
00:56:26,400 --> 00:56:31,599
better job being president of United

1524
00:56:28,960 --> 00:56:33,359
States, CEO of a major company, you

1525
00:56:31,599 --> 00:56:37,359
know, running a very large scientific

1526
00:56:33,359 --> 00:56:39,119
lab than any person can even with the

1527
00:56:37,359 --> 00:56:40,000
assistance of AI.

1528
00:56:39,119 --> 00:56:41,280
>> Okay,

1529
00:56:40,000 --> 00:56:44,160
>> I think this was an interesting thing

1530
00:56:41,280 --> 00:56:47,359
about what happened with chess where

1531
00:56:44,160 --> 00:56:49,200
chess got it could be humans. I remember

1532
00:56:47,359 --> 00:56:50,720
this very vividly. uh that deep blue

1533
00:56:49,200 --> 00:56:53,359
thing and then there was a period of

1534
00:56:50,720 --> 00:56:56,319
time where

1535
00:56:53,359 --> 00:57:00,000
a human and the AI together were better

1536
00:56:56,319 --> 00:57:01,520
than an AI by itself and then the person

1537
00:57:00,000 --> 00:57:03,520
was just making it worse and the

1538
00:57:01,520 --> 00:57:06,640
smartest thing was the unaded AI that

1539
00:57:03,520 --> 00:57:08,799
didn't have the human like

1540
00:57:06,640 --> 00:57:11,520
not understanding its its great

1541
00:57:08,799 --> 00:57:12,960
intelligence. Um

1542
00:57:11,520 --> 00:57:14,559
I think something like that is like an

1543
00:57:12,960 --> 00:57:15,680
interesting framework for super

1544
00:57:14,559 --> 00:57:17,920
intelligence. I think it's like a long

1545
00:57:15,680 --> 00:57:20,160
way off, but I would love to have like a

1546
00:57:17,920 --> 00:57:22,079
cleaner definition this time around.

1547
00:57:20,160 --> 00:57:24,960
>> Well, Sam, look, I I have uh been in

1548
00:57:22,079 --> 00:57:26,720
your products uh using them daily for 3

1549
00:57:24,960 --> 00:57:28,960
years. Um

1550
00:57:26,720 --> 00:57:30,400
>> definitely gotten a lot better. Can't

1551
00:57:28,960 --> 00:57:31,520
even imagine where they go from here.

1552
00:57:30,400 --> 00:57:32,400
>> We'll we'll try to keep getting them

1553
00:57:31,520 --> 00:57:34,559
better fast.

1554
00:57:32,400 --> 00:57:36,640
>> Okay. And uh this is our second time

1555
00:57:34,559 --> 00:57:38,480
speaking and I appreciate how open

1556
00:57:36,640 --> 00:57:39,520
you've been uh both times. So, thank you

1557
00:57:38,480 --> 00:57:41,280
for your time.

1558
00:57:39,520 --> 00:57:43,200
>> Thank you everybody for listening and

1559
00:57:41,280 --> 00:57:45,520
watching. If you're here for the first

1560
00:57:43,200 --> 00:57:47,119
time, please hit follow or subscribe. We

1561
00:57:45,520 --> 00:57:48,880
have lots of great interviews on the

1562
00:57:47,119 --> 00:57:51,119
feed and more on the way. This past

1563
00:57:48,880 --> 00:57:53,920
year, we've had Google DeepMind CEO

1564
00:57:51,119 --> 00:57:56,240
Demisabus on twice, including one with

1565
00:57:53,920 --> 00:57:59,520
Google founder Sergey Brin. We've also

1566
00:57:56,240 --> 00:58:01,680
had Dario Ammoday, the CEO of Anthropic.

1567
00:57:59,520 --> 00:58:04,480
And we have plenty of big interviews

1568
00:58:01,680 --> 00:58:06,079
coming up in 2026. Thanks again, and

1569
00:58:04,480 --> 00:58:09,640
we'll see you next time on Big

1570
00:58:06,079 --> 00:58:09,640
Technology Podcast.

